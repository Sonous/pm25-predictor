{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PM2.5 Prediction - Data Preprocessing with HDFS\n",
    "\n",
    "## üéØ M·ª•c ƒê√≠ch\n",
    "Notebook n√†y demo **t√≠ch h·ª£p HDFS v√†o quy tr√¨nh preprocessing** cho m√¥n Big Data:\n",
    "- ‚úÖ **Input:** ƒê·ªçc d·ªØ li·ªáu t·ª´ HDFS (`/data/raw/`)\n",
    "- ‚úÖ **Processing:** X·ª≠ l√Ω v·ªõi PySpark (local mode)\n",
    "- ‚úÖ **Output:** L∆∞u k·∫øt qu·∫£ l√™n HDFS (`/data/processed/`)\n",
    "\n",
    "## ‚öôÔ∏è Workflow Architecture\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   HDFS      ‚îÇ      ‚îÇ   Windows    ‚îÇ      ‚îÇ   HDFS      ‚îÇ\n",
    "‚îÇ  /data/raw  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   + Spark    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ/data/proc   ‚îÇ\n",
    "‚îÇ  (Input)    ‚îÇ Download‚îÇ  Processing ‚îÇUpload‚îÇ (Output)   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Technical Approach: Hybrid Mode\n",
    "\n",
    "Do Spark tr√™n Windows kh√¥ng k·∫øt n·ªëi tr·ª±c ti·∫øp HDFS qua RPC ƒë∆∞·ª£c, ta d√πng ph∆∞∆°ng ph√°p:\n",
    "\n",
    "1. **Download t·ª´ HDFS:** `docker exec hdfs-namenode hdfs dfs -get` ‚Üí local temp\n",
    "2. **Process v·ªõi Spark:** ƒê·ªçc local files, x·ª≠ l√Ω nh∆∞ b√¨nh th∆∞·ªùng\n",
    "3. **Upload l√™n HDFS:** `docker exec hdfs-namenode hdfs dfs -put` ‚Üí HDFS\n",
    "\n",
    "C√°ch n√†y v·∫´n **ƒë·∫ßy ƒë·ªß demo HDFS** cho m√¥n h·ªçc m√† tr√°nh ƒë∆∞·ª£c l·ªói k·∫øt n·ªëi!\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "### 1. Kh·ªüi ƒë·ªông HDFS Cluster\n",
    "```bash\n",
    "docker-compose up -d\n",
    "```\n",
    "\n",
    "### 2. Upload d·ªØ li·ªáu l√™n HDFS\n",
    "```powershell\n",
    ".\\upload_to_hdfs.ps1\n",
    "```\n",
    "\n",
    "### 3. Ki·ªÉm tra HDFS\n",
    "- NameNode UI: http://localhost:9870\n",
    "- Verify: C√≥ 28 files trong `/data/raw/`\n",
    "\n",
    "### 4. C·∫•u tr√∫c HDFS\n",
    "```\n",
    "/data/\n",
    "  ‚îú‚îÄ‚îÄ raw/                          # Input t·ª´ HDFS\n",
    "  ‚îÇ   ‚îú‚îÄ‚îÄ pollutant_location_*.csv  (14 files)\n",
    "  ‚îÇ   ‚îî‚îÄ‚îÄ weather_location_*.csv    (14 files)\n",
    "  ‚îî‚îÄ‚îÄ processed/                    # Output l√™n HDFS\n",
    "      ‚îú‚îÄ‚îÄ cnn_sequences/\n",
    "      ‚îú‚îÄ‚îÄ lstm_sequences/\n",
    "      ‚îú‚îÄ‚îÄ xgboost/\n",
    "      ‚îú‚îÄ‚îÄ datasets_ready.json\n",
    "      ‚îî‚îÄ‚îÄ scaler_params.json\n",
    "```\n",
    "\n",
    "## üöÄ Ready to Run!\n",
    "Ch·∫°y All Cells ƒë·ªÉ th·ª±c hi·ªán preprocessing v·ªõi HDFS integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:03:16.949403Z",
     "iopub.status.busy": "2025-12-01T16:03:16.949066Z",
     "iopub.status.idle": "2025-12-01T16:03:26.798005Z",
     "shell.execute_reply": "2025-12-01T16:03:26.796589Z",
     "shell.execute_reply.started": "2025-12-01T16:03:16.949379Z"
    },
    "papermill": {
     "duration": 8.655759,
     "end_time": "2025-11-20T10:28:24.331187",
     "exception": false,
     "start_time": "2025-11-20T10:28:15.675428",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Using Java: C:\\Program Files\\Java\\jdk-21\n",
      "[OK] All imports successful!\n",
      "[OK] All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Set Java for PySpark (local machine)\n",
    "os.environ['JAVA_HOME'] = r'C:\\Program Files\\Java\\jdk-21'\n",
    "os.environ['PATH'] = os.environ['JAVA_HOME'] + r'\\bin;' + os.environ.get('PATH', '')\n",
    "print(f\"[OK] Using Java: {os.environ['JAVA_HOME']}\")\n",
    "\n",
    "# Common imports\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"[OK] All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007426,
     "end_time": "2025-11-20T10:28:24.346377",
     "exception": false,
     "start_time": "2025-11-20T10:28:24.338951",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. K·∫øt n·ªëi Spark v·ªõi HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:03:26.800283Z",
     "iopub.status.busy": "2025-12-01T16:03:26.799874Z",
     "iopub.status.idle": "2025-12-01T16:03:40.821433Z",
     "shell.execute_reply": "2025-12-01T16:03:40.820377Z",
     "shell.execute_reply.started": "2025-12-01T16:03:26.800250Z"
    },
    "papermill": {
     "duration": 8.251884,
     "end_time": "2025-11-20T10:28:32.605710",
     "exception": false,
     "start_time": "2025-11-20T10:28:24.353826",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Configuring Spark with HDFS support...\n",
      "[INFO] HDFS NameNode (RPC): hdfs://localhost:9000\n",
      "[INFO] WebHDFS (HTTP): webhdfs://localhost:9870\n",
      "[OK] Spark version: 4.0.1\n",
      "[OK] Spark mode: local[8]\n",
      "[OK] Application ID: local-1764997463457\n",
      "[OK] Cores: 16\n",
      "\n",
      "[TEST] Testing HDFS connection via WebHDFS...\n",
      "[OK] Spark version: 4.0.1\n",
      "[OK] Spark mode: local[8]\n",
      "[OK] Application ID: local-1764997463457\n",
      "[OK] Cores: 16\n",
      "\n",
      "[TEST] Testing HDFS connection via WebHDFS...\n",
      "[ERROR] ‚úó HDFS connection failed: An error occurred while calling o61.load.\n",
      ": org.apache.spark.SparkException: [FAILED_READ_FILE.NO_HINT] Encountered error while reading file hdfs://localhost:9000/data/raw/pollutant_location_7727.csv.  SQLSTATE: KD001\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:856)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:142)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\n",
      "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\n",
      "\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)\n",
      "\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:121)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:72)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:63)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:219)\n",
      "\tat scala.Option.orElse(Option.scala:477)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:216)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:422)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:100)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n",
      "\t\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:856)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:142)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:142)\n",
      "\t\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\t\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\t\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\t\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n",
      "\t\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
      "\t\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
      "\t\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\t\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\t\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\t\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\t\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\t\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\t\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\t\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\t\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n",
      "\t\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n",
      "\t\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n",
      "\t\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\n",
      "\t\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\n",
      "\t\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)\n",
      "\t\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1379)\n",
      "\t\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\t\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n",
      "\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n",
      "\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n",
      "\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\n",
      "\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\n",
      "\t\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\t\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\t\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:121)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:72)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:63)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:219)\n",
      "\t\tat scala.Option.orElse(Option.scala:477)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:216)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:422)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\t\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\t\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n",
      "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\t\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\t\t... 20 more\n",
      "Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1949370538-172.18.0.2-1764993111268:blk_1073741827_1003 file=/data/raw/pollutant_location_7727.csv No live nodes contain current block Block locations: DatanodeInfoWithStorage[172.18.0.3:9866,DS-47222eb1-3b31-4548-b669-edb6631cc138,DISK] DatanodeInfoWithStorage[172.18.0.2:9866,DS-dcfb5780-c388-4f58-a289-0fcb6f79f8ef,DISK] Dead nodes:  DatanodeInfoWithStorage[172.18.0.3:9866,DS-47222eb1-3b31-4548-b669-edb6631cc138,DISK] DatanodeInfoWithStorage[172.18.0.2:9866,DS-dcfb5780-c388-4f58-a289-0fcb6f79f8ef,DISK]\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:978)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:953)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:931)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:637)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:845)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:919)\n",
      "\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:166)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:206)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:71)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:131)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:292)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:131)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:140)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\t... 1 more\n",
      "\n",
      "\n",
      "[INFO] Troubleshooting steps:\n",
      "       1. Restart HDFS: docker-compose restart\n",
      "       2. Wait 30 seconds for HDFS to be fully ready\n",
      "       3. Verify upload: docker exec hdfs-namenode hdfs dfs -ls /data/raw\n",
      "       4. Check safe mode: docker exec hdfs-namenode hdfs dfsadmin -safemode get\n",
      "       5. If still fails, try using local files instead of HDFS\n",
      "[ERROR] ‚úó HDFS connection failed: An error occurred while calling o61.load.\n",
      ": org.apache.spark.SparkException: [FAILED_READ_FILE.NO_HINT] Encountered error while reading file hdfs://localhost:9000/data/raw/pollutant_location_7727.csv.  SQLSTATE: KD001\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:856)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:142)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\n",
      "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\n",
      "\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)\n",
      "\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\n",
      "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:121)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:72)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:63)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:219)\n",
      "\tat scala.Option.orElse(Option.scala:477)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:216)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:422)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:100)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n",
      "\t\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:856)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:142)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:142)\n",
      "\t\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\t\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\t\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\t\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n",
      "\t\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
      "\t\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
      "\t\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\t\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\t\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\t\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\t\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\t\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\t\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\t\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\t\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\t\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n",
      "\t\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n",
      "\t\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n",
      "\t\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\n",
      "\t\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\n",
      "\t\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\n",
      "\t\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)\n",
      "\t\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1379)\n",
      "\t\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\t\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n",
      "\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n",
      "\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n",
      "\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\n",
      "\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\n",
      "\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\n",
      "\t\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n",
      "\t\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\n",
      "\t\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:121)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:72)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:63)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:219)\n",
      "\t\tat scala.Option.orElse(Option.scala:477)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:216)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:422)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\t\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\t\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n",
      "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n",
      "\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\t\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\t\t... 20 more\n",
      "Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1949370538-172.18.0.2-1764993111268:blk_1073741827_1003 file=/data/raw/pollutant_location_7727.csv No live nodes contain current block Block locations: DatanodeInfoWithStorage[172.18.0.3:9866,DS-47222eb1-3b31-4548-b669-edb6631cc138,DISK] DatanodeInfoWithStorage[172.18.0.2:9866,DS-dcfb5780-c388-4f58-a289-0fcb6f79f8ef,DISK] Dead nodes:  DatanodeInfoWithStorage[172.18.0.3:9866,DS-47222eb1-3b31-4548-b669-edb6631cc138,DISK] DatanodeInfoWithStorage[172.18.0.2:9866,DS-dcfb5780-c388-4f58-a289-0fcb6f79f8ef,DISK]\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:978)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:953)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:931)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:637)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:845)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:919)\n",
      "\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:166)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:206)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:71)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:131)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:292)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:131)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:140)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\t... 1 more\n",
      "\n",
      "\n",
      "[INFO] Troubleshooting steps:\n",
      "       1. Restart HDFS: docker-compose restart\n",
      "       2. Wait 30 seconds for HDFS to be fully ready\n",
      "       3. Verify upload: docker exec hdfs-namenode hdfs dfs -ls /data/raw\n",
      "       4. Check safe mode: docker exec hdfs-namenode hdfs dfsadmin -safemode get\n",
      "       5. If still fails, try using local files instead of HDFS\n"
     ]
    }
   ],
   "source": [
    "# HDFS Configuration - Using Docker commands for file I/O\n",
    "# Since Spark RPC from Windows to HDFS has connectivity issues,\n",
    "# we'll use a hybrid approach:\n",
    "# 1. Download files from HDFS to temp directory (via docker exec)\n",
    "# 2. Process with Spark locally\n",
    "# 3. Upload results back to HDFS (via docker exec)\n",
    "\n",
    "import subprocess\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "HDFS_NAMENODE = \"hdfs://namenode:9000\"  # Internal HDFS address\n",
    "HDFS_RAW_DATA_PATH = \"/data/raw\"\n",
    "HDFS_PROCESSED_DATA_PATH = \"/data/processed\"\n",
    "TEMP_DIR = tempfile.mkdtemp(prefix=\"hdfs_staging_\")\n",
    "\n",
    "print(\"[INFO] HDFS Hybrid Mode Configuration\")\n",
    "print(f\"[INFO] HDFS Raw Data: {HDFS_RAW_DATA_PATH}\")\n",
    "print(f\"[INFO] HDFS Processed Data: {HDFS_PROCESSED_DATA_PATH}\")\n",
    "print(f\"[INFO] Local Staging: {TEMP_DIR}\")\n",
    "\n",
    "# Helper functions for HDFS operations\n",
    "def hdfs_download_directory(hdfs_path, local_path):\n",
    "    \"\"\"Download entire directory from HDFS to local via docker exec\"\"\"\n",
    "    print(f\"\\n[DOWNLOAD] HDFS {hdfs_path} -> Local {local_path}\")\n",
    "    \n",
    "    # Create local directory\n",
    "    os.makedirs(local_path, exist_ok=True)\n",
    "    \n",
    "    # Step 1: Get from HDFS to container /tmp\n",
    "    container_tmp = f\"/tmp/hdfs_download_{os.path.basename(hdfs_path)}\"\n",
    "    subprocess.run(\n",
    "        [\"docker\", \"exec\", \"hdfs-namenode\", \"hdfs\", \"dfs\", \"-get\", hdfs_path, container_tmp],\n",
    "        check=True,\n",
    "        capture_output=True\n",
    "    )\n",
    "    \n",
    "    # Step 2: Copy from container to Windows host\n",
    "    subprocess.run(\n",
    "        [\"docker\", \"cp\", f\"hdfs-namenode:{container_tmp}/.\", local_path],\n",
    "        check=True,\n",
    "        capture_output=True\n",
    "    )\n",
    "    \n",
    "    # Step 3: Cleanup container temp\n",
    "    subprocess.run(\n",
    "        [\"docker\", \"exec\", \"hdfs-namenode\", \"rm\", \"-rf\", container_tmp],\n",
    "        check=False,\n",
    "        capture_output=True\n",
    "    )\n",
    "    \n",
    "    # Count files\n",
    "    file_count = len([f for f in os.listdir(local_path) if os.path.isfile(os.path.join(local_path, f))])\n",
    "    print(f\"[OK] Downloaded {file_count} files\")\n",
    "    return local_path\n",
    "\n",
    "def hdfs_upload_directory(local_path, hdfs_path):\n",
    "    \"\"\"Upload entire directory from local to HDFS via docker exec\"\"\"\n",
    "    print(f\"\\n[UPLOAD] Local {local_path} -> HDFS {hdfs_path}\")\n",
    "    \n",
    "    # Step 1: Copy from Windows host to container /tmp\n",
    "    container_tmp = f\"/tmp/hdfs_upload_{os.path.basename(local_path)}\"\n",
    "    subprocess.run(\n",
    "        [\"docker\", \"exec\", \"hdfs-namenode\", \"rm\", \"-rf\", container_tmp],\n",
    "        check=False,\n",
    "        capture_output=True\n",
    "    )\n",
    "    subprocess.run(\n",
    "        [\"docker\", \"cp\", f\"{local_path}/.\", f\"hdfs-namenode:{container_tmp}\"],\n",
    "        check=True,\n",
    "        capture_output=True\n",
    "    )\n",
    "    \n",
    "    # Step 2: Put from container to HDFS (create parent dir first)\n",
    "    hdfs_parent = os.path.dirname(hdfs_path)\n",
    "    subprocess.run(\n",
    "        [\"docker\", \"exec\", \"hdfs-namenode\", \"hdfs\", \"dfs\", \"-mkdir\", \"-p\", hdfs_parent],\n",
    "        check=False,\n",
    "        capture_output=True\n",
    "    )\n",
    "    subprocess.run(\n",
    "        [\"docker\", \"exec\", \"hdfs-namenode\", \"hdfs\", \"dfs\", \"-rm\", \"-r\", \"-f\", hdfs_path],\n",
    "        check=False,\n",
    "        capture_output=True\n",
    "    )\n",
    "    subprocess.run(\n",
    "        [\"docker\", \"exec\", \"hdfs-namenode\", \"hdfs\", \"dfs\", \"-put\", container_tmp, hdfs_path],\n",
    "        check=True,\n",
    "        capture_output=True\n",
    "    )\n",
    "    \n",
    "    # Step 3: Cleanup container temp\n",
    "    subprocess.run(\n",
    "        [\"docker\", \"exec\", \"hdfs-namenode\", \"rm\", \"-rf\", container_tmp],\n",
    "        check=False,\n",
    "        capture_output=True\n",
    "    )\n",
    "    \n",
    "    print(f\"[OK] Uploaded to {hdfs_path}\")\n",
    "\n",
    "# Create Spark Session (local mode - no HDFS direct access needed)\n",
    "print(\"\\n[INFO] Creating Spark Session (local mode)...\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PM25-Preprocessing-HDFS-Hybrid\") \\\n",
    "    .master(\"local[8]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"16\") \\\n",
    "    .config(\"spark.default.parallelism\", \"16\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(f\"[OK] Spark Session created: {spark.version}\")\n",
    "print(f\"[OK] Master: {spark.sparkContext.master}\")\n",
    "print(f\"[OK] App Name: {spark.sparkContext.appName}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.0083,
     "end_time": "2025-11-20T10:28:32.622759",
     "exception": false,
     "start_time": "2025-11-20T10:28:32.614459",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. ƒê·ªãnh nghƒ©a Schema v√† Scan Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:03:40.823838Z",
     "iopub.status.busy": "2025-12-01T16:03:40.823159Z",
     "iopub.status.idle": "2025-12-01T16:03:40.836375Z",
     "shell.execute_reply": "2025-12-01T16:03:40.834735Z",
     "shell.execute_reply.started": "2025-12-01T16:03:40.823764Z"
    },
    "papermill": {
     "duration": 0.018341,
     "end_time": "2025-11-20T10:28:32.649081",
     "exception": false,
     "start_time": "2025-11-20T10:28:32.630740",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Schemas defined\n"
     ]
    }
   ],
   "source": [
    "# Download data from HDFS to local staging directory\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 1: Download Data from HDFS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Download raw data from HDFS\n",
    "LOCAL_RAW_PATH = os.path.join(TEMP_DIR, \"raw\")\n",
    "hdfs_download_directory(HDFS_RAW_DATA_PATH, LOCAL_RAW_PATH)\n",
    "\n",
    "# Verify downloaded files\n",
    "pollutant_files = [f for f in os.listdir(LOCAL_RAW_PATH) if f.startswith('pollutant_')]\n",
    "weather_files = [f for f in os.listdir(LOCAL_RAW_PATH) if f.startswith('weather_')]\n",
    "\n",
    "print(f\"\\n[VERIFY] Pollutant files: {len(pollutant_files)}\")\n",
    "print(f\"[VERIFY] Weather files: {len(weather_files)}\")\n",
    "print(f\"[OK] All data downloaded from HDFS to: {LOCAL_RAW_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007921,
     "end_time": "2025-11-20T10:28:32.664951",
     "exception": false,
     "start_time": "2025-11-20T10:28:32.657030",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.1 Scan v√† Map Files theo Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:03:40.839833Z",
     "iopub.status.busy": "2025-12-01T16:03:40.839025Z",
     "iopub.status.idle": "2025-12-01T16:03:41.094850Z",
     "shell.execute_reply": "2025-12-01T16:03:41.093337Z",
     "shell.execute_reply.started": "2025-12-01T16:03:40.839761Z"
    },
    "papermill": {
     "duration": 0.146926,
     "end_time": "2025-11-20T10:28:32.819742",
     "exception": false,
     "start_time": "2025-11-20T10:28:32.672816",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HDFS] Files in /data/raw:\n",
      "  - pollutant_location_233335.csv (11.96 MB)\n",
      "  - pollutant_location_233336.csv (12.28 MB)\n",
      "  - pollutant_location_7727.csv (13.20 MB)\n",
      "  - pollutant_location_7728.csv (11.49 MB)\n",
      "  - pollutant_location_7730.csv (9.97 MB)\n",
      "  - pollutant_location_7732.csv (10.25 MB)\n",
      "  - pollutant_location_7733.csv (10.22 MB)\n",
      "  - pollutant_location_7734.csv (10.77 MB)\n",
      "  - pollutant_location_7735.csv (10.55 MB)\n",
      "  - pollutant_location_7736.csv (10.65 MB)\n",
      "  - pollutant_location_7737.csv (10.19 MB)\n",
      "  - pollutant_location_7739.csv (12.76 MB)\n",
      "  - pollutant_location_7740.csv (12.84 MB)\n",
      "  - pollutant_location_7742.csv (12.85 MB)\n",
      "  - weather_location_233335.csv (1.07 MB)\n",
      "  - weather_location_233336.csv (1.08 MB)\n",
      "  - weather_location_7727.csv (1.08 MB)\n",
      "  - weather_location_7728.csv (1.07 MB)\n",
      "  - weather_location_7730.csv (1.07 MB)\n",
      "  - weather_location_7732.csv (1.08 MB)\n",
      "  - weather_location_7733.csv (1.07 MB)\n",
      "  - weather_location_7734.csv (1.07 MB)\n",
      "  - weather_location_7735.csv (1.07 MB)\n",
      "  - weather_location_7736.csv (1.07 MB)\n",
      "  - weather_location_7737.csv (1.07 MB)\n",
      "  - weather_location_7739.csv (1.07 MB)\n",
      "  - weather_location_7740.csv (1.07 MB)\n",
      "  - weather_location_7742.csv (1.07 MB)\n"
     ]
    }
   ],
   "source": [
    "# Scan downloaded files from local staging\n",
    "import re\n",
    "\n",
    "pollutant_files = sorted([f for f in os.listdir(LOCAL_RAW_PATH) if f.startswith('pollutant_')])\n",
    "location_mapping = {}\n",
    "\n",
    "for pollutant_file in pollutant_files:\n",
    "    match = re.search(r'pollutant_location_(\\d+)\\.csv', pollutant_file)\n",
    "    if match:\n",
    "        location_id = match.group(1)\n",
    "        weather_file = f'weather_location_{location_id}.csv'\n",
    "        \n",
    "        if weather_file in os.listdir(LOCAL_RAW_PATH):\n",
    "            location_mapping[location_id] = {\n",
    "                'pollutant': os.path.join(LOCAL_RAW_PATH, pollutant_file),\n",
    "                'weather': os.path.join(LOCAL_RAW_PATH, weather_file)\n",
    "            }\n",
    "\n",
    "print(f\"[INFO] Found {len(location_mapping)} location pairs\")\n",
    "print(f\"[INFO] Location IDs: {sorted(location_mapping.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:03:41.096665Z",
     "iopub.status.busy": "2025-12-01T16:03:41.096363Z",
     "iopub.status.idle": "2025-12-01T16:03:41.140731Z",
     "shell.execute_reply": "2025-12-01T16:03:41.139536Z",
     "shell.execute_reply.started": "2025-12-01T16:03:41.096637Z"
    },
    "papermill": {
     "duration": 0.057939,
     "end_time": "2025-11-20T10:28:32.885885",
     "exception": false,
     "start_time": "2025-11-20T10:28:32.827946",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HDFS] Using HDFS path: hdfs://localhost:9000/data/raw\n",
      "[FILES] Found 14 pollutant files in HDFS\n",
      "  [OK] Location 233335: pollutant_location_233335.csv + weather_location_233335.csv\n",
      "  [OK] Location 233336: pollutant_location_233336.csv + weather_location_233336.csv\n",
      "  [OK] Location 7727: pollutant_location_7727.csv + weather_location_7727.csv\n",
      "  [OK] Location 7728: pollutant_location_7728.csv + weather_location_7728.csv\n",
      "  [OK] Location 7730: pollutant_location_7730.csv + weather_location_7730.csv\n",
      "  [OK] Location 7728: pollutant_location_7728.csv + weather_location_7728.csv\n",
      "  [OK] Location 7730: pollutant_location_7730.csv + weather_location_7730.csv\n",
      "  [OK] Location 7732: pollutant_location_7732.csv + weather_location_7732.csv\n",
      "  [OK] Location 7733: pollutant_location_7733.csv + weather_location_7733.csv\n",
      "  [OK] Location 7734: pollutant_location_7734.csv + weather_location_7734.csv\n",
      "  [OK] Location 7735: pollutant_location_7735.csv + weather_location_7735.csv\n",
      "  [OK] Location 7736: pollutant_location_7736.csv + weather_location_7736.csv\n",
      "  [OK] Location 7737: pollutant_location_7737.csv + weather_location_7737.csv\n",
      "  [OK] Location 7732: pollutant_location_7732.csv + weather_location_7732.csv\n",
      "  [OK] Location 7733: pollutant_location_7733.csv + weather_location_7733.csv\n",
      "  [OK] Location 7734: pollutant_location_7734.csv + weather_location_7734.csv\n",
      "  [OK] Location 7735: pollutant_location_7735.csv + weather_location_7735.csv\n",
      "  [OK] Location 7736: pollutant_location_7736.csv + weather_location_7736.csv\n",
      "  [OK] Location 7737: pollutant_location_7737.csv + weather_location_7737.csv\n",
      "  [OK] Location 7739: pollutant_location_7739.csv + weather_location_7739.csv\n",
      "  [OK] Location 7740: pollutant_location_7740.csv + weather_location_7740.csv\n",
      "  [OK] Location 7742: pollutant_location_7742.csv + weather_location_7742.csv\n",
      "\n",
      "[SUCCESS] Total locations to process: 14\n",
      "  [OK] Location 7739: pollutant_location_7739.csv + weather_location_7739.csv\n",
      "  [OK] Location 7740: pollutant_location_7740.csv + weather_location_7740.csv\n",
      "  [OK] Location 7742: pollutant_location_7742.csv + weather_location_7742.csv\n",
      "\n",
      "[SUCCESS] Total locations to process: 14\n"
     ]
    }
   ],
   "source": [
    "# This cell is now handled by the previous cells that download from HDFS\n",
    "# Data is already in LOCAL_RAW_PATH and location_mapping is ready\n",
    "print(f\"[INFO] Using downloaded data from: {LOCAL_RAW_PATH}\")\n",
    "print(f\"[INFO] Total locations ready: {len(location_mapping)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:03:41.142375Z",
     "iopub.status.busy": "2025-12-01T16:03:41.142080Z",
     "iopub.status.idle": "2025-12-01T16:03:41.152804Z",
     "shell.execute_reply": "2025-12-01T16:03:41.151528Z",
     "shell.execute_reply.started": "2025-12-01T16:03:41.142352Z"
    },
    "papermill": {
     "duration": 0.016296,
     "end_time": "2025-11-20T10:28:32.910603",
     "exception": false,
     "start_time": "2025-11-20T10:28:32.894307",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location mapping:\n",
      "  233335: {'pollutant': 'hdfs://localhost:9000/data/raw/pollutant_location_233335.csv', 'weather': 'hdfs://localhost:9000/data/raw/weather_location_233335.csv'}\n",
      "  233336: {'pollutant': 'hdfs://localhost:9000/data/raw/pollutant_location_233336.csv', 'weather': 'hdfs://localhost:9000/data/raw/weather_location_233336.csv'}\n",
      "  7727: {'pollutant': 'hdfs://localhost:9000/data/raw/pollutant_location_7727.csv', 'weather': 'hdfs://localhost:9000/data/raw/weather_location_7727.csv'}\n",
      "  7728: {'pollutant': 'hdfs://localhost:9000/data/raw/pollutant_location_7728.csv', 'weather': 'hdfs://localhost:9000/data/raw/weather_location_7728.csv'}\n",
      "  7730: {'pollutant': 'hdfs://localhost:9000/data/raw/pollutant_location_7730.csv', 'weather': 'hdfs://localhost:9000/data/raw/weather_location_7730.csv'}\n",
      "  7732: {'pollutant': 'hdfs://localhost:9000/data/raw/pollutant_location_7732.csv', 'weather': 'hdfs://localhost:9000/data/raw/weather_location_7732.csv'}\n",
      "  7733: {'pollutant': 'hdfs://localhost:9000/data/raw/pollutant_location_7733.csv', 'weather': 'hdfs://localhost:9000/data/raw/weather_location_7733.csv'}\n",
      "  7734: {'pollutant': 'hdfs://localhost:9000/data/raw/pollutant_location_7734.csv', 'weather': 'hdfs://localhost:9000/data/raw/weather_location_7734.csv'}\n",
      "  7735: {'pollutant': 'hdfs://localhost:9000/data/raw/pollutant_location_7735.csv', 'weather': 'hdfs://localhost:9000/data/raw/weather_location_7735.csv'}\n",
      "  7736: {'pollutant': 'hdfs://localhost:9000/data/raw/pollutant_location_7736.csv', 'weather': 'hdfs://localhost:9000/data/raw/weather_location_7736.csv'}\n",
      "  7737: {'pollutant': 'hdfs://localhost:9000/data/raw/pollutant_location_7737.csv', 'weather': 'hdfs://localhost:9000/data/raw/weather_location_7737.csv'}\n",
      "  7739: {'pollutant': 'hdfs://localhost:9000/data/raw/pollutant_location_7739.csv', 'weather': 'hdfs://localhost:9000/data/raw/weather_location_7739.csv'}\n",
      "  7740: {'pollutant': 'hdfs://localhost:9000/data/raw/pollutant_location_7740.csv', 'weather': 'hdfs://localhost:9000/data/raw/weather_location_7740.csv'}\n",
      "  7742: {'pollutant': 'hdfs://localhost:9000/data/raw/pollutant_location_7742.csv', 'weather': 'hdfs://localhost:9000/data/raw/weather_location_7742.csv'}\n"
     ]
    }
   ],
   "source": [
    "# Display location mapping\n",
    "print(\"=\"*60)\n",
    "print(\"Location Files Ready for Processing:\")\n",
    "print(\"=\"*60)\n",
    "for loc_id in sorted(location_mapping.keys()):\n",
    "    files = location_mapping[loc_id]\n",
    "    print(f\"Location {loc_id}:\")\n",
    "    print(f\"  - Pollutant: {os.path.basename(files['pollutant'])}\")\n",
    "    print(f\"  - Weather:   {os.path.basename(files['weather'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008175,
     "end_time": "2025-11-20T10:28:32.926869",
     "exception": false,
     "start_time": "2025-11-20T10:28:32.918694",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.2 X·ª≠ l√Ω t·ª´ng Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:03:41.154161Z",
     "iopub.status.busy": "2025-12-01T16:03:41.153873Z",
     "iopub.status.idle": "2025-12-01T16:04:27.521102Z",
     "shell.execute_reply": "2025-12-01T16:04:27.519994Z",
     "shell.execute_reply.started": "2025-12-01T16:03:41.154135Z"
    },
    "papermill": {
     "duration": 35.305843,
     "end_time": "2025-11-20T10:29:08.240953",
     "exception": false,
     "start_time": "2025-11-20T10:28:32.935110",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PROCESSING] Processing Location 233335...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PROCESSING] Processing Location 233335...\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o246.count.\n: org.apache.spark.SparkException: [FAILED_READ_FILE.NO_HINT] Encountered error while reading file hdfs://localhost:9000/data/raw/pollutant_location_233335.csv.  SQLSTATE: KD001\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:856)\r\n\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:142)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:142)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1949370538-172.18.0.2-1764993111268:blk_1073741825_1001 file=/data/raw/pollutant_location_233335.csv No live nodes contain current block Block locations: DatanodeInfoWithStorage[172.18.0.2:9866,DS-dcfb5780-c388-4f58-a289-0fcb6f79f8ef,DISK] DatanodeInfoWithStorage[172.18.0.3:9866,DS-47222eb1-3b31-4548-b669-edb6631cc138,DISK] Dead nodes:  DatanodeInfoWithStorage[172.18.0.3:9866,DS-47222eb1-3b31-4548-b669-edb6631cc138,DISK] DatanodeInfoWithStorage[172.18.0.2:9866,DS-dcfb5780-c388-4f58-a289-0fcb6f79f8ef,DISK]\r\n\tat org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:978)\r\n\tat org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:953)\r\n\tat org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:931)\r\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:637)\r\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:845)\r\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:919)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\r\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\r\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\r\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:138)\r\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.$anonfun$_iterator$2(HadoopFileLinesReader.scala:66)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryInitializeResource(SparkErrorUtils.scala:59)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryInitializeResource$(SparkErrorUtils.scala:56)\r\n\tat org.apache.spark.util.Utils$.tryInitializeResource(Utils.scala:99)\r\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:65)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.$anonfun$readFile$1(CSVDataSource.scala:105)\r\n\tat org.apache.spark.TaskContextImpl.createResourceUninterruptibly(TaskContextImpl.scala:332)\r\n\tat org.apache.spark.util.Utils$.$anonfun$createResourceUninterruptiblyIfInTaskThread$1(Utils.scala:3097)\r\n\tat scala.Option.map(Option.scala:242)\r\n\tat org.apache.spark.util.Utils$.createResourceUninterruptiblyIfInTaskThread(Utils.scala:3096)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:105)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:147)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:230)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:289)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:131)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:140)\r\n\t... 20 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# ƒê·ªçc weather data\u001b[39;00m\n\u001b[32m     20\u001b[39m df_weather = spark.read.csv(\n\u001b[32m     21\u001b[39m     files[\u001b[33m'\u001b[39m\u001b[33mweather\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     22\u001b[39m     header=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     23\u001b[39m     schema=weather_schema\n\u001b[32m     24\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  [DATA] Air quality (PM2.5, PM10, SO2, NO2): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdf_air\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m records\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  [?]  Weather: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_weather.count()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m records\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Weather data - drop missing (√≠t missing)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\University_Projects\\pm25-predictor\\.venv\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:439\u001b[39m, in \u001b[36mDataFrame.count\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\University_Projects\\pm25-predictor\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\University_Projects\\pm25-predictor\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\University_Projects\\pm25-predictor\\.venv\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o246.count.\n: org.apache.spark.SparkException: [FAILED_READ_FILE.NO_HINT] Encountered error while reading file hdfs://localhost:9000/data/raw/pollutant_location_233335.csv.  SQLSTATE: KD001\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:856)\r\n\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:142)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:142)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1949370538-172.18.0.2-1764993111268:blk_1073741825_1001 file=/data/raw/pollutant_location_233335.csv No live nodes contain current block Block locations: DatanodeInfoWithStorage[172.18.0.2:9866,DS-dcfb5780-c388-4f58-a289-0fcb6f79f8ef,DISK] DatanodeInfoWithStorage[172.18.0.3:9866,DS-47222eb1-3b31-4548-b669-edb6631cc138,DISK] Dead nodes:  DatanodeInfoWithStorage[172.18.0.3:9866,DS-47222eb1-3b31-4548-b669-edb6631cc138,DISK] DatanodeInfoWithStorage[172.18.0.2:9866,DS-dcfb5780-c388-4f58-a289-0fcb6f79f8ef,DISK]\r\n\tat org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:978)\r\n\tat org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:953)\r\n\tat org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:931)\r\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:637)\r\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:845)\r\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:919)\r\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:158)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\r\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\r\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\r\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\r\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:138)\r\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.$anonfun$_iterator$2(HadoopFileLinesReader.scala:66)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryInitializeResource(SparkErrorUtils.scala:59)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryInitializeResource$(SparkErrorUtils.scala:56)\r\n\tat org.apache.spark.util.Utils$.tryInitializeResource(Utils.scala:99)\r\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:65)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.$anonfun$readFile$1(CSVDataSource.scala:105)\r\n\tat org.apache.spark.TaskContextImpl.createResourceUninterruptibly(TaskContextImpl.scala:332)\r\n\tat org.apache.spark.util.Utils$.$anonfun$createResourceUninterruptiblyIfInTaskThread$1(Utils.scala:3097)\r\n\tat scala.Option.map(Option.scala:242)\r\n\tat org.apache.spark.util.Utils$.createResourceUninterruptiblyIfInTaskThread(Utils.scala:3096)\r\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:105)\r\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:147)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:230)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:289)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:131)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:140)\r\n\t... 20 more\r\n"
     ]
    }
   ],
   "source": [
    "# List ƒë·ªÉ ch·ª©a dataframes c·ªßa t·ª´ng location\n",
    "all_locations_data = []\n",
    "\n",
    "for location_id, files in location_mapping.items():\n",
    "    print(f\"\\n[PROCESSING] Processing Location {location_id}...\")\n",
    "    \n",
    "    # ƒê·ªçc pollutant data\n",
    "    df_air = spark.read.csv(\n",
    "        files['pollutant'],\n",
    "        header=True,\n",
    "        schema=openaq_schema\n",
    "    )\n",
    "    \n",
    "    # [?] L·ªåC CH·ªà L·∫§Y C√ÅC CH·ªà S·ªê QUAN T√ÇM: PM2.5, PM10, SO2, NO2\n",
    "    df_air = df_air.filter(\n",
    "        F.col(\"parameter\").isin([\"pm25\", \"pm10\", \"so2\", \"no2\"])\n",
    "    )\n",
    "    \n",
    "    # ƒê·ªçc weather data\n",
    "    df_weather = spark.read.csv(\n",
    "        files['weather'],\n",
    "        header=True,\n",
    "        schema=weather_schema\n",
    "    )\n",
    "    \n",
    "    print(f\"  [DATA] Air quality (PM2.5, PM10, SO2, NO2): {df_air.count():,} records\")\n",
    "    print(f\"  [?]  Weather: {df_weather.count():,} records\")\n",
    "    \n",
    "    # Weather data - drop missing (√≠t missing)\n",
    "    df_weather_clean = df_weather.na.drop()\n",
    "    \n",
    "    # Pivot pollutant data\n",
    "    df_air_pivot = df_air.groupBy(\n",
    "        \"location_id\", \"location\", \"datetime\", \"lat\", \"lon\"\n",
    "    ).pivot(\"parameter\").agg(F.first(\"value\"))\n",
    "    \n",
    "    # Rename columns\n",
    "    column_mapping = {\n",
    "        \"pm25\": \"PM2_5\",\n",
    "        \"pm10\": \"PM10\",\n",
    "        \"no2\": \"NO2\",\n",
    "        \"so2\": \"SO2\"\n",
    "    }\n",
    "    \n",
    "    for old_name, new_name in column_mapping.items():\n",
    "        if old_name in df_air_pivot.columns:\n",
    "            df_air_pivot = df_air_pivot.withColumnRenamed(old_name, new_name)\n",
    "    \n",
    "    # Join v·ªõi weather data (theo datetime)\n",
    "    df_location = df_air_pivot.join(\n",
    "        df_weather_clean,\n",
    "        df_air_pivot.datetime == df_weather_clean.time,\n",
    "        \"inner\"\n",
    "    ).drop(\"time\")\n",
    "    \n",
    "    print(f\"  [OK] After join: {df_location.count():,} records\")\n",
    "    \n",
    "    # Th√™m v√†o list\n",
    "    all_locations_data.append(df_location)\n",
    "\n",
    "print(f\"\\n[SUCCESS] Processed {len(all_locations_data)} locations successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009653,
     "end_time": "2025-11-20T10:29:08.260972",
     "exception": false,
     "start_time": "2025-11-20T10:29:08.251319",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.3 G·ªôp t·∫•t c·∫£ Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:04:27.522995Z",
     "iopub.status.busy": "2025-12-01T16:04:27.522651Z",
     "iopub.status.idle": "2025-12-01T16:05:06.001940Z",
     "shell.execute_reply": "2025-12-01T16:05:05.998779Z",
     "shell.execute_reply.started": "2025-12-01T16:04:27.522971Z"
    },
    "papermill": {
     "duration": 31.318232,
     "end_time": "2025-11-20T10:29:39.588783",
     "exception": false,
     "start_time": "2025-11-20T10:29:08.270551",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROCESSING] Combining 14 locations...\n",
      "‚è≥ Computing combined dataset (this may take a moment)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 466:=================================================>   (104 + 5) / 112]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Combined dataset: 300,318 total records\n",
      "[SUCCESS] Number of locations: 14\n",
      "\n",
      "[METADATA] Sample records (unsorted):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-------------------+--------+---------+----+----+-----+---+--------------+--------------------+--------------+------------------+----------------+-------------+\n",
      "|location_id|location    |datetime           |lat     |lon      |NO2 |PM10|PM2_5|SO2|temperature_2m|relative_humidity_2m|wind_speed_10m|wind_direction_10m|surface_pressure|precipitation|\n",
      "+-----------+------------+-------------------+--------+---------+----+----+-----+---+--------------+--------------------+--------------+------------------+----------------+-------------+\n",
      "|233335     |North-245631|2022-11-02 06:00:00|22.49671|114.12824|42.5|15.4|11.3 |1.8|18.0          |80.0                |22.8          |18.0              |1005.4          |0.5          |\n",
      "|233335     |North-245631|2022-11-03 15:00:00|22.49671|114.12824|20.4|10.6|6.7  |0.9|22.2          |91.0                |16.2          |102.0             |1010.5          |3.3          |\n",
      "|233335     |North-245631|2022-11-04 16:00:00|22.49671|114.12824|21.3|24.6|17.1 |2.3|23.0          |87.0                |12.2          |56.0              |1013.9          |0.1          |\n",
      "|233335     |North-245631|2022-11-05 11:00:00|22.49671|114.12824|50.2|27.0|14.1 |1.9|21.1          |78.0                |13.1          |32.0              |1019.0          |0.0          |\n",
      "|233335     |North-245631|2022-11-06 13:00:00|22.49671|114.12824|37.3|17.8|14.5 |1.2|20.4          |82.0                |10.8          |37.0              |1017.3          |0.4          |\n",
      "|233335     |North-245631|2022-11-06 15:00:00|22.49671|114.12824|35.8|18.7|14.1 |1.3|19.8          |86.0                |10.2          |42.0              |1015.6          |0.7          |\n",
      "|233335     |North-245631|2022-11-06 03:00:00|22.49671|114.12824|27.4|NULL|NULL |1.6|18.5          |87.0                |11.2          |48.0              |1017.0          |0.0          |\n",
      "|233335     |North-245631|2022-11-06 19:00:00|22.49671|114.12824|23.6|21.2|17.2 |0.9|19.1          |91.0                |11.5          |46.0              |1015.9          |0.0          |\n",
      "|233335     |North-245631|2022-11-08 07:00:00|22.49671|114.12824|37.5|17.3|15.9 |2.6|19.5          |92.0                |9.7           |39.0              |1016.1          |0.2          |\n",
      "|233335     |North-245631|2022-11-08 05:00:00|22.49671|114.12824|35.6|20.8|19.4 |2.3|19.5          |93.0                |8.8           |35.0              |1014.8          |0.0          |\n",
      "+-----------+------------+-------------------+--------+---------+----+----+-----+---+--------------+--------------------+--------------+------------------+----------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# G·ªôp t·∫•t c·∫£ locations l·∫°i\n",
    "print(f\"[PROCESSING] Combining {len(all_locations_data)} locations...\")\n",
    "\n",
    "df_combined = all_locations_data[0]\n",
    "for df in all_locations_data[1:]:\n",
    "    df_combined = df_combined.union(df)\n",
    "\n",
    "# OPTIMIZE: Cache ƒë·ªÉ tr√°nh recompute nhi·ªÅu l·∫ßn\n",
    "df_combined = df_combined.cache()\n",
    "\n",
    "# OPTIMIZE: Trigger action 1 l·∫ßn, tr√°nh count() nhi·ªÅu l·∫ßn\n",
    "print(\"‚è≥ Computing combined dataset (this may take a moment)...\")\n",
    "total_records = df_combined.count()\n",
    "num_locations = df_combined.select('location_id').distinct().count()\n",
    "\n",
    "print(f\"[SUCCESS] Combined dataset: {total_records:,} total records\")\n",
    "print(f\"[SUCCESS] Number of locations: {num_locations}\")\n",
    "\n",
    "# OPTIMIZE: Ch·ªâ show sample, kh√¥ng orderBy to√†n b·ªô dataset (r·∫•t ch·∫≠m!)\n",
    "print(\"\\n[METADATA] Sample records (unsorted):\")\n",
    "df_combined.show(10, truncate=False)\n",
    "\n",
    "# OPTIONAL: N·∫øu c·∫ßn sort, ch·ªâ sort 1 partition nh·ªè ƒë·ªÉ xem\n",
    "# df_combined.orderBy(\"location_id\", \"datetime\").limit(50).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022358,
     "end_time": "2025-11-20T10:29:39.637407",
     "exception": false,
     "start_time": "2025-11-20T10:29:39.615049",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. T·ªïng quan Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:05:06.003411Z",
     "iopub.status.busy": "2025-12-01T16:05:06.003074Z",
     "iopub.status.idle": "2025-12-01T16:05:12.751897Z",
     "shell.execute_reply": "2025-12-01T16:05:12.748757Z",
     "shell.execute_reply.started": "2025-12-01T16:05:06.003378Z"
    },
    "papermill": {
     "duration": 5.649894,
     "end_time": "2025-11-20T10:29:45.309716",
     "exception": false,
     "start_time": "2025-11-20T10:29:39.659822",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATA] Dataset Overview by Location:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----+\n",
      "|location_id|location            |count|\n",
      "+-----------+--------------------+-----+\n",
      "|233335     |North-245631        |21679|\n",
      "|233336     |Southern-245632     |21255|\n",
      "|7727       |Tung Chung-7727     |22360|\n",
      "|7728       |Mong Kok-7728       |21676|\n",
      "|7730       |Central/Western-7730|21267|\n",
      "|7732       |Causeway Bay-7732   |21299|\n",
      "|7733       |Sha Tin-7733        |21228|\n",
      "|7734       |Sham Shui Po-7734   |21247|\n",
      "|7735       |Kwun Tong-7735      |21270|\n",
      "|7736       |Kwai Chung-7736     |21274|\n",
      "|7737       |Tai Po-7737         |21268|\n",
      "|7739       |Yuen Long-7739      |21721|\n",
      "|7740       |Tsuen Wan-7740      |21687|\n",
      "|7742       |Tuen Mun-7742       |2368 |\n",
      "|7742       |Tuen Mun-932161     |18719|\n",
      "+-----------+--------------------+-----+\n",
      "\n",
      "\n",
      "[?] Time Range by Location:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 644:=================================================>   (104 + 4) / 112]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-------------------+-------+\n",
      "|location_id|start_date         |end_date           |records|\n",
      "+-----------+-------------------+-------------------+-------+\n",
      "|233335     |2022-11-01 00:00:00|2025-09-30 16:00:00|21679  |\n",
      "|233336     |2022-11-01 00:00:00|2025-09-30 16:00:00|21255  |\n",
      "|7727       |2022-11-01 00:00:00|2025-09-30 16:00:00|22360  |\n",
      "|7728       |2022-11-01 00:00:00|2025-09-30 16:00:00|21676  |\n",
      "|7730       |2022-11-01 00:00:00|2025-09-30 16:00:00|21267  |\n",
      "|7732       |2022-11-01 00:00:00|2025-09-30 16:00:00|21299  |\n",
      "|7733       |2022-11-01 00:00:00|2025-09-30 16:00:00|21228  |\n",
      "|7734       |2022-11-01 00:00:00|2025-09-30 16:00:00|21247  |\n",
      "|7735       |2022-11-01 00:00:00|2025-09-30 16:00:00|21270  |\n",
      "|7736       |2022-11-01 00:00:00|2025-09-30 16:00:00|21274  |\n",
      "|7737       |2022-11-01 00:00:00|2025-09-30 16:00:00|21268  |\n",
      "|7739       |2022-11-01 00:00:00|2025-09-30 16:00:00|21721  |\n",
      "|7740       |2022-11-01 00:00:00|2025-09-30 16:00:00|21687  |\n",
      "|7742       |2022-11-01 00:00:00|2025-09-30 16:00:00|21087  |\n",
      "+-----------+-------------------+-------------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Th·ªëng k√™ theo location\n",
    "print(\"[DATA] Dataset Overview by Location:\")\n",
    "df_combined.groupBy(\"location_id\", \"location\").count().orderBy(\"location_id\").show(truncate=False)\n",
    "\n",
    "# Time range c·ªßa t·ª´ng location\n",
    "print(\"\\n[?] Time Range by Location:\")\n",
    "df_combined.groupBy(\"location_id\").agg(\n",
    "    F.min(\"datetime\").alias(\"start_date\"),\n",
    "    F.max(\"datetime\").alias(\"end_date\"),\n",
    "    F.count(\"*\").alias(\"records\")\n",
    ").orderBy(\"location_id\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:05:12.757772Z",
     "iopub.status.busy": "2025-12-01T16:05:12.756736Z",
     "iopub.status.idle": "2025-12-01T16:06:10.724292Z",
     "shell.execute_reply": "2025-12-01T16:06:10.723499Z",
     "shell.execute_reply.started": "2025-12-01T16:05:12.757736Z"
    },
    "papermill": {
     "duration": 45.938252,
     "end_time": "2025-11-20T10:30:31.304069",
     "exception": false,
     "start_time": "2025-11-20T10:29:45.365817",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING]  Missing Values Summary:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NO2                      :    7,612 (  2.53%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PM10                     :    3,391 (  1.13%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PM2_5                    :   11,161 (  3.72%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  SO2                      :    7,424 (  2.47%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Ki·ªÉm tra missing values\n",
    "print(\"[WARNING]  Missing Values Summary:\")\n",
    "for col_name in df_combined.columns:\n",
    "    null_count = df_combined.filter(F.col(col_name).isNull()).count()\n",
    "    total = df_combined.count()\n",
    "    pct = (null_count / total) * 100\n",
    "    if null_count > 0:  # Ch·ªâ hi·ªÉn th·ªã c·ªôt c√≥ missing\n",
    "        print(f\"  {col_name:25s}: {null_count:8,} ({pct:6.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:06:10.725971Z",
     "iopub.status.busy": "2025-12-01T16:06:10.725613Z",
     "iopub.status.idle": "2025-12-01T16:06:13.970485Z",
     "shell.execute_reply": "2025-12-01T16:06:13.967622Z",
     "shell.execute_reply.started": "2025-12-01T16:06:10.725944Z"
    },
    "papermill": {
     "duration": 2.68063,
     "end_time": "2025-11-20T10:30:34.017878",
     "exception": false,
     "start_time": "2025-11-20T10:30:31.337248",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[?] Overall Statistics:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/01 16:06:11 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 2473:===============================================>    (103 + 4) / 112]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+-----------------+--------------------+------------------+------------------+\n",
      "|summary|             PM2_5|              PM10|               NO2|               SO2|   temperature_2m|relative_humidity_2m|    wind_speed_10m|     precipitation|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+--------------------+------------------+------------------+\n",
      "|  count|            289157|            296927|            292706|            292894|           300318|              300318|            300318|            300318|\n",
      "|   mean|15.684073703904803|25.418589417600963|  39.2034498780346|3.7197607325517086|23.17203064751364|   79.55055974000892|12.503190950925351|0.2767030281235236|\n",
      "| stddev|10.897151413820698| 19.75489546779464|26.134835003178324|2.4372186299768455|5.550822884231528|   15.46350485998542| 6.256373957792993|1.1696306615920375|\n",
      "|    min|               0.0|               0.0|               0.0|               0.0|              1.9|                16.0|               0.0|               0.0|\n",
      "|    max|             182.5|             401.7|             292.6|              76.9|             36.5|               100.0|              86.6|              53.2|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+--------------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Statistics t·ªïng quan\n",
    "print(\"[?] Overall Statistics:\")\n",
    "df_combined.select(\n",
    "    \"PM2_5\", \"PM10\", \"NO2\", \"SO2\",\n",
    "    \"temperature_2m\", \"relative_humidity_2m\", \"wind_speed_10m\", \"precipitation\"\n",
    ").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030872,
     "end_time": "2025-11-20T10:30:34.087938",
     "exception": false,
     "start_time": "2025-11-20T10:30:34.057066",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. L√†m s·∫°ch D·ªØ li·ªáu\n",
    "\n",
    "**Quy tr√¨nh l√†m s·∫°ch:**\n",
    "1. **Lo·∫°i b·ªè Outliers tr∆∞·ªõc** - ƒê·ªÉ tr√°nh gi√° tr·ªã c·ª±c ƒëoan ·∫£nh h∆∞·ªüng ƒë·∫øn t√≠nh to√°n statistics\n",
    "2. **Fill Missing Values sau** - Imputation d·ª±a tr√™n d·ªØ li·ªáu ƒë√£ lo·∫°i b·ªè outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03182,
     "end_time": "2025-11-20T10:30:34.151425",
     "exception": false,
     "start_time": "2025-11-20T10:30:34.119605",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.1. Lo·∫°i b·ªè Outliers\n",
    "\n",
    "Lo·∫°i b·ªè c√°c gi√° tr·ªã c·ª±c ƒëoan tr∆∞·ªõc khi imputation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:06:13.971651Z",
     "iopub.status.busy": "2025-12-01T16:06:13.971345Z",
     "iopub.status.idle": "2025-12-01T16:06:37.909564Z",
     "shell.execute_reply": "2025-12-01T16:06:37.908877Z",
     "shell.execute_reply.started": "2025-12-01T16:06:13.971624Z"
    },
    "papermill": {
     "duration": 18.170165,
     "end_time": "2025-11-20T10:30:52.353326",
     "exception": false,
     "start_time": "2025-11-20T10:30:34.183161",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATA] Outlier Removal:\n",
      "  Before: 300,318 records\n",
      "  After:  289,157 records\n",
      "  Removed: 11,161 records (3.72%)\n",
      "\n",
      "  [WARNING]  Note: Records with PM2.5 = null are REMOVED (target variable must have real values)\n",
      "\n",
      "[WARNING]  Missing values after outlier removal:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PM2_5     :        0 (  0.00%) [SUCCESS] (Target - must be 0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PM10      :      296 (  0.10%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NO2       :    7,361 (  2.55%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3063:==============================================>      (99 + 4) / 112]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  SO2       :    7,178 (  2.48%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lo·∫°i b·ªè outliers theo WHO/EPA International Standards (cho d·ªØ li·ªáu Hong Kong)\n",
    "# [WARNING]  QUAN TR·ªåNG: PM2.5 l√† TARGET variable - PH·∫¢I c√≥ gi√° tr·ªã th·∫≠t!\n",
    "#     -> Records c√≥ PM2.5 = null s·∫Ω B·ªä LO·∫†I B·ªé\n",
    "#     -> Ch·ªâ c√°c features kh√°c (PM10, NO2, SO2) m·ªõi ƒë∆∞·ª£c ph√©p null v√† impute sau\n",
    "\n",
    "df_no_outliers = df_combined.filter(\n",
    "    # [TARGET] TARGET: PM2.5 theo WHO Emergency threshold (kh√¥ng cho ph√©p null)\n",
    "    (F.col(\"PM2_5\").isNotNull()) & \n",
    "    (F.col(\"PM2_5\") >= 0) & (F.col(\"PM2_5\") < 250) &  # WHO Emergency: 250 Œºg/m¬≥\n",
    "    \n",
    "    # [DATA] FEATURES: WHO/EPA International Standards - Cho ph√©p null, ch·ªâ lo·∫°i outliers\n",
    "    ((F.col(\"PM10\").isNull()) | ((F.col(\"PM10\") >= 0) & (F.col(\"PM10\") < 430))) &  # WHO Emergency: 430 Œºg/m¬≥\n",
    "    ((F.col(\"NO2\").isNull()) | ((F.col(\"NO2\") >= 0) & (F.col(\"NO2\") < 400))) &     # WHO/EU: 400 Œºg/m¬≥ (1-hour)\n",
    "    ((F.col(\"SO2\").isNull()) | ((F.col(\"SO2\") >= 0) & (F.col(\"SO2\") < 500))) &     # WHO/EU: 500 Œºg/m¬≥ (10-min)\n",
    "    \n",
    "    # [?] WEATHER: WMO standards cho Hong Kong\n",
    "    (F.col(\"precipitation\") >= 0) & (F.col(\"precipitation\") < 100)  # WMO: 100mm/h extreme rain\n",
    ")\n",
    "\n",
    "records_before = df_combined.count()\n",
    "records_after = df_no_outliers.count()\n",
    "removed = records_before - records_after\n",
    "\n",
    "print(f\"[DATA] Outlier Removal:\")\n",
    "print(f\"  Before: {records_before:,} records\")\n",
    "print(f\"  After:  {records_after:,} records\")\n",
    "print(f\"  Removed: {removed:,} records ({removed/records_before*100:.2f}%)\")\n",
    "print(f\"\\n  [WARNING]  Note: Records with PM2.5 = null are REMOVED (target variable must have real values)\")\n",
    "\n",
    "# Ki·ªÉm tra missing values sau khi lo·∫°i outliers\n",
    "print(\"\\n[WARNING]  Missing values after outlier removal:\")\n",
    "for col_name in [\"PM2_5\", \"PM10\", \"NO2\", \"SO2\"]:\n",
    "    if col_name in df_no_outliers.columns:\n",
    "        null_count = df_no_outliers.filter(F.col(col_name).isNull()).count()\n",
    "        total = df_no_outliers.count()\n",
    "        pct = (null_count / total) * 100\n",
    "        if null_count > 0:\n",
    "            print(f\"  {col_name:10s}: {null_count:8,} ({pct:6.2f}%)\")\n",
    "        elif col_name == \"PM2_5\":\n",
    "            print(f\"  {col_name:10s}: {null_count:8,} ({pct:6.2f}%) [SUCCESS] (Target - must be 0%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019614,
     "end_time": "2025-11-20T10:30:52.403875",
     "exception": false,
     "start_time": "2025-11-20T10:30:52.384261",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.2. X·ª≠ l√Ω Missing Values (Interpolation)\n",
    "\n",
    "**Chi·∫øn l∆∞·ª£c Imputation cho Time Series:**\n",
    "- **PM2.5**: ƒê√£ lo·∫°i b·ªè t·∫•t c·∫£ records c√≥ null (target variable)\n",
    "- **PM10, NO2, SO2**: S·ª≠ d·ª•ng **Linear Interpolation** (t·ªët nh·∫•t cho time series)\n",
    "  - B∆∞·ªõc 1: **Linear Interpolation** - N·ªôi suy tuy·∫øn t√≠nh d·ª±a tr√™n gi√° tr·ªã tr∆∞·ªõc & sau\n",
    "  - B∆∞·ªõc 2: **Forward Fill** - X·ª≠ l√Ω missing ·ªü cu·ªëi chu·ªói (kh√¥ng c√≥ gi√° tr·ªã sau)\n",
    "  - B∆∞·ªõc 3: **Backward Fill** - X·ª≠ l√Ω missing ·ªü ƒë·∫ßu chu·ªói (kh√¥ng c√≥ gi√° tr·ªã tr∆∞·ªõc)\n",
    "  - B∆∞·ªõc 4: **Mean** - Backup cu·ªëi c√πng (n·∫øu c√≤n missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:06:37.910587Z",
     "iopub.status.busy": "2025-12-01T16:06:37.910318Z",
     "iopub.status.idle": "2025-12-01T16:06:52.535261Z",
     "shell.execute_reply": "2025-12-01T16:06:52.533745Z",
     "shell.execute_reply.started": "2025-12-01T16:06:37.910565Z"
    },
    "papermill": {
     "duration": 11.540306,
     "end_time": "2025-11-20T10:31:03.963703",
     "exception": false,
     "start_time": "2025-11-20T10:30:52.423397",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROCESSING] Time Series Imputation Strategy (PySpark Native):\n",
      "   1. True Linear Interpolation - y = y‚ÇÅ + (y‚ÇÇ-y‚ÇÅ) √ó (t-t‚ÇÅ)/(t‚ÇÇ-t‚ÇÅ)\n",
      "   2. Forward Fill - If only prev value available\n",
      "   3. Backward Fill - If only next value available\n",
      "   4. Null - If no surrounding values (rare)\n",
      "\n",
      "   Columns to impute: ['PM10', 'NO2', 'SO2']\n",
      "   PM2.5 NOT imputed (target variable - already removed nulls)\n",
      "   [?] Safe: Window partitioned by location_id (no cross-location interpolation)\n",
      "\n",
      "[WARNING]  Missing values BEFORE interpolation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PM10      :      296 (  0.10%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NO2       :    7,361 (  2.55%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3446:===========================================>         (92 + 5) / 112]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  SO2       :    7,178 (  2.48%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Chi·∫øn l∆∞·ª£c Imputation cho Time Series Data\n",
    "# S·ª≠ d·ª•ng PySpark Window Functions - N·ªôi suy tuy·∫øn t√≠nh d·ª±a tr√™n kho·∫£ng c√°ch th·ªùi gian\n",
    "\n",
    "# List c√°c c·ªôt FEATURES c·∫ßn impute (KH√îNG bao g·ªìm PM2.5 - target variable)\n",
    "pollutant_cols = [\"PM10\", \"NO2\", \"SO2\"]  # [WARNING] Kh√¥ng c√≥ PM2.5!\n",
    "\n",
    "print(f\"[PROCESSING] Time Series Imputation Strategy (PySpark Native):\")\n",
    "print(f\"   1. True Linear Interpolation - y = y‚ÇÅ + (y‚ÇÇ-y‚ÇÅ) √ó (t-t‚ÇÅ)/(t‚ÇÇ-t‚ÇÅ)\")\n",
    "print(f\"   2. Forward Fill - If only prev value available\")\n",
    "print(f\"   3. Backward Fill - If only next value available\")\n",
    "print(f\"   4. Null - If no surrounding values (rare)\")\n",
    "print(f\"\\n   Columns to impute: {pollutant_cols}\")\n",
    "print(f\"   PM2.5 NOT imputed (target variable - already removed nulls)\")\n",
    "print(f\"   [?] Safe: Window partitioned by location_id (no cross-location interpolation)\\n\")\n",
    "\n",
    "# Cache ƒë·ªÉ tƒÉng performance\n",
    "df_filled = df_no_outliers.cache()\n",
    "\n",
    "# Ki·ªÉm tra missing TR∆Ø·ªöC khi interpolate\n",
    "print(\"[WARNING]  Missing values BEFORE interpolation:\")\n",
    "for col_name in pollutant_cols:\n",
    "    if col_name in df_filled.columns:\n",
    "        null_count = df_filled.filter(F.col(col_name).isNull()).count()\n",
    "        total = df_filled.count()\n",
    "        pct = (null_count / total) * 100\n",
    "        if null_count > 0:\n",
    "            print(f\"  {col_name:10s}: {null_count:8,} ({pct:6.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:06:52.536547Z",
     "iopub.status.busy": "2025-12-01T16:06:52.536150Z",
     "iopub.status.idle": "2025-12-01T16:15:38.790623Z",
     "shell.execute_reply": "2025-12-01T16:15:38.789169Z",
     "shell.execute_reply.started": "2025-12-01T16:06:52.536522Z"
    },
    "papermill": {
     "duration": 512.335766,
     "end_time": "2025-11-20T10:39:36.325299",
     "exception": false,
     "start_time": "2025-11-20T10:31:03.989533",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROCESSING] Applying true linear interpolation per location (PySpark native)...\n",
      "  ‚ñ∂ Interpolating PM10... [OK]\n",
      "  ‚ñ∂ Interpolating NO2... [OK]\n",
      "  ‚ñ∂ Interpolating SO2... [OK]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3535:=================================================>      (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUCCESS] Linear interpolation completed! Total records: 289,157\n",
      "   [GEAR]  Method: True linear interpolation based on time distance (epoch)\n",
      "   [?] Safe: No cross-location interpolation (partitioned by location_id)\n",
      "   [RUN] Optimized: Native PySpark (no Pandas conversion)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# √Åp d·ª•ng True Linear Interpolation v·ªõi PySpark (kh√¥ng d√πng Pandas)\n",
    "# N·ªôi suy tuy·∫øn t√≠nh d·ª±a tr√™n kho·∫£ng c√°ch th·ªùi gian TH·ª∞C (epoch)\n",
    "# Window function ƒë·∫£m b·∫£o KH√îNG n·ªôi suy ch√©o gi·ªØa c√°c locations\n",
    "\n",
    "print(\"[PROCESSING] Applying true linear interpolation per location (PySpark native)...\")\n",
    "\n",
    "# T·∫°o c·ªôt epoch (timestamp d·∫°ng s·ªë) ƒë·ªÉ t√≠nh to√°n kho·∫£ng c√°ch th·ªùi gian\n",
    "df_filled = df_filled.withColumn(\"epoch\", F.col(\"datetime\").cast(\"long\"))\n",
    "\n",
    "# ƒê·ªãnh nghƒ©a Window cho t·ª´ng location\n",
    "w_forward = (\n",
    "    Window.partitionBy(\"location_id\")\n",
    "    .orderBy(\"epoch\")\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    ")\n",
    "\n",
    "w_backward = (\n",
    "    Window.partitionBy(\"location_id\")\n",
    "    .orderBy(\"epoch\")\n",
    "    .rowsBetween(Window.currentRow, Window.unboundedFollowing)\n",
    ")\n",
    "\n",
    "# X·ª≠ l√Ω t·ª´ng pollutant column\n",
    "for col_name in pollutant_cols:\n",
    "    if col_name not in df_filled.columns:\n",
    "        continue\n",
    "    \n",
    "    print(f\"  ‚ñ∂ Interpolating {col_name}...\", end=\" \", flush=True)\n",
    "    \n",
    "    # B∆∞·ªõc 1: T√¨m gi√° tr·ªã & timestamp TR∆Ø·ªöC v√† SAU g·∫ßn nh·∫•t (c√≥ gi√° tr·ªã non-null)\n",
    "    df_filled = (\n",
    "        df_filled\n",
    "        .withColumn(f\"{col_name}_prev_value\", F.last(col_name, True).over(w_forward))\n",
    "        .withColumn(f\"{col_name}_next_value\", F.first(col_name, True).over(w_backward))\n",
    "        .withColumn(f\"{col_name}_prev_time\", F.last(F.when(F.col(col_name).isNotNull(), F.col(\"epoch\")), True).over(w_forward))\n",
    "        .withColumn(f\"{col_name}_next_time\", F.first(F.when(F.col(col_name).isNotNull(), F.col(\"epoch\")), True).over(w_backward))\n",
    "    )\n",
    "    \n",
    "    # B∆∞·ªõc 2: T√≠nh to√°n Linear Interpolation theo c√¥ng th·ª©c:\n",
    "    # y = y‚ÇÅ + (y‚ÇÇ - y‚ÇÅ) * (t - t‚ÇÅ) / (t‚ÇÇ - t‚ÇÅ)\n",
    "    interpolated_value = (\n",
    "        F.col(f\"{col_name}_prev_value\") +\n",
    "        (F.col(f\"{col_name}_next_value\") - F.col(f\"{col_name}_prev_value\")) *\n",
    "        ((F.col(\"epoch\") - F.col(f\"{col_name}_prev_time\")) /\n",
    "         (F.col(f\"{col_name}_next_time\") - F.col(f\"{col_name}_prev_time\")))\n",
    "    )\n",
    "    \n",
    "    # B∆∞·ªõc 3: Logic ch·ªçn gi√° tr·ªã cu·ªëi c√πng v·ªõi fallback\n",
    "    df_filled = df_filled.withColumn(\n",
    "        col_name,\n",
    "        F.when(F.col(col_name).isNotNull(), F.col(col_name))  # Gi·ªØ nguy√™n n·∫øu c√≥ gi√° tr·ªã\n",
    "         .when(\n",
    "             # Linear interpolation n·∫øu c√≥ c·∫£ prev & next v√† kh√¥ng chia 0\n",
    "             (F.col(f\"{col_name}_prev_value\").isNotNull()) &\n",
    "             (F.col(f\"{col_name}_next_value\").isNotNull()) &\n",
    "             ((F.col(f\"{col_name}_next_time\") - F.col(f\"{col_name}_prev_time\")) != 0),\n",
    "             interpolated_value\n",
    "         )\n",
    "         .when(F.col(f\"{col_name}_prev_value\").isNotNull(), F.col(f\"{col_name}_prev_value\"))  # Forward fill\n",
    "         .when(F.col(f\"{col_name}_next_value\").isNotNull(), F.col(f\"{col_name}_next_value\"))  # Backward fill\n",
    "         .otherwise(None)  # V·∫´n null n·∫øu kh√¥ng c√≥ data n√†o\n",
    "    )\n",
    "    \n",
    "    # B∆∞·ªõc 4: X√≥a c√°c c·ªôt ph·ª• ƒë·ªÉ gi·∫£m memory\n",
    "    df_filled = df_filled.drop(\n",
    "        f\"{col_name}_prev_value\", f\"{col_name}_next_value\",\n",
    "        f\"{col_name}_prev_time\", f\"{col_name}_next_time\"\n",
    "    )\n",
    "    \n",
    "    print(\"[OK]\")\n",
    "\n",
    "# Cache k·∫øt qu·∫£ sau khi interpolation\n",
    "df_filled = df_filled.cache()\n",
    "\n",
    "# Trigger computation v√† ƒë·∫øm records\n",
    "count = df_filled.count()\n",
    "print(f\"\\n[SUCCESS] Linear interpolation completed! Total records: {count:,}\")\n",
    "print(f\"   [GEAR]  Method: True linear interpolation based on time distance (epoch)\")\n",
    "print(f\"   [?] Safe: No cross-location interpolation (partitioned by location_id)\")\n",
    "print(f\"   [RUN] Optimized: Native PySpark (no Pandas conversion)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:15:38.791962Z",
     "iopub.status.busy": "2025-12-01T16:15:38.791655Z",
     "iopub.status.idle": "2025-12-01T16:15:44.484887Z",
     "shell.execute_reply": "2025-12-01T16:15:44.483815Z",
     "shell.execute_reply.started": "2025-12-01T16:15:38.791934Z"
    },
    "papermill": {
     "duration": 4.847058,
     "end_time": "2025-11-20T10:39:41.196521",
     "exception": false,
     "start_time": "2025-11-20T10:39:36.349463",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[METADATA] Final Missing Values Check (After Interpolation):\n",
      "  PM2_5 (Target): 0 nulls [SUCCESS] (Must be 0)\n",
      "  PM10      : 0 nulls [SUCCESS]\n",
      "  NO2       : 0 nulls [SUCCESS]\n",
      "  SO2       : 0 nulls [SUCCESS]\n",
      "\n",
      "  [SUCCESS] No missing values in any feature columns!\n",
      "\n",
      "[DATA] Final Verification:\n",
      "  PM2_5     : 0 nulls [SUCCESS]\n",
      "  PM10      : 0 nulls [SUCCESS]\n",
      "  NO2       : 0 nulls [SUCCESS]\n",
      "  SO2       : 0 nulls [SUCCESS]\n",
      "\n",
      "[SUCCESS] Data cleaning completed with True Linear Interpolation!\n",
      "   Final dataset: 289,157 records\n",
      "   [WARNING]  All records have REAL PM2.5 values (target variable)\n",
      "   [SUCCESS] Features interpolated smoothly (time-based linear interpolation)\n",
      "   [SUCCESS] Edge cases (no surrounding data) removed\n",
      "   [RUN] Performance: Native PySpark (no Pandas conversion)\n"
     ]
    }
   ],
   "source": [
    "# Verify: PM2.5 kh√¥ng c√≥ null, c√°c features kh√°c kh√¥ng c√≥ null\n",
    "print(\"\\n[METADATA] Final Missing Values Check (After Interpolation):\")\n",
    "\n",
    "# Ki·ªÉm tra PM2.5 (target)\n",
    "pm25_nulls = df_filled.filter(F.col(\"PM2_5\").isNull()).count()\n",
    "print(f\"  PM2_5 (Target): {pm25_nulls:,} nulls [SUCCESS] (Must be 0)\")\n",
    "\n",
    "# Ki·ªÉm tra features\n",
    "total_nulls = 0\n",
    "for col_name in pollutant_cols:\n",
    "    if col_name in df_filled.columns:\n",
    "        null_count = df_filled.filter(F.col(col_name).isNull()).count()\n",
    "        total_nulls += null_count\n",
    "        if null_count > 0:\n",
    "            print(f\"  {col_name:10s}: {null_count:,} nulls [WARNING]\")\n",
    "        else:\n",
    "            print(f\"  {col_name:10s}: {null_count:,} nulls [SUCCESS]\")\n",
    "\n",
    "# X·ª≠ l√Ω edge case: Drop records c√≤n null (kh√¥ng c√≥ gi√° tr·ªã xung quanh ƒë·ªÉ interpolate)\n",
    "if total_nulls > 0:\n",
    "    print(f\"\\n[WARNING]  Found {total_nulls} remaining nulls (edge cases with no surrounding data)\")\n",
    "    print(f\"   -> Dropping these records to ensure data quality...\")\n",
    "    \n",
    "    records_before_drop = df_filled.count()\n",
    "    \n",
    "    # Drop records c√≥ b·∫•t k·ª≥ feature n√†o c√≤n null\n",
    "    for col_name in pollutant_cols:\n",
    "        df_filled = df_filled.filter(F.col(col_name).isNotNull())\n",
    "    \n",
    "    records_after_drop = df_filled.count()\n",
    "    dropped = records_before_drop - records_after_drop\n",
    "    \n",
    "    print(f\"   Before drop: {records_before_drop:,} records\")\n",
    "    print(f\"   After drop:  {records_after_drop:,} records\")\n",
    "    print(f\"   Dropped:     {dropped:,} records ({dropped/records_before_drop*100:.2f}%)\")\n",
    "    print(f\"\\n   [SUCCESS] All feature columns now have 0 nulls!\")\n",
    "else:\n",
    "    print(\"\\n  [SUCCESS] No missing values in any feature columns!\")\n",
    "\n",
    "# X√≥a c·ªôt epoch (ƒë√£ d√πng xong)\n",
    "df_filled = df_filled.drop(\"epoch\")\n",
    "\n",
    "# Verify l·∫ßn cu·ªëi\n",
    "print(f\"\\n[DATA] Final Verification:\")\n",
    "for col_name in [\"PM2_5\"] + pollutant_cols:\n",
    "    if col_name in df_filled.columns:\n",
    "        null_count = df_filled.filter(F.col(col_name).isNull()).count()\n",
    "        print(f\"  {col_name:10s}: {null_count:,} nulls [SUCCESS]\")\n",
    "\n",
    "print(f\"\\n[SUCCESS] Data cleaning completed with True Linear Interpolation!\")\n",
    "print(f\"   Final dataset: {df_filled.count():,} records\")\n",
    "print(f\"   [WARNING]  All records have REAL PM2.5 values (target variable)\")\n",
    "print(f\"   [SUCCESS] Features interpolated smoothly (time-based linear interpolation)\")\n",
    "print(f\"   [SUCCESS] Edge cases (no surrounding data) removed\")\n",
    "print(f\"   [RUN] Performance: Native PySpark (no Pandas conversion)\")\n",
    "\n",
    "# C·∫≠p nh·∫≠t df_combined v·ªõi d·ªØ li·ªáu ƒë√£ clean v√† s·∫Øp x·∫øp\n",
    "df_combined = df_filled.orderBy(\"location_id\", \"datetime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.033824,
     "end_time": "2025-11-20T10:39:41.253233",
     "exception": false,
     "start_time": "2025-11-20T10:39:41.219409",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Feature Engineering & Normalization\n",
    "\n",
    "**Quy tr√¨nh ƒê√öNG ƒë·ªÉ tr√°nh Data Leakage:**\n",
    "1. **Time Features** - Th√™m cyclic encoding (sin/cos) v√† is_weekend (kh√¥ng c·∫ßn normalize)\n",
    "2. **Temporal Split** - Chia train/validation/test theo th·ªùi gian (70/15/15)\n",
    "3. **Normalization** - Chu·∫©n h√≥a **CH·ªà numerical G·ªêC** b·∫±ng Min-Max t·ª´ train set\n",
    "4. **Lag Features** - T·∫°o lag T·ª™ C√ÅC C·ªòT ƒê√É SCALE (gi·ªØ ƒë√∫ng scale relationship)\n",
    "5. **Model-Specific Datasets** - Chu·∫©n b·ªã ri√™ng cho Deep Learning v√† XGBoost\n",
    "6. **Null Handling** - X·ª≠ l√Ω nulls trong lag features cu·ªëi c√πng\n",
    "\n",
    "**[WARNING] QUAN TR·ªåNG:** Lag features ph·∫£i t·∫°o SAU khi normalize ƒë·ªÉ gi·ªØ ƒë√∫ng m·ªëi quan h·ªá scale!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:15:44.487785Z",
     "iopub.status.busy": "2025-12-01T16:15:44.486289Z",
     "iopub.status.idle": "2025-12-01T16:15:45.671576Z",
     "shell.execute_reply": "2025-12-01T16:15:45.669840Z",
     "shell.execute_reply.started": "2025-12-01T16:15:44.487752Z"
    },
    "papermill": {
     "duration": 0.953853,
     "end_time": "2025-11-20T10:39:42.236159",
     "exception": false,
     "start_time": "2025-11-20T10:39:41.282306",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROCESSING] Step 1: Adding Time Features (No normalization needed)...\n",
      "[OK] Time features added successfully!\n",
      "[OK] Total records: 289,157\n",
      "[OK] Total columns: 23\n",
      "\n",
      "[METADATA] Time Features Created:\n",
      "  Cyclic (sin/cos): hour, month, day_of_week -> Already in [-1, 1]\n",
      "  Binary: is_weekend -> Already in [0, 1]\n",
      "  [SUCCESS] No normalization needed for time features!\n"
     ]
    }
   ],
   "source": [
    "# B∆∞·ªõc 1: Th√™m Time Features t·ª´ d·ªØ li·ªáu ƒë√£ clean\n",
    "print(\"[PROCESSING] Step 1: Adding Time Features (No normalization needed)...\")\n",
    "\n",
    "import math\n",
    "\n",
    "df_features = df_combined \\\n",
    "    .withColumn(\"hour\", F.hour(\"datetime\")) \\\n",
    "    .withColumn(\"month\", F.month(\"datetime\")) \\\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(\"datetime\"))\n",
    "\n",
    "# Cyclic encoding cho hour (24h cycle)\n",
    "df_features = df_features \\\n",
    "    .withColumn(\"hour_sin\", F.sin(2 * math.pi * F.col(\"hour\") / 24)) \\\n",
    "    .withColumn(\"hour_cos\", F.cos(2 * math.pi * F.col(\"hour\") / 24))\n",
    "\n",
    "# Cyclic encoding cho month (12 month cycle)\n",
    "df_features = df_features \\\n",
    "    .withColumn(\"month_sin\", F.sin(2 * math.pi * F.col(\"month\") / 12)) \\\n",
    "    .withColumn(\"month_cos\", F.cos(2 * math.pi * F.col(\"month\") / 12))\n",
    "\n",
    "# Cyclic encoding cho day_of_week (7 day cycle)\n",
    "df_features = df_features \\\n",
    "    .withColumn(\"day_of_week_sin\", F.sin(2 * math.pi * F.col(\"day_of_week\") / 7)) \\\n",
    "    .withColumn(\"day_of_week_cos\", F.cos(2 * math.pi * F.col(\"day_of_week\") / 7))\n",
    "\n",
    "# Binary feature: is_weekend\n",
    "df_features = df_features \\\n",
    "    .withColumn(\"is_weekend\", F.when(F.col(\"day_of_week\").isin([1, 7]), 1).otherwise(0))\n",
    "\n",
    "# ‚úÖ FIX: Th√™m cyclic encoding cho wind_direction\n",
    "df_features = df_features \\\n",
    "    .withColumn(\"wind_direction_sin\", F.sin(2 * math.pi * F.col(\"wind_direction_10m\") / 360)) \\\n",
    "    .withColumn(\"wind_direction_cos\", F.cos(2 * math.pi * F.col(\"wind_direction_10m\") / 360))\n",
    "\n",
    "# X√≥a c√°c c·ªôt trung gian\n",
    "df_features = df_features.drop(\"hour\", \"month\", \"day_of_week\", \"wind_direction_10m\")\n",
    "\n",
    "print(\"[OK] Time features added successfully!\")\n",
    "print(f\"[OK] Total records: {df_features.count():,}\")\n",
    "print(f\"[OK] Total columns: {len(df_features.columns)}\")\n",
    "\n",
    "print(\"\\n[METADATA] Time Features Created:\")\n",
    "print(\"  Cyclic (sin/cos): hour, month, day_of_week -> Already in [-1, 1]\")\n",
    "print(\"  Binary: is_weekend -> Already in [0, 1]\")\n",
    "print(\"  [SUCCESS] No normalization needed for time features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:15:45.672736Z",
     "iopub.status.busy": "2025-12-01T16:15:45.672439Z",
     "iopub.status.idle": "2025-12-01T16:15:59.134301Z",
     "shell.execute_reply": "2025-12-01T16:15:59.132852Z",
     "shell.execute_reply.started": "2025-12-01T16:15:45.672711Z"
    },
    "papermill": {
     "duration": 2.987047,
     "end_time": "2025-11-20T10:39:45.262958",
     "exception": false,
     "start_time": "2025-11-20T10:39:42.275911",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PROCESSING] Step 2: STRATIFIED Temporal Train/Val/Test Split...\n",
      "======================================================================\n",
      "[STRATEGY] Stratified Temporal Split:\n",
      "   - Split by MONTH: Each month divided 70/15/15\n",
      "   - Ensures ALL seasons represented in train/val/test\n",
      "   - Maintains temporal order WITHIN each month\n",
      "   - Reduces distribution shift between splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DATA] Total months in dataset: 35\n",
      "   First month: 2022-11\n",
      "   Last month: 2025-09\n",
      "\n",
      "[PROCESSING] Applying stratified split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DATA] Stratified Split Results:\n",
      "  Train:  202,154 (69.9%)\n",
      "  Val:     43,397 (15.0%)\n",
      "  Test:    43,606 (15.1%)\n",
      "\n",
      "[VERIFY] Checking PM2.5 distribution across splits...\n",
      "   Train: mean=15.50, std=10.63\n",
      "   Val:   mean=15.19, std=9.74\n",
      "   Test:  mean=17.03, std=12.93\n",
      "\n",
      "[IMPROVEMENT] Distribution alignment:\n",
      "   Train-Val difference:  2.0% (target: < 10%)\n",
      "   Train-Test difference: 9.8% (target: < 10%)\n",
      "   [SUCCESS] Stratified split significantly reduced distribution shift!\n",
      "\n",
      "[SUCCESS] Stratified temporal split completed!\n",
      "   Each split now contains data from ALL seasons\n",
      "   Next: Normalize using TRAIN SET statistics ONLY\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# B∆∞·ªõc 2: STRATIFIED TEMPORAL SPLIT (Gi·∫£i quy·∫øt Distribution Shift)\n",
    "print(\"\\n[PROCESSING] Step 2: STRATIFIED Temporal Train/Val/Test Split...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ========================================\n",
    "# STRATIFIED TEMPORAL SPLIT\n",
    "# ========================================\n",
    "# V·∫•n ƒë·ªÅ v·ªõi Simple Temporal Split:\n",
    "# - Train: Nov 2022 - Nov 2024 (ch·ªß y·∫øu m√πa ƒë√¥ng/xu√¢n)\n",
    "# - Val: Nov 2024 - Apr 2025 (m√πa ƒë√¥ng - √¥ nhi·ªÖm CAO)\n",
    "# - Test: Apr 2025 - Sep 2025 (m√πa h√® - √¥ nhi·ªÖm TH·∫§P)\n",
    "# => Ph√¢n ph·ªëi KH√îNG ƒë·ªìng nh·∫•t gi·ªØa c√°c t·∫≠p\n",
    "#\n",
    "# Gi·∫£i ph√°p: Stratified Temporal Split\n",
    "# - Chia theo TH√ÅNG, m·ªói th√°ng chia 70/15/15\n",
    "# - ƒê·∫£m b·∫£o m·ªói t·∫≠p c√≥ ƒë·ªß d·ªØ li·ªáu t·ª´ T·∫§T C·∫¢ c√°c m√πa\n",
    "# - V·∫´n gi·ªØ temporal order trong t·ª´ng th√°ng (tr√°nh data leakage)\n",
    "\n",
    "print(\"[STRATEGY] Stratified Temporal Split:\")\n",
    "print(\"   - Split by MONTH: Each month divided 70/15/15\")\n",
    "print(\"   - Ensures ALL seasons represented in train/val/test\")\n",
    "print(\"   - Maintains temporal order WITHIN each month\")\n",
    "print(\"   - Reduces distribution shift between splits\")\n",
    "\n",
    "# Th√™m c·ªôt month ƒë·ªÉ stratify\n",
    "df_with_month = df_features.withColumn(\"split_month\", F.date_format(\"datetime\", \"yyyy-MM\"))\n",
    "\n",
    "# L·∫•y danh s√°ch c√°c th√°ng\n",
    "months = df_with_month.select(\"split_month\").distinct().orderBy(\"split_month\").collect()\n",
    "months = [row[\"split_month\"] for row in months]\n",
    "\n",
    "print(f\"\\n[DATA] Total months in dataset: {len(months)}\")\n",
    "print(f\"   First month: {months[0]}\")\n",
    "print(f\"   Last month: {months[-1]}\")\n",
    "\n",
    "# H√†m chia stratified cho m·ªói th√°ng\n",
    "def stratified_split_by_month(df, train_ratio=0.70, val_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Split data by month, ensuring each month contributes to all splits\n",
    "    Maintains temporal order within each month\n",
    "    \"\"\"\n",
    "    from pyspark.sql.window import Window\n",
    "    \n",
    "    # Window ƒë·ªÉ rank theo th·ªùi gian trong t·ª´ng th√°ng v√† location\n",
    "    w = Window.partitionBy(\"split_month\", \"location_id\").orderBy(\"datetime\")\n",
    "    \n",
    "    # Th√™m row number v√† t·ªïng s·ªë rows trong m·ªói group\n",
    "    df_ranked = df.withColumn(\"row_num\", F.row_number().over(w))\n",
    "    \n",
    "    # T√≠nh t·ªïng s·ªë records trong m·ªói th√°ng-location\n",
    "    w_count = Window.partitionBy(\"split_month\", \"location_id\")\n",
    "    df_ranked = df_ranked.withColumn(\"total_rows\", F.count(\"*\").over(w_count))\n",
    "    \n",
    "    # T√≠nh cutoff points\n",
    "    df_ranked = df_ranked.withColumn(\n",
    "        \"train_cutoff\", (F.col(\"total_rows\") * train_ratio).cast(\"int\")\n",
    "    ).withColumn(\n",
    "        \"val_cutoff\", (F.col(\"total_rows\") * (train_ratio + val_ratio)).cast(\"int\")\n",
    "    )\n",
    "    \n",
    "    # Assign split based on row position\n",
    "    df_split = df_ranked.withColumn(\n",
    "        \"split_type\",\n",
    "        F.when(F.col(\"row_num\") <= F.col(\"train_cutoff\"), \"train\")\n",
    "         .when(F.col(\"row_num\") <= F.col(\"val_cutoff\"), \"val\")\n",
    "         .otherwise(\"test\")\n",
    "    )\n",
    "    \n",
    "    # Drop helper columns\n",
    "    df_split = df_split.drop(\"row_num\", \"total_rows\", \"train_cutoff\", \"val_cutoff\", \"split_month\")\n",
    "    \n",
    "    return df_split\n",
    "\n",
    "print(\"\\n[PROCESSING] Applying stratified split...\")\n",
    "df_stratified = stratified_split_by_month(df_with_month, train_ratio=0.70, val_ratio=0.15)\n",
    "\n",
    "# Cache v√† split\n",
    "df_stratified = df_stratified.cache()\n",
    "\n",
    "df_train_raw = df_stratified.filter(F.col(\"split_type\") == \"train\").drop(\"split_type\")\n",
    "df_val_raw = df_stratified.filter(F.col(\"split_type\") == \"val\").drop(\"split_type\")\n",
    "df_test_raw = df_stratified.filter(F.col(\"split_type\") == \"test\").drop(\"split_type\")\n",
    "\n",
    "# Count\n",
    "train_count = df_train_raw.count()\n",
    "val_count = df_val_raw.count()\n",
    "test_count = df_test_raw.count()\n",
    "total_count = train_count + val_count + test_count\n",
    "\n",
    "print(f\"\\n[DATA] Stratified Split Results:\")\n",
    "print(f\"  Train: {train_count:8,} ({train_count/total_count*100:.1f}%)\")\n",
    "print(f\"  Val:   {val_count:8,} ({val_count/total_count*100:.1f}%)\")\n",
    "print(f\"  Test:  {test_count:8,} ({test_count/total_count*100:.1f}%)\")\n",
    "\n",
    "# Verify distribution similarity\n",
    "print(f\"\\n[VERIFY] Checking PM2.5 distribution across splits...\")\n",
    "train_stats = df_train_raw.select(F.mean(\"PM2_5\").alias(\"mean\"), F.stddev(\"PM2_5\").alias(\"std\")).collect()[0]\n",
    "val_stats = df_val_raw.select(F.mean(\"PM2_5\").alias(\"mean\"), F.stddev(\"PM2_5\").alias(\"std\")).collect()[0]\n",
    "test_stats = df_test_raw.select(F.mean(\"PM2_5\").alias(\"mean\"), F.stddev(\"PM2_5\").alias(\"std\")).collect()[0]\n",
    "\n",
    "print(f\"   Train: mean={train_stats['mean']:.2f}, std={train_stats['std']:.2f}\")\n",
    "print(f\"   Val:   mean={val_stats['mean']:.2f}, std={val_stats['std']:.2f}\")\n",
    "print(f\"   Test:  mean={test_stats['mean']:.2f}, std={test_stats['std']:.2f}\")\n",
    "\n",
    "# Calculate improvement\n",
    "mean_diff_val = abs(val_stats['mean'] - train_stats['mean']) / train_stats['mean'] * 100\n",
    "mean_diff_test = abs(test_stats['mean'] - train_stats['mean']) / train_stats['mean'] * 100\n",
    "\n",
    "print(f\"\\n[IMPROVEMENT] Distribution alignment:\")\n",
    "print(f\"   Train-Val difference:  {mean_diff_val:.1f}% (target: < 10%)\")\n",
    "print(f\"   Train-Test difference: {mean_diff_test:.1f}% (target: < 10%)\")\n",
    "\n",
    "if mean_diff_val < 15 and mean_diff_test < 15:\n",
    "    print(f\"   [SUCCESS] Stratified split significantly reduced distribution shift!\")\n",
    "else:\n",
    "    print(f\"   [WARNING] Some distribution shift remains - consider additional techniques\")\n",
    "\n",
    "# Cache final splits\n",
    "df_train_raw = df_train_raw.cache()\n",
    "df_val_raw = df_val_raw.cache()\n",
    "df_test_raw = df_test_raw.cache()\n",
    "\n",
    "# Cleanup\n",
    "df_stratified.unpersist()\n",
    "\n",
    "print(f\"\\n[SUCCESS] Stratified temporal split completed!\")\n",
    "print(f\"   Each split now contains data from ALL seasons\")\n",
    "print(f\"   Next: Normalize using TRAIN SET statistics ONLY\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:15:59.135795Z",
     "iopub.status.busy": "2025-12-01T16:15:59.135468Z",
     "iopub.status.idle": "2025-12-01T16:16:18.832294Z",
     "shell.execute_reply": "2025-12-01T16:16:18.831357Z",
     "shell.execute_reply.started": "2025-12-01T16:15:59.135769Z"
    },
    "papermill": {
     "duration": 13.388816,
     "end_time": "2025-11-20T10:39:58.674831",
     "exception": false,
     "start_time": "2025-11-20T10:39:45.286015",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PROCESSING] Step 3: Normalize NUMERICAL BASE FEATURES using TRAIN SET ONLY...\n",
      "[LOG TRANSFORM] Applying log1p transformation to PM2.5 (target)...\n",
      "   Reason: PM2.5 distribution is highly skewed (skewness > 1.5)\n",
      "   Formula: PM2_5_log = log(1 + PM2_5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Original PM2.5: mean=15.50, std=10.63\n",
      "   Log PM2.5:      mean=2.60, std=0.66\n",
      "[SUCCESS] Log transformation applied!\n",
      "[DATA] Normalizing 9 BASE features (NO lag features yet)...\n",
      "   Features to normalize: ['PM2_5_log', 'PM10', 'NO2', 'SO2', 'temperature_2m', 'relative_humidity_2m', 'wind_speed_10m', 'surface_pressure', 'precipitation']\n",
      "   [WARNING]  Computing min/max from TRAIN SET ONLY (preventing data leakage)\n",
      "  [OK] PM2_5_log                     : [    0.00,     5.12] -> [0, 1]\n",
      "  [OK] PM10                          : [    0.00,   401.70] -> [0, 1]\n",
      "  [OK] NO2                           : [    0.00,   253.20] -> [0, 1]\n",
      "  [OK] SO2                           : [    0.00,    76.30] -> [0, 1]\n",
      "  [OK] temperature_2m                : [    4.90,    36.50] -> [0, 1]\n",
      "  [OK] relative_humidity_2m          : [   16.00,   100.00] -> [0, 1]\n",
      "  [OK] wind_speed_10m                : [    0.00,    82.30] -> [0, 1]\n",
      "  [OK] surface_pressure              : [  974.70,  1032.50] -> [0, 1]\n",
      "  [OK] precipitation                 : [    0.00,    53.20] -> [0, 1]\n",
      "\n",
      "[SUCCESS] Scaler parameters computed from TRAIN SET only!\n",
      "\n",
      " Applying Min-Max scaling [0, 1] to all splits...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Base feature normalization completed!\n",
      "   [DATA] All splits normalized using train statistics only\n",
      "   [WARNING]  Next: Create lag features FROM SCALED COLUMNS\n"
     ]
    }
   ],
   "source": [
    "# B∆∞·ªõc 3: Normalize NUMERICAL G·ªêC (CH·ªà g·ªëc, KH√îNG c√≥ lag features)\n",
    "print(f\"\\n[PROCESSING] Step 3: Normalize NUMERICAL BASE FEATURES using TRAIN SET ONLY...\")\n",
    "\n",
    "# ========================================\n",
    "# LOG TRANSFORMATION FOR SKEWED TARGET (PM2.5)\n",
    "# ========================================\n",
    "# PM2.5 distribution is highly skewed (skewness=1.53)\n",
    "# Apply log1p transformation to reduce skewness and improve model learning\n",
    "print(f\"[LOG TRANSFORM] Applying log1p transformation to PM2.5 (target)...\")\n",
    "print(f\"   Reason: PM2.5 distribution is highly skewed (skewness > 1.5)\")\n",
    "print(f\"   Formula: PM2_5_log = log(1 + PM2_5)\")\n",
    "\n",
    "df_train_raw = df_train_raw.withColumn(\"PM2_5_log\", F.log1p(F.col(\"PM2_5\")))\n",
    "df_val_raw = df_val_raw.withColumn(\"PM2_5_log\", F.log1p(F.col(\"PM2_5\")))\n",
    "df_test_raw = df_test_raw.withColumn(\"PM2_5_log\", F.log1p(F.col(\"PM2_5\")))\n",
    "\n",
    "# Verify log transformation\n",
    "log_stats = df_train_raw.select(\n",
    "    F.mean(\"PM2_5\").alias(\"original_mean\"),\n",
    "    F.stddev(\"PM2_5\").alias(\"original_std\"),\n",
    "    F.mean(\"PM2_5_log\").alias(\"log_mean\"),\n",
    "    F.stddev(\"PM2_5_log\").alias(\"log_std\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"   Original PM2.5: mean={log_stats['original_mean']:.2f}, std={log_stats['original_std']:.2f}\")\n",
    "print(f\"   Log PM2.5:      mean={log_stats['log_mean']:.2f}, std={log_stats['log_std']:.2f}\")\n",
    "print(f\"[SUCCESS] Log transformation applied!\")\n",
    "\n",
    "# [WARNING] QUAN TR·ªåNG: CH·ªà normalize c√°c c·ªôt G·ªêC, KH√îNG bao g·ªìm lag features\n",
    "# Lag features s·∫Ω t·∫°o SAU t·ª´ c√°c c·ªôt ƒë√£ scale\n",
    "# Use PM2_5_log instead of PM2_5 for target normalization\n",
    "numerical_base_cols = [\n",
    "    # Target with log transformation\n",
    "    \"PM2_5_log\",\n",
    "    # Other Pollutants (current values only)\n",
    "    \"PM10\", \"NO2\", \"SO2\",\n",
    "    # Weather features (current values only)\n",
    "    \"temperature_2m\", \"relative_humidity_2m\", \"wind_speed_10m\",\n",
    "    \"surface_pressure\", \"precipitation\"\n",
    "]\n",
    "\n",
    "print(f\"[DATA] Normalizing {len(numerical_base_cols)} BASE features (NO lag features yet)...\")\n",
    "print(f\"   Features to normalize: {numerical_base_cols}\")\n",
    "print(f\"   [WARNING]  Computing min/max from TRAIN SET ONLY (preventing data leakage)\")\n",
    "\n",
    "# T√≠nh min/max CH·ªà T·ª™ TRAIN SET\n",
    "scaler_params = {}\n",
    "\n",
    "for col_name in numerical_base_cols:\n",
    "    if col_name in df_train_raw.columns:\n",
    "        # CH·ªà D√ôNG TRAIN SET ƒê·ªÇ T√çNH MIN/MAX  \n",
    "        stats = df_train_raw.select(\n",
    "            F.min(col_name).alias(\"min\"),\n",
    "            F.max(col_name).alias(\"max\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        min_val = stats[\"min\"]\n",
    "        max_val = stats[\"max\"]\n",
    "        \n",
    "        # [WARNING] CRITICAL: Handle None values from null columns\n",
    "        if min_val is None or max_val is None:\n",
    "            print(f\"  [WARNING]  Skipping {col_name}: All values are null\")\n",
    "            continue\n",
    "        \n",
    "        # [WARNING] CRITICAL: Tr√°nh chia 0 khi min = max\n",
    "        if max_val == min_val:\n",
    "            max_val = min_val + 1\n",
    "        \n",
    "        scaler_params[col_name] = {\"min\": min_val, \"max\": max_val}\n",
    "        print(f\"  [OK] {col_name:30s}: [{min_val:8.2f}, {max_val:8.2f}] -> [0, 1]\")\n",
    "\n",
    "print(f\"\\n[SUCCESS] Scaler parameters computed from TRAIN SET only!\")\n",
    "\n",
    "# √Åp d·ª•ng normalization cho t·∫•t c·∫£ splits\n",
    "def apply_scaling(df, scaler_params):\n",
    "    \"\"\"Apply Min-Max scaling using precomputed parameters\"\"\"\n",
    "    df_scaled = df\n",
    "    for col_name, params in scaler_params.items():\n",
    "        if col_name in df.columns:\n",
    "            min_val = params[\"min\"]\n",
    "            max_val = params[\"max\"]\n",
    "            df_scaled = df_scaled.withColumn(\n",
    "                f\"{col_name}_scaled\",\n",
    "                (F.col(col_name) - min_val) / (max_val - min_val)\n",
    "            )\n",
    "    return df_scaled\n",
    "\n",
    "print(f\"\\n Applying Min-Max scaling [0, 1] to all splits...\")\n",
    "\n",
    "# Apply scaling and trigger computation\n",
    "df_train = apply_scaling(df_train_raw, scaler_params)\n",
    "df_val = apply_scaling(df_val_raw, scaler_params)\n",
    "df_test = apply_scaling(df_test_raw, scaler_params)\n",
    "\n",
    "# Trigger computation and cache\n",
    "_ = df_train.count()\n",
    "_ = df_val.count()\n",
    "_ = df_test.count()\n",
    "\n",
    "df_train = df_train.cache()\n",
    "df_val = df_val.cache()\n",
    "df_test = df_test.cache()\n",
    "\n",
    "# Unpersist raw versions to free memory\n",
    "df_train_raw.unpersist()\n",
    "df_val_raw.unpersist()\n",
    "df_test_raw.unpersist()\n",
    "\n",
    "print(f\"[SUCCESS] Base feature normalization completed!\")\n",
    "print(f\"   [DATA] All splits normalized using train statistics only\")\n",
    "print(f\"   [WARNING]  Next: Create lag features FROM SCALED COLUMNS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:16:18.834013Z",
     "iopub.status.busy": "2025-12-01T16:16:18.833564Z",
     "iopub.status.idle": "2025-12-01T16:16:18.849545Z",
     "shell.execute_reply": "2025-12-01T16:16:18.848435Z",
     "shell.execute_reply.started": "2025-12-01T16:16:18.833983Z"
    },
    "papermill": {
     "duration": 0.047192,
     "end_time": "2025-11-20T10:39:58.747182",
     "exception": false,
     "start_time": "2025-11-20T10:39:58.699990",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SAVE] Step 4: Saving Scaler Parameters...\n",
      "[KAGGLE] Kaggle mode: Saving to /kaggle/working/processed\n",
      "[SUCCESS] Scaler parameters saved to: /kaggle/working/processed/scaler_params.json\n",
      "   - Computed from TRAIN SET only (no data leakage)\n",
      "   - Used for denormalizing predictions during inference\n",
      "   - Contains 9 base features\n",
      "\n",
      "[METADATA] Example scaler params (from train set):\n",
      "  temperature_2m      : min=4.90, max=36.50\n",
      "  wind_speed_10m      : min=0.00, max=82.30\n"
     ]
    }
   ],
   "source": [
    "# B∆∞·ªõc 4: L∆∞u Scaler Parameters\n",
    "print(f\"\\n[SAVE] Step 4: Saving Scaler Parameters...\")\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Include log transformation info in scaler params\n",
    "scaler_json = {\n",
    "    col: {\"min\": float(params[\"min\"]), \"max\": float(params[\"max\"])} \n",
    "    for col, params in scaler_params.items()\n",
    "}\n",
    "\n",
    "# Add metadata for log transformation (needed for inverse transform during inference)\n",
    "scaler_json[\"_metadata\"] = {\n",
    "    \"log_transformed_features\": [\"PM2_5\"],  # Features that were log1p transformed\n",
    "    \"target_feature\": \"PM2_5_log_scaled\",\n",
    "    \"inverse_transform_order\": [\"denormalize\", \"expm1\"]  # Order: first denormalize, then exp(x)-1\n",
    "}\n",
    "\n",
    "# ========================================\n",
    "# ADAPTIVE PATH (Kaggle vs Colab vs Local)\n",
    "# ========================================\n",
    "if IN_KAGGLE:\n",
    "    # [KAGGLE] Kaggle: Write to /kaggle/working (auto-saved on commit)\n",
    "    processed_dir = Path(\"/kaggle/working/processed\")\n",
    "    print(f\"[KAGGLE] Kaggle mode: Saving to {processed_dir}\")\n",
    "    \n",
    "elif IN_COLAB:\n",
    "    # [COLAB] Colab: Write to Google Drive\n",
    "    processed_dir = Path(\"/content/drive/MyDrive/pm25-data/processed\")\n",
    "    print(f\"[COLAB] Colab mode: Saving to Google Drive\")\n",
    "    \n",
    "else:\n",
    "    # [LOCAL] Local: Write to project folder\n",
    "    processed_dir = Path(\"../data/processed\")\n",
    "    print(f\"[LOCAL] Local mode: Saving to {processed_dir}\")\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c v·ªõi parents=True (t·∫°o c·∫£ parent directories n·∫øu ch∆∞a c√≥)\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# L∆∞u ra file JSON\n",
    "scaler_path = processed_dir / \"scaler_params.json\"\n",
    "with open(scaler_path, 'w') as f:\n",
    "    json.dump(scaler_json, f, indent=2)\n",
    "\n",
    "print(f\"[SUCCESS] Scaler parameters saved to: {scaler_path}\")\n",
    "print(f\"   - Computed from TRAIN SET only (no data leakage)\")\n",
    "print(f\"   - Used for denormalizing predictions during inference\")\n",
    "print(f\"   - Contains {len(scaler_params)} base features\")\n",
    "\n",
    "# Hi·ªÉn th·ªã v√≠ d·ª•\n",
    "print(f\"\\n[METADATA] Example scaler params (from train set):\")\n",
    "example_cols = [\"PM2_5\", \"temperature_2m\", \"wind_speed_10m\"]\n",
    "for col in example_cols:\n",
    "    if col in scaler_params:\n",
    "        params = scaler_params[col]\n",
    "        print(f\"  {col:20s}: min={params['min']:.2f}, max={params['max']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:16:18.851108Z",
     "iopub.status.busy": "2025-12-01T16:16:18.850825Z",
     "iopub.status.idle": "2025-12-01T16:16:18.883934Z",
     "shell.execute_reply": "2025-12-01T16:16:18.883003Z",
     "shell.execute_reply.started": "2025-12-01T16:16:18.851087Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[location_id: string, location: string, datetime: timestamp, lat: double, lon: double, NO2: double, PM10: double, PM2_5: double, SO2: double, temperature_2m: double, relative_humidity_2m: double, wind_speed_10m: double, surface_pressure: double, precipitation: double, hour_sin: double, hour_cos: double, month_sin: double, month_cos: double, day_of_week_sin: double, day_of_week_cos: double, is_weekend: int, wind_direction_sin: double, wind_direction_cos: double, PM2_5_log: double, PM2_5_log_scaled: double, PM10_scaled: double, NO2_scaled: double, SO2_scaled: double, temperature_2m_scaled: double, relative_humidity_2m_scaled: double, wind_speed_10m_scaled: double, surface_pressure_scaled: double, precipitation_scaled: double]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:16:18.885770Z",
     "iopub.status.busy": "2025-12-01T16:16:18.885437Z",
     "iopub.status.idle": "2025-12-01T16:17:24.302786Z",
     "shell.execute_reply": "2025-12-01T16:17:24.301884Z",
     "shell.execute_reply.started": "2025-12-01T16:16:18.885744Z"
    },
    "papermill": {
     "duration": 59.454423,
     "end_time": "2025-11-20T10:40:58.224966",
     "exception": false,
     "start_time": "2025-11-20T10:39:58.770543",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PROCESSING] Step 5: Creating Lag Features FROM SCALED COLUMNS (XGBoost only)...\n",
      "\n",
      "[METADATA] Creating lag features:\n",
      "   Deep Learning models: No lags needed (learn from sequences)\n",
      "   XGBoost: 6 lags x 9 variables = 54 features\n",
      "   [SUCCESS] Using SCALED columns as source (proper scale relationship)\n",
      "   [NOTE] PM2_5 lags use log transformed version for consistency\n",
      "\n",
      "[PROCESSING] Creating lag features for all splits...\n",
      "  [OK] Train: 54 lag features created\n",
      "  [OK] Val:   54 lag features created\n",
      "  [OK] Test:  54 lag features created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUCCESS] Lag features created successfully!\n",
      "   [SUCCESS] All lags created FROM SCALED columns\n",
      "   [SUCCESS] Lag and base features have SAME scale parameters\n",
      "   [SUCCESS] Proper temporal relationship preserved\n",
      "\n",
      "[PROCESSING] Handling null values in lag features...\n",
      "\n",
      "[DATA] Null counts BEFORE handling:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PM2_5_log_lag1_scaled              :       14 nulls (0.01%)\n",
      "  PM2_5_log_lag2_scaled              :       28 nulls (0.01%)\n",
      "  PM2_5_log_lag3_scaled              :       42 nulls (0.02%)\n",
      "\n",
      "[WARNING]  Reason: First 24 hours of each location have no previous data\n",
      "   Strategy: DROP records with ANY null lag feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[?]  Dropping records with null lag features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DATA] Records dropped (null lag features):\n",
      "  [?] Train: 202,154 -> 201,818 (dropped 336, 0.17%)\n",
      "  [?] Val:   43,397 -> 43,061 (dropped 336, 0.77%)\n",
      "  [?] Test:  43,606 -> 43,270 (dropped 336, 0.77%)\n",
      "\n",
      "[SUCCESS] Verification - checking for remaining nulls...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PM2_5_log_lag1_scaled              :        0 nulls [SUCCESS]\n",
      "  PM2_5_log_lag2_scaled              :        0 nulls [SUCCESS]\n",
      "  PM2_5_log_lag3_scaled              :        0 nulls [SUCCESS]\n",
      "\n",
      "[SUCCESS] All lag features are clean!\n",
      "\n",
      "[SUCCESS] Lag features + Null handling completed!\n",
      "   - Created 54 lag features FROM SCALED columns\n",
      "   - Lost only first 24 hours per location\n",
      "   - All lag features now have valid values\n",
      "   - Data quality ensured for XGBoost training\n"
     ]
    }
   ],
   "source": [
    "# B∆∞·ªõc 5: T·∫°o Lag Features T·ª™ C√ÅC C·ªòT ƒê√É SCALE (CH·ªà CHO XGBOOST)\n",
    "print(f\"\\n[PROCESSING] Step 5: Creating Lag Features FROM SCALED COLUMNS (XGBoost only)...\")\n",
    "\n",
    "# [WARNING] QUAN TR·ªåNG: Lag features ƒë∆∞·ª£c t·∫°o T·ª™ C√ÅC C·ªòT ƒê√É SCALE\n",
    "# -> ƒê·∫£m b·∫£o lag v√† g·ªëc c√≥ C√ôNG SCALE PARAMETERS\n",
    "# -> Gi·ªØ ƒë√∫ng m·ªëi quan h·ªá gi·ªØa gi√° tr·ªã hi·ªán t·∫°i v√† qu√° kh·ª©\n",
    "\n",
    "LAG_STEPS = [1, 2, 3, 6, 12, 24]  # 1h, 2h, 3h, 6h, 12h, 24h tr∆∞·ªõc\n",
    "\n",
    "# Columns c·∫ßn t·∫°o lag (s·ª≠ d·ª•ng b·∫£n SCALED)\n",
    "# Note: Use PM2_5_log for lags as well (log transformed version)\n",
    "lag_base_columns = [\"PM2_5_log\", \"PM10\", \"NO2\", \"SO2\", \n",
    "                    \"temperature_2m\", \"relative_humidity_2m\", \n",
    "                    \"wind_speed_10m\", \"surface_pressure\", \"precipitation\"]\n",
    "\n",
    "print(f\"\\n[METADATA] Creating lag features:\")\n",
    "print(f\"   Deep Learning models: No lags needed (learn from sequences)\")\n",
    "print(f\"   XGBoost: {len(LAG_STEPS)} lags x {len(lag_base_columns)} variables = {len(LAG_STEPS) * len(lag_base_columns)} features\")\n",
    "print(f\"   [SUCCESS] Using SCALED columns as source (proper scale relationship)\")\n",
    "print(f\"   [NOTE] PM2_5 lags use log transformed version for consistency\")\n",
    "\n",
    "# Window cho t·ª´ng location (s·∫Øp x·∫øp theo th·ªùi gian)\n",
    "w_lag = Window.partitionBy(\"location_id\").orderBy(\"datetime\")\n",
    "\n",
    "# T·∫°o lag features cho t·ª´ng split (train, val, test)\n",
    "def create_lag_features(df, lag_base_columns, lag_steps):\n",
    "    \"\"\"Create lag features from SCALED columns\"\"\"\n",
    "    df_with_lags = df\n",
    "    \n",
    "    for col_name in lag_base_columns:\n",
    "        col_scaled = f\"{col_name}_scaled\"\n",
    "        \n",
    "        if col_scaled in df.columns:\n",
    "            for lag in lag_steps:\n",
    "                lag_col_name = f\"{col_name}_lag{lag}_scaled\"\n",
    "                \n",
    "                # [SUCCESS] T·∫°o lag T·ª™ C·ªòT ƒê√É SCALE\n",
    "                df_with_lags = df_with_lags.withColumn(\n",
    "                    lag_col_name,\n",
    "                    F.lag(col_scaled, lag).over(w_lag)\n",
    "                )\n",
    "    \n",
    "    return df_with_lags\n",
    "\n",
    "# Apply to all splits\n",
    "print(f\"\\n[PROCESSING] Creating lag features for all splits...\")\n",
    "df_train = create_lag_features(df_train, lag_base_columns, LAG_STEPS)\n",
    "df_val = create_lag_features(df_val, lag_base_columns, LAG_STEPS)\n",
    "df_test = create_lag_features(df_test, lag_base_columns, LAG_STEPS)\n",
    "\n",
    "print(f\"  [OK] Train: {len(LAG_STEPS) * len(lag_base_columns)} lag features created\")\n",
    "print(f\"  [OK] Val:   {len(LAG_STEPS) * len(lag_base_columns)} lag features created\")\n",
    "print(f\"  [OK] Test:  {len(LAG_STEPS) * len(lag_base_columns)} lag features created\")\n",
    "\n",
    "# Trigger computation and cache\n",
    "_ = df_train.count()\n",
    "_ = df_val.count()\n",
    "_ = df_test.count()\n",
    "\n",
    "df_train = df_train.cache()\n",
    "df_val = df_val.cache()\n",
    "df_test = df_test.cache()\n",
    "\n",
    "print(f\"\\n[SUCCESS] Lag features created successfully!\")\n",
    "print(f\"   [SUCCESS] All lags created FROM SCALED columns\")\n",
    "print(f\"   [SUCCESS] Lag and base features have SAME scale parameters\")\n",
    "print(f\"   [SUCCESS] Proper temporal relationship preserved\")\n",
    "\n",
    "# ========================================\n",
    "# X·ª¨ L√ù NULL VALUES TRONG LAG FEATURES\n",
    "# ========================================\n",
    "print(f\"\\n[PROCESSING] Handling null values in lag features...\")\n",
    "\n",
    "# T·∫°o list t·∫•t c·∫£ lag feature names\n",
    "lag_feature_names = [f\"{col}_lag{lag}_scaled\" for col in lag_base_columns for lag in LAG_STEPS]\n",
    "\n",
    "# ƒê·∫øm nulls TR∆Ø·ªöC khi x·ª≠ l√Ω\n",
    "print(f\"\\n[DATA] Null counts BEFORE handling:\")\n",
    "sample_lag_features = lag_feature_names[:3]\n",
    "for lag_col in sample_lag_features:\n",
    "    if lag_col in df_train.columns:\n",
    "        null_count = df_train.filter(F.col(lag_col).isNull()).count()\n",
    "        total_count = df_train.count()\n",
    "        print(f\"  {lag_col:35s}: {null_count:8,} nulls ({null_count/total_count*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n[WARNING]  Reason: First {max(LAG_STEPS)} hours of each location have no previous data\")\n",
    "print(f\"   Strategy: DROP records with ANY null lag feature\")\n",
    "\n",
    "# Track counts before drop\n",
    "train_before = df_train.count()\n",
    "val_before = df_val.count()\n",
    "test_before = df_test.count()\n",
    "\n",
    "# Function to drop nulls\n",
    "def drop_lag_nulls(df, lag_features):\n",
    "    \"\"\"Drop records with any null lag feature\"\"\"\n",
    "    df_clean = df\n",
    "    for col in lag_features:\n",
    "        if col in df.columns:\n",
    "            df_clean = df_clean.filter(F.col(col).isNotNull())\n",
    "    return df_clean\n",
    "\n",
    "# Apply to all splits\n",
    "print(f\"\\n[?]  Dropping records with null lag features...\")\n",
    "df_train_clean = drop_lag_nulls(df_train, lag_feature_names)\n",
    "df_val_clean = drop_lag_nulls(df_val, lag_feature_names)\n",
    "df_test_clean = drop_lag_nulls(df_test, lag_feature_names)\n",
    "\n",
    "# Count after\n",
    "train_after = df_train_clean.count()\n",
    "val_after = df_val_clean.count()\n",
    "test_after = df_test_clean.count()\n",
    "\n",
    "# Cache cleaned datasets\n",
    "df_train_clean = df_train_clean.cache()\n",
    "df_val_clean = df_val_clean.cache()\n",
    "df_test_clean = df_test_clean.cache()\n",
    "\n",
    "# Unpersist old ones\n",
    "df_train.unpersist()\n",
    "df_val.unpersist()\n",
    "df_test.unpersist()\n",
    "\n",
    "# Reassign\n",
    "df_train = df_train_clean\n",
    "df_val = df_val_clean\n",
    "df_test = df_test_clean\n",
    "\n",
    "print(f\"\\n[DATA] Records dropped (null lag features):\")\n",
    "print(f\"  [?] Train: {train_before:,} -> {train_after:,} (dropped {train_before - train_after:,}, {(train_before - train_after)/train_before*100:.2f}%)\")\n",
    "print(f\"  [?] Val:   {val_before:,} -> {val_after:,} (dropped {val_before - val_after:,}, {(val_before - val_after)/val_before*100:.2f}%)\")\n",
    "print(f\"  [?] Test:  {test_before:,} -> {test_after:,} (dropped {test_before - test_after:,}, {(test_before - test_after)/test_before*100:.2f}%)\")\n",
    "\n",
    "# Verify no nulls\n",
    "print(f\"\\n[SUCCESS] Verification - checking for remaining nulls...\")\n",
    "sample_check = lag_feature_names[:3]\n",
    "total_nulls_after = 0\n",
    "for lag_col in sample_check:\n",
    "    if lag_col in df_train.columns:\n",
    "        null_count = df_train.filter(F.col(lag_col).isNull()).count()\n",
    "        total_nulls_after += null_count\n",
    "        status = \"[SUCCESS]\" if null_count == 0 else \"[ERROR]\"\n",
    "        print(f\"  {lag_col:35s}: {null_count:8,} nulls {status}\")\n",
    "\n",
    "if total_nulls_after == 0:\n",
    "    print(f\"\\n[SUCCESS] All lag features are clean!\")\n",
    "else:\n",
    "    print(f\"\\n[WARNING]  Still {total_nulls_after} nulls found!\")\n",
    "\n",
    "print(f\"\\n[SUCCESS] Lag features + Null handling completed!\")\n",
    "print(f\"   - Created {len(lag_feature_names)} lag features FROM SCALED columns\")\n",
    "print(f\"   - Lost only first {max(LAG_STEPS)} hours per location\")\n",
    "print(f\"   - All lag features now have valid values\")\n",
    "print(f\"   - Data quality ensured for XGBoost training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:17:24.303984Z",
     "iopub.status.busy": "2025-12-01T16:17:24.303748Z",
     "iopub.status.idle": "2025-12-01T16:17:24.321487Z",
     "shell.execute_reply": "2025-12-01T16:17:24.320058Z",
     "shell.execute_reply.started": "2025-12-01T16:17:24.303964Z"
    },
    "papermill": {
     "duration": 0.043725,
     "end_time": "2025-11-20T10:40:58.300881",
     "exception": false,
     "start_time": "2025-11-20T10:40:58.257156",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PROCESSING] Step 6: Preparing Model-Specific Features...\n",
      "[MODEL] DEEP LEARNING Features: 17 features\n",
      "   - Current pollutants (scaled): 3\n",
      "   - Weather (scaled): 5\n",
      "   - Time (cyclic): 6\n",
      "   - Time (binary): 1\n",
      "   - NO LAG FEATURES (models learn from sequences)\n",
      "\n",
      "[DATA] XGBOOST Features: 71 features\n",
      "   - Deep Learning base features: 17\n",
      "   - Lag features (from scaled columns): 54\n",
      "   - Total: 71 features\n",
      "\n",
      "[SUCCESS] Model-specific features prepared:\n",
      "  [MODEL] CNN1D-BLSTM-Attention: 17 features\n",
      "  [MODEL] LSTM: 17 features\n",
      "  [DATA] XGBoost: 71 features\n",
      "  [TARGET] Target: PM2_5_log_scaled (log transformed)\n",
      "\n",
      "[SUCCESS] All feature columns exist in datasets!\n"
     ]
    }
   ],
   "source": [
    "# B∆∞·ªõc 6: Chu·∫©n b·ªã Features cho t·ª´ng Model\n",
    "print(\"\\n[PROCESSING] Step 6: Preparing Model-Specific Features...\")\n",
    "\n",
    "# ========================================\n",
    "# FEATURES CHO DEEP LEARNING MODELS (CNN1D-BLSTM, LSTM)\n",
    "# ========================================\n",
    "# Kh√¥ng c·∫ßn lag features v√¨ models t·ª± h·ªçc temporal patterns t·ª´ sequences\n",
    "\n",
    "dl_input_features = []\n",
    "\n",
    "# 1. Pollutants scaled (tr·ª´ PM2_5 - ƒë√¢y l√† target)\n",
    "dl_input_features.extend([\"PM10_scaled\", \"NO2_scaled\", \"SO2_scaled\"])\n",
    "\n",
    "# 2. Weather features scaled (core features)\n",
    "dl_input_features.extend([\n",
    "    \"temperature_2m_scaled\", \"relative_humidity_2m_scaled\",\n",
    "    \"wind_speed_10m_scaled\", \"surface_pressure_scaled\", \"precipitation_scaled\"  # ‚úÖ Added surface_pressure\n",
    "])\n",
    "\n",
    "# 3. Time features (cyclic encoding - ƒë√£ ·ªü d·∫°ng sin/cos trong [-1, 1])\n",
    "dl_input_features.extend([\n",
    "    \"hour_sin\", \"hour_cos\", \n",
    "    \"month_sin\", \"month_cos\",\n",
    "    \"day_of_week_sin\", \"day_of_week_cos\",\n",
    "    \"wind_direction_sin\", \"wind_direction_cos\"\n",
    "])\n",
    "\n",
    "# 4. Time features (binary)\n",
    "dl_input_features.extend([\"is_weekend\"])\n",
    "\n",
    "print(f\"[MODEL] DEEP LEARNING Features: {len(dl_input_features)} features\")\n",
    "print(f\"   - Current pollutants (scaled): 3\")\n",
    "print(f\"   - Weather (scaled): 5\") \n",
    "print(f\"   - Time (cyclic): 6\")\n",
    "print(f\"   - Time (binary): 1\")\n",
    "print(f\"   - NO LAG FEATURES (models learn from sequences)\")\n",
    "\n",
    "# ========================================  \n",
    "# FEATURES CHO XGBOOST\n",
    "# ========================================\n",
    "# C·∫ßn lag features v√¨ kh√¥ng c√≥ kh·∫£ nƒÉng x·ª≠ l√Ω sequences\n",
    "\n",
    "xgb_input_features = dl_input_features.copy()  # Start with DL features\n",
    "\n",
    "# Th√™m lag features CH·ªà CHO XGBOOST (ƒë√£ ƒë∆∞·ª£c t·∫°o t·ª´ scaled columns)\n",
    "for col_name in lag_base_columns:\n",
    "    for lag in LAG_STEPS:\n",
    "        lag_col_name = f\"{col_name}_lag{lag}_scaled\"\n",
    "        xgb_input_features.append(lag_col_name)\n",
    "\n",
    "print(f\"\\n[DATA] XGBOOST Features: {len(xgb_input_features)} features\")\n",
    "print(f\"   - Deep Learning base features: {len(dl_input_features)}\")\n",
    "print(f\"   - Lag features (from scaled columns): {len(lag_base_columns) * len(LAG_STEPS)}\")\n",
    "print(f\"   - Total: {len(xgb_input_features)} features\")\n",
    "\n",
    "# Target variable (log transformed and scaled for better training)\n",
    "# Using PM2_5_log_scaled instead of PM2_5_scaled to handle skewness\n",
    "target_feature = \"PM2_5_log_scaled\"\n",
    "\n",
    "print(f\"\\n[SUCCESS] Model-specific features prepared:\")\n",
    "print(f\"  [MODEL] CNN1D-BLSTM-Attention: {len(dl_input_features)} features\")\n",
    "print(f\"  [MODEL] LSTM: {len(dl_input_features)} features\")  \n",
    "print(f\"  [DATA] XGBoost: {len(xgb_input_features)} features\")\n",
    "print(f\"  [TARGET] Target: {target_feature} (log transformed)\")\n",
    "\n",
    "# [WARNING] CRITICAL: Verify ALL columns exist\n",
    "missing_dl = [col for col in dl_input_features if col not in df_train.columns]\n",
    "missing_xgb = [col for col in xgb_input_features if col not in df_train.columns]\n",
    "missing_target = target_feature not in df_train.columns\n",
    "\n",
    "if missing_dl or missing_xgb or missing_target:\n",
    "    print(f\"\\n[ERROR] MISSING COLUMNS DETECTED:\")\n",
    "    if missing_dl: \n",
    "        print(f\"  DL models: {missing_dl}\")\n",
    "    if missing_xgb: \n",
    "        print(f\"  XGBoost: {missing_xgb[:5]}...\")  # Show first 5\n",
    "    if missing_target:\n",
    "        print(f\"  Target: {target_feature}\")\n",
    "    \n",
    "    print(f\"\\n[WARNING]  Available scaled columns:\")\n",
    "    scaled_cols = [c for c in df_train.columns if c.endswith('_scaled')]\n",
    "    print(f\"  {scaled_cols[:10]}...\")\n",
    "    \n",
    "    raise ValueError(\"Missing required feature columns! Check normalization step.\")\n",
    "else:\n",
    "    print(f\"\\n[SUCCESS] All feature columns exist in datasets!\")\n",
    "\n",
    "dl_input_features.extend([\"PM2_5_log_scaled\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:17:24.322978Z",
     "iopub.status.busy": "2025-12-01T16:17:24.322699Z",
     "iopub.status.idle": "2025-12-01T16:17:41.710263Z",
     "shell.execute_reply": "2025-12-01T16:17:41.708885Z",
     "shell.execute_reply.started": "2025-12-01T16:17:24.322958Z"
    },
    "papermill": {
     "duration": 14.426037,
     "end_time": "2025-11-20T10:41:12.752486",
     "exception": false,
     "start_time": "2025-11-20T10:40:58.326449",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PROCESSING] Step 7: Preparing Final Model-Specific Datasets...\n",
      "\n",
      "[MODEL] Deep Learning datasets (no lag features):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] Train: 201,818 records, 18 features\n",
      "  [OK] Val:   43,061 records, 18 features\n",
      "  [OK] Test:  43,270 records, 18 features\n",
      "\n",
      "[DATA] XGBoost datasets (with lag features):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8276:=================================================>      (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] Train: 201,818 records, 71 features\n",
      "  [OK] Val:   43,061 records, 71 features\n",
      "  [OK] Test:  43,270 records, 71 features\n",
      "\n",
      "[SUCCESS] Final datasets prepared!\n",
      "   [MODEL] Deep Learning: 18 features (no lags)\n",
      "   [DATA] XGBoost: 71 features (with 54 lags)\n",
      "   [TARGET] Target: PM2_5_log_scaled\n",
      "   [SUCCESS] All datasets cleaned and ready for training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# B∆∞·ªõc 7: Prepare Final Model Datasets\n",
    "print(\"\\n[PROCESSING] Step 7: Preparing Final Model-Specific Datasets...\")\n",
    "\n",
    "# ========================================\n",
    "# DEEP LEARNING DATASETS (CNN1D-BLSTM & LSTM)\n",
    "# ========================================\n",
    "# Kh√¥ng c·∫ßn lag features, ch·ªâ c·∫ßn base features + time features\n",
    "\n",
    "print(f\"\\n[MODEL] Deep Learning datasets (no lag features):\")\n",
    "\n",
    "# Select only DL features + target\n",
    "dl_train = df_train.select(\"location_id\", \"datetime\", target_feature, *dl_input_features)\n",
    "dl_val = df_val.select(\"location_id\", \"datetime\", target_feature, *dl_input_features)\n",
    "dl_test = df_test.select(\"location_id\", \"datetime\", target_feature, *dl_input_features)\n",
    "\n",
    "# Cache\n",
    "dl_train = dl_train.cache()\n",
    "dl_val = dl_val.cache()\n",
    "dl_test = dl_test.cache()\n",
    "\n",
    "dl_train_count = dl_train.count()\n",
    "dl_val_count = dl_val.count()\n",
    "dl_test_count = dl_test.count()\n",
    "\n",
    "print(f\"  [OK] Train: {dl_train_count:,} records, {len(dl_input_features)} features\")\n",
    "print(f\"  [OK] Val:   {dl_val_count:,} records, {len(dl_input_features)} features\")\n",
    "print(f\"  [OK] Test:  {dl_test_count:,} records, {len(dl_input_features)} features\")\n",
    "\n",
    "# ========================================\n",
    "# XGBOOST DATASETS\n",
    "# ========================================\n",
    "# C·∫ßn c·∫£ base features + lag features\n",
    "\n",
    "print(f\"\\n[DATA] XGBoost datasets (with lag features):\")\n",
    "\n",
    "# Select XGB features + target\n",
    "xgb_train = df_train.select(\"location_id\", \"datetime\", target_feature, *xgb_input_features)\n",
    "xgb_val = df_val.select(\"location_id\", \"datetime\", target_feature, *xgb_input_features)\n",
    "xgb_test = df_test.select(\"location_id\", \"datetime\", target_feature, *xgb_input_features)\n",
    "\n",
    "# Cache\n",
    "xgb_train = xgb_train.cache()\n",
    "xgb_val = xgb_val.cache()\n",
    "xgb_test = xgb_test.cache()\n",
    "\n",
    "xgb_train_count = xgb_train.count()\n",
    "xgb_val_count = xgb_val.count()\n",
    "xgb_test_count = xgb_test.count()\n",
    "\n",
    "print(f\"  [OK] Train: {xgb_train_count:,} records, {len(xgb_input_features)} features\")\n",
    "print(f\"  [OK] Val:   {xgb_val_count:,} records, {len(xgb_input_features)} features\")\n",
    "print(f\"  [OK] Test:  {xgb_test_count:,} records, {len(xgb_input_features)} features\")\n",
    "\n",
    "print(f\"\\n[SUCCESS] Final datasets prepared!\")\n",
    "print(f\"   [MODEL] Deep Learning: {len(dl_input_features)} features (no lags)\")\n",
    "print(f\"   [DATA] XGBoost: {len(xgb_input_features)} features (with {len(lag_base_columns) * len(LAG_STEPS)} lags)\")\n",
    "print(f\"   [TARGET] Target: {target_feature}\")\n",
    "print(f\"   [SUCCESS] All datasets cleaned and ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:17:41.711949Z",
     "iopub.status.busy": "2025-12-01T16:17:41.711635Z",
     "iopub.status.idle": "2025-12-01T16:17:43.126074Z",
     "shell.execute_reply": "2025-12-01T16:17:43.125060Z",
     "shell.execute_reply.started": "2025-12-01T16:17:41.711923Z"
    },
    "papermill": {
     "duration": 1.157316,
     "end_time": "2025-11-20T10:41:13.936053",
     "exception": false,
     "start_time": "2025-11-20T10:41:12.778737",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[DATA] FEATURE ENGINEERING PIPELINE SUMMARY\n",
      "================================================================================\n",
      "\n",
      "[SUCCESS] PIPELINE EXECUTION ORDER (Correct - No Data Leakage):\n",
      "   [1] Time Features -> Added cyclic (sin/cos) + is_weekend\n",
      "   [2] Temporal Split -> 70% train / 15% val / 15% test\n",
      "   [3] Normalization -> Min-Max [0,1] using TRAIN statistics ONLY\n",
      "   [4] Lag Features + Null Handling -> Created FROM SCALED columns, dropped nulls\n",
      "   [5] Scaler Params -> Saved for inference\n",
      "   [6] Model Features -> Prepared for Deep Learning & XGBoost\n",
      "   [7] Final Datasets -> Ready for training\n",
      "\n",
      "[DATA] DATASET STATISTICS:\n",
      "   Total records: 288,149\n",
      "   Total locations: 14\n",
      "\n",
      "[METADATA] FEATURE BREAKDOWN:\n",
      "   [MODEL] Deep Learning (CNN1D-BLSTM & LSTM): 18 features\n",
      "      ‚îú‚îÄ Pollutants (scaled): 3 (PM10, NO2, SO2)\n",
      "      ‚îú‚îÄ Weather (scaled): 5 (temp, humidity, wind, precipitation)\n",
      "      ‚îú‚îÄ Time (cyclic): 6 (hour, month, day_of_week -> sin/cos)\n",
      "      ‚îî‚îÄ Time (binary): 1 (is_weekend)\n",
      "   \n",
      "   [DATA] XGBoost: 71 features\n",
      "      ‚îú‚îÄ Deep Learning features: 18\n",
      "      ‚îî‚îÄ Lag features: 54 (9 vars √ó 6 lags)\n",
      "\n",
      "[TARGET] TARGET VARIABLE:\n",
      "   PM2_5_log_scaled (normalized PM2.5 in [0, 1])\n",
      "\n",
      "[SUCCESS] DATA QUALITY CHECKS:\n",
      "   [OK] No missing values in target\n",
      "   [OK] No missing values in features\n",
      "   [OK] No outliers (removed by WHO/EPA standards)\n",
      "   [OK] Proper temporal ordering\n",
      "   [OK] STRATIFIED split (all seasons in each split)\n",
      "   [OK] Correct scale relationship (lag from scaled columns)\n",
      "   [OK] No nulls in lag features (first 24h dropped)\n",
      "   [OK] Log transform applied to target (reduced skewness)\n",
      "\n",
      "[SAVE] SAVED ARTIFACTS:\n",
      "   [FILES] scaler_params.json -> Min-Max parameters (train set only)\n",
      "   [FILES] feature_metadata.json -> Feature lists & configuration\n",
      "\n",
      "[RUN] READY FOR NEXT PHASE:\n",
      "   Variables in memory:\n",
      "   - Deep Learning: dl_train, dl_val, dl_test\n",
      "   - XGBoost: xgb_train, xgb_val, xgb_test\n",
      "   Next step: Sequence creation for Deep Learning models\n",
      "================================================================================\n",
      "\n",
      "[KAGGLE] Kaggle mode: Saving metadata to /kaggle/working/processed\n",
      "\n",
      "[SAVE] Feature metadata saved to: /kaggle/working/processed/feature_metadata.json\n",
      "   [SUCCESS] Pipeline version: 2.0 (refactored - no data leakage)\n",
      "   [SUCCESS] Contains: feature lists, lag config, split info, dataset counts\n"
     ]
    }
   ],
   "source": [
    "# B∆∞·ªõc 8: Feature Engineering Summary + Metadata Saving\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[DATA] FEATURE ENGINEERING PIPELINE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[SUCCESS] PIPELINE EXECUTION ORDER (Correct - No Data Leakage):\")\n",
    "print(f\"   [1] Time Features -> Added cyclic (sin/cos) + is_weekend\")\n",
    "print(f\"   [2] Temporal Split -> 70% train / 15% val / 15% test\")\n",
    "print(f\"   [3] Normalization -> Min-Max [0,1] using TRAIN statistics ONLY\")\n",
    "print(f\"   [4] Lag Features + Null Handling -> Created FROM SCALED columns, dropped nulls\")\n",
    "print(f\"   [5] Scaler Params -> Saved for inference\")\n",
    "print(f\"   [6] Model Features -> Prepared for Deep Learning & XGBoost\")\n",
    "print(f\"   [7] Final Datasets -> Ready for training\")\n",
    "\n",
    "print(f\"\\n[DATA] DATASET STATISTICS:\")\n",
    "print(f\"   Total records: {dl_train_count + dl_val_count + dl_test_count:,}\")\n",
    "print(f\"   Total locations: {df_train.select('location_id').distinct().count()}\")\n",
    "\n",
    "print(f\"\\n[METADATA] FEATURE BREAKDOWN:\")\n",
    "print(f\"   [MODEL] Deep Learning (CNN1D-BLSTM & LSTM): {len(dl_input_features)} features\")\n",
    "print(f\"      ‚îú‚îÄ Pollutants (scaled): 3 (PM10, NO2, SO2)\")\n",
    "print(f\"      ‚îú‚îÄ Weather (scaled): 5 (temp, humidity, wind, precipitation)\")\n",
    "print(f\"      ‚îú‚îÄ Time (cyclic): 6 (hour, month, day_of_week -> sin/cos)\")\n",
    "print(f\"      ‚îî‚îÄ Time (binary): 1 (is_weekend)\")\n",
    "print(f\"   \")\n",
    "print(f\"   [DATA] XGBoost: {len(xgb_input_features)} features\")\n",
    "print(f\"      ‚îú‚îÄ Deep Learning features: {len(dl_input_features)}\")\n",
    "print(f\"      ‚îî‚îÄ Lag features: {len(lag_base_columns) * len(LAG_STEPS)} ({len(lag_base_columns)} vars √ó {len(LAG_STEPS)} lags)\")\n",
    "\n",
    "print(f\"\\n[TARGET] TARGET VARIABLE:\")\n",
    "print(f\"   {target_feature} (normalized PM2.5 in [0, 1])\")\n",
    "\n",
    "print(f\"\\n[SUCCESS] DATA QUALITY CHECKS:\")\n",
    "print(f\"   [OK] No missing values in target\")\n",
    "print(f\"   [OK] No missing values in features\")\n",
    "print(f\"   [OK] No outliers (removed by WHO/EPA standards)\")\n",
    "print(f\"   [OK] Proper temporal ordering\")\n",
    "print(f\"   [OK] STRATIFIED split (all seasons in each split)\")\n",
    "print(f\"   [OK] Correct scale relationship (lag from scaled columns)\")\n",
    "print(f\"   [OK] No nulls in lag features (first {max(LAG_STEPS)}h dropped)\")\n",
    "print(f\"   [OK] Log transform applied to target (reduced skewness)\")\n",
    "\n",
    "print(f\"\\n[SAVE] SAVED ARTIFACTS:\")\n",
    "print(f\"   [FILES] scaler_params.json -> Min-Max parameters (train set only)\")\n",
    "print(f\"   [FILES] feature_metadata.json -> Feature lists & configuration\")\n",
    "\n",
    "print(f\"\\n[RUN] READY FOR NEXT PHASE:\")\n",
    "print(f\"   Variables in memory:\")\n",
    "print(f\"   - Deep Learning: dl_train, dl_val, dl_test\")\n",
    "print(f\"   - XGBoost: xgb_train, xgb_val, xgb_test\")\n",
    "print(f\"   Next step: Sequence creation for Deep Learning models\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========================================\n",
    "# SAVE FEATURE METADATA\n",
    "# ========================================\n",
    "# L∆∞u metadata v·ªÅ feature engineering ƒë·ªÉ tham kh·∫£o trong t∆∞∆°ng lai\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Metadata cho feature engineering\n",
    "dataset_metadata = {\n",
    "    \"project\": \"PM2.5 Prediction\",\n",
    "    \"preprocessing_version\": \"3.0_stratified_split\",\n",
    "    \"pipeline_order\": [\n",
    "        \"Time Features (cyclic encoding)\",\n",
    "        \"STRATIFIED Temporal Split (70/15/15 per month)\",\n",
    "        \"Log Transform (target PM2.5)\",\n",
    "        \"Normalization (train stats only)\",\n",
    "        \"Lag Features (from scaled columns)\",\n",
    "        \"Null Handling (drop first 24h per location)\"\n",
    "    ],\n",
    "    \"split_strategy\": {\n",
    "        \"method\": \"stratified_temporal\",\n",
    "        \"description\": \"Each month split 70/15/15, ensures all seasons in each split\",\n",
    "        \"ratios\": {\"train\": 0.70, \"val\": 0.15, \"test\": 0.15}\n",
    "    },\n",
    "    \"deep_learning_features\": dl_input_features,\n",
    "    \"xgboost_features\": xgb_input_features,\n",
    "    \"target_feature\": target_feature,\n",
    "    \"target_transform\": \"log1p\",\n",
    "    \"lag_config\": {\n",
    "        \"lag_steps\": LAG_STEPS,\n",
    "        \"lag_base_columns\": lag_base_columns,\n",
    "        \"total_lag_features\": len(lag_base_columns) * len(LAG_STEPS)\n",
    "    },\n",
    "    \"dataset_counts\": {\n",
    "        \"dl_train\": dl_train_count,\n",
    "        \"dl_val\": dl_val_count,\n",
    "        \"dl_test\": dl_test_count,\n",
    "        \"xgb_train\": xgb_train_count,\n",
    "        \"xgb_val\": xgb_val_count,\n",
    "        \"xgb_test\": xgb_test_count\n",
    "    },\n",
    "    \"total_features\": {\n",
    "        \"deep_learning\": len(dl_input_features),\n",
    "        \"xgboost\": len(xgb_input_features)\n",
    "    }\n",
    "}\n",
    "\n",
    "# ========================================\n",
    "# ADAPTIVE PATH (Kaggle vs Colab vs Local)\n",
    "# ========================================\n",
    "if IN_KAGGLE:\n",
    "    # [KAGGLE] Kaggle: Write to /kaggle/working (auto-saved on commit)\n",
    "    processed_dir = Path(\"/kaggle/working/processed\")\n",
    "    print(f\"\\n[KAGGLE] Kaggle mode: Saving metadata to {processed_dir}\")\n",
    "    \n",
    "elif IN_COLAB:\n",
    "    # [COLAB] Colab: Write to Google Drive\n",
    "    processed_dir = Path(\"/content/drive/MyDrive/pm25-data/processed\")\n",
    "    print(f\"\\n[COLAB] Colab mode: Saving metadata to Google Drive\")\n",
    "    \n",
    "else:\n",
    "    # [LOCAL] Local: Write to project folder\n",
    "    processed_dir = Path(\"../data/processed\")\n",
    "    print(f\"\\n[LOCAL] Local mode: Saving metadata to {processed_dir}\")\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c v·ªõi parents=True (t·∫°o c·∫£ parent directories n·∫øu ch∆∞a c√≥)\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# L∆∞u metadata\n",
    "metadata_path = processed_dir / \"feature_metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(dataset_metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\n[SAVE] Feature metadata saved to: {metadata_path}\")\n",
    "print(f\"   [SUCCESS] Pipeline version: 2.0 (refactored - no data leakage)\")\n",
    "print(f\"   [SUCCESS] Contains: feature lists, lag config, split info, dataset counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:17:43.127542Z",
     "iopub.status.busy": "2025-12-01T16:17:43.126974Z",
     "iopub.status.idle": "2025-12-01T16:28:53.960612Z",
     "shell.execute_reply": "2025-12-01T16:28:53.959540Z",
     "shell.execute_reply.started": "2025-12-01T16:17:43.127486Z"
    },
    "papermill": {
     "duration": 530.139687,
     "end_time": "2025-11-20T10:50:04.112386",
     "exception": false,
     "start_time": "2025-11-20T10:41:13.972699",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PROCESSING] Step 9: Creating Sequence Data for Deep Learning Models...\n",
      "[GEAR]  Sequence Configuration:\n",
      "   - CNN1D-BLSTM-Attention: 48 timesteps\n",
      "   - LSTM: 24 timesteps\n",
      "\n",
      "[DATA] Creating sequences for each model...\n",
      "\n",
      "[MODEL] CNN1D-BLSTM-Attention (48 timesteps):\n",
      "    Creating 48-step sequences...\n",
      "      [?]  Layer 1: Dropped first 48 records/location\n",
      "         Records: 201,818\n",
      "        [INSTALL] Processing 5 batches (18 features)...\n",
      "           Batch 1/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 2/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 3/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 4/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 5/5: 2 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [?] Filtering null sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [?]  Layer 2: Dropped 672 records containing NaN/Null inside sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/01 16:21:14 WARN DAGScheduler: Broadcasting large task binary with size 1666.9 KiB\n",
      "25/12/01 16:21:21 WARN DAGScheduler: Broadcasting large task binary with size 1672.3 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [SUCCESS] Final: 201,146 records (99.7% retained)\n",
      "    Creating 48-step sequences...\n",
      "      [?]  Layer 1: Dropped first 48 records/location\n",
      "         Records: 43,061\n",
      "        [INSTALL] Processing 5 batches (18 features)...\n",
      "           Batch 1/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 2/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 3/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 4/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 5/5: 2 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [?] Filtering null sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [?]  Layer 2: Dropped 672 records containing NaN/Null inside sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [SUCCESS] Final: 42,389 records (98.4% retained)\n",
      "    Creating 48-step sequences...\n",
      "      [?]  Layer 1: Dropped first 48 records/location\n",
      "         Records: 43,270\n",
      "        [INSTALL] Processing 5 batches (18 features)...\n",
      "           Batch 1/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 2/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 3/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 4/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 5/5: 2 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [?] Filtering null sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [?]  Layer 2: Dropped 672 records containing NaN/Null inside sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [SUCCESS] Final: 42,598 records (98.4% retained)\n",
      "    [SUCCESS] CNN sequences created successfully\n",
      "\n",
      "[PROCESSING] LSTM (24 timesteps):\n",
      "    Creating 24-step sequences...\n",
      "      [?]  Layer 1: Dropped first 24 records/location\n",
      "         Records: 201,818\n",
      "        [INSTALL] Processing 5 batches (18 features)...\n",
      "           Batch 1/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 2/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 3/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 4/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 5/5: 2 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [?] Filtering null sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [?]  Layer 2: Dropped 336 records containing NaN/Null inside sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/01 16:26:34 WARN DAGScheduler: Broadcasting large task binary with size 1265.1 KiB\n",
      "25/12/01 16:26:43 WARN DAGScheduler: Broadcasting large task binary with size 1270.4 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [SUCCESS] Final: 201,482 records (99.8% retained)\n",
      "    Creating 24-step sequences...\n",
      "      [?]  Layer 1: Dropped first 24 records/location\n",
      "         Records: 43,061\n",
      "        [INSTALL] Processing 5 batches (18 features)...\n",
      "           Batch 1/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 2/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 3/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 4/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 5/5: 2 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [?] Filtering null sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [?]  Layer 2: Dropped 336 records containing NaN/Null inside sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [SUCCESS] Final: 42,725 records (99.2% retained)\n",
      "    Creating 24-step sequences...\n",
      "      [?]  Layer 1: Dropped first 24 records/location\n",
      "         Records: 43,270\n",
      "        [INSTALL] Processing 5 batches (18 features)...\n",
      "           Batch 1/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 2/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 3/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 4/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 5/5: 2 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [?] Filtering null sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [?]  Layer 2: Dropped 336 records containing NaN/Null inside sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13223:================================================>      (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [SUCCESS] Final: 42,934 records (99.2% retained)\n",
      "    [SUCCESS] LSTM sequences created successfully\n",
      "\n",
      "[SUCCESS] Sequence data preparation completed!\n",
      "\n",
      "[METADATA] Data Quality Guarantee:\n",
      "   [OK] Layer 1: No incomplete history (first 48/24 records dropped)\n",
      "   [OK] Layer 2: No data gaps in middle (nulls filtered out)\n",
      "   [OK] Result: 100% clean sequences with ZERO nulls\n",
      "   [OK] Ready for high-quality model training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# B∆∞·ªõc 9: Create Sequence Data for Deep Learning Models\n",
    "print(\"\\n[PROCESSING] Step 9: Creating Sequence Data for Deep Learning Models...\")\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "# Sequence configuration (optimized for Colab)\n",
    "CNN_SEQUENCE_LENGTH = 48  # Optimal for long-term patterns\n",
    "LSTM_SEQUENCE_LENGTH = 24  # Optimal for medium-term patterns\n",
    "\n",
    "print(f\"[GEAR]  Sequence Configuration:\")\n",
    "print(f\"   - CNN1D-BLSTM-Attention: {CNN_SEQUENCE_LENGTH} timesteps\")\n",
    "print(f\"   - LSTM: {LSTM_SEQUENCE_LENGTH} timesteps\")\n",
    "\n",
    "def create_sequences_optimized(df, feature_cols, target_col, sequence_length):\n",
    "    \"\"\"\n",
    "    Optimized sequence creation with checkpointing to avoid StackOverflow\n",
    "    \n",
    "    [TARGET] Key Strategy:\n",
    "    - Batch processing to avoid deep logical plans\n",
    "    - Checkpoint after each batch to reset plan depth\n",
    "    - Use broadcast joins for efficiency\n",
    "    - Single final filter for null handling\n",
    "    \n",
    "    [?] Null Handling (2-Layer Protection):\n",
    "    Layer 1: Drop first N records/location (incomplete history)\n",
    "    Layer 2: Filter ANY null in sequences (data gaps)\n",
    "    Result: 100% clean sequences with ZERO nulls\n",
    "    \"\"\"\n",
    "    print(f\"    Creating {sequence_length}-step sequences...\")\n",
    "    \n",
    "    window_spec = Window.partitionBy(\"location_id\").orderBy(\"datetime\")\n",
    "    \n",
    "    # ========================================\n",
    "    # LAYER 1: Drop first N records (incomplete history)\n",
    "    # ========================================\n",
    "    # df_base = df.select(\"location_id\", \"datetime\", target_col, *feature_cols) \\\n",
    "    #             .repartition(4, \"location_id\") \\\n",
    "    #             .withColumn(\"row_num\", F.row_number().over(window_spec)) \\\n",
    "    #             .filter(F.col(\"row_num\") > sequence_length) \\\n",
    "    #             .drop(\"row_num\") \\\n",
    "    #             .cache()\n",
    "    df_base = df\n",
    "    \n",
    "    records_after_layer1 = df_base.count()  # Materialize\n",
    "    print(f\"      [?]  Layer 1: Dropped first {sequence_length} records/location\")\n",
    "    print(f\"         Records: {records_after_layer1:,}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # BATCH PROCESSING (ÈÅøÂÖç StackOverflow)\n",
    "    # ========================================\n",
    "    # Chia features th√†nh batches nh·ªè ƒë·ªÉ tr√°nh logical plan qu√° s√¢u\n",
    "    BATCH_SIZE = 4  # M·ªói batch x·ª≠ l√Ω 4 features (4 √ó 48 lags = 192 ops - safe)\n",
    "    feature_batches = [feature_cols[i:i+BATCH_SIZE] for i in range(0, len(feature_cols), BATCH_SIZE)]\n",
    "    \n",
    "    print(f\"        [INSTALL] Processing {len(feature_batches)} batches ({len(feature_cols)} features)...\")\n",
    "    \n",
    "    base_cols = [\"location_id\", \"datetime\"]\n",
    "    result_df = df_base.select(*base_cols)\n",
    "    \n",
    "    for batch_idx, batch_features in enumerate(feature_batches, 1):\n",
    "        print(f\"           Batch {batch_idx}/{len(feature_batches)}: {len(batch_features)} features\")\n",
    "        \n",
    "        # T·∫°o batch DataFrame\n",
    "        batch_df = df_base.select(*base_cols, *batch_features)\n",
    "        \n",
    "        # T·∫°o sequences cho batch n√†y\n",
    "        for col_name in batch_features:\n",
    "            # T·∫°o array of lags [t-N, ...,t-2, t-1]\n",
    "            lag_exprs = [F.lag(col_name, step).over(window_spec)\n",
    "             for step in range(sequence_length, 0, -1)]  # ‚úÖ ƒê·∫£o ng∆∞·ª£c: N -> 1\n",
    "            batch_df = batch_df.withColumn(f\"{col_name}_sequence\", F.array(*lag_exprs))\n",
    "        \n",
    "        # Select ch·ªâ sequence columns\n",
    "        sequence_cols = [f\"{col}_sequence\" for col in batch_features]\n",
    "        batch_df = batch_df.select(*base_cols, *sequence_cols).cache()\n",
    "        batch_df.count()  # Materialize ƒë·ªÉ reset logical plan\n",
    "        \n",
    "        # Join v√†o result\n",
    "        result_df = result_df.join(batch_df, base_cols, \"inner\")\n",
    "        \n",
    "        # Unpersist batch (gi·∫£i ph√≥ng memory)\n",
    "        batch_df.unpersist()\n",
    "    \n",
    "    # ========================================\n",
    "    # LAYER 2: Filter nulls in sequences\n",
    "    # ========================================\n",
    "    print(f\"        [?] Filtering null sequences...\")\n",
    "    \n",
    "    all_sequence_cols = [f\"{col}_sequence\" for col in feature_cols]\n",
    "\n",
    "    # H√†m ki·ªÉm tra ch·∫∑t ch·∫Ω t·ª´ng c·ªôt sequence\n",
    "    def get_valid_sequence_condition(col_name):\n",
    "        col = F.col(col_name)\n",
    "        # ƒêi·ªÅu ki·ªán 1: B·∫£n th√¢n m·∫£ng kh√¥ng ƒë∆∞·ª£c null v√† ph·∫£i ƒë·ªß k√≠ch th∆∞·ªõc\n",
    "        basic_cond = col.isNotNull() & (F.size(col) == sequence_length)\n",
    "        \n",
    "        # ƒêi·ªÅu ki·ªán 2 (QUAN TR·ªåNG NH·∫§T): Duy·ªát t·ª´ng ph·∫ßn t·ª≠ x trong m·∫£ng\n",
    "        # x ph·∫£i NOT NULL v√† x ph·∫£i NOT NaN\n",
    "        # H√†m forall c√≥ t·ª´ Spark 3.1+\n",
    "        element_check = F.forall(col, lambda x: x.isNotNull() & (~F.isnan(x)))\n",
    "        \n",
    "        return basic_cond & element_check\n",
    "    \n",
    "    # Build null filter: ALL sequences must be NOT NULL\n",
    "    from functools import reduce\n",
    "    final_filter = reduce(\n",
    "        lambda acc, col_name: acc & get_valid_sequence_condition(col_name),\n",
    "        all_sequence_cols,\n",
    "        F.lit(True)\n",
    "    )\n",
    "    \n",
    "    # √Åp d·ª•ng l·ªçc\n",
    "    result_df = result_df.filter(final_filter)\n",
    "    \n",
    "    # T√≠nh to√°n s·ªë l∆∞·ª£ng sau khi l·ªçc\n",
    "    records_after_layer2 = result_df.count()\n",
    "    dropped = records_after_layer1 - records_after_layer2\n",
    "    \n",
    "    if dropped > 0:\n",
    "        print(f\"      [?]  Layer 2: Dropped {dropped:,} records containing NaN/Null inside sequences\")\n",
    "    else:\n",
    "        print(f\"      [SUCCESS] Layer 2: Data clean, no gaps found\")\n",
    "    # ========================================\n",
    "    # FINAL: Add target and clean up\n",
    "    # ========================================\n",
    "    result_df = result_df.join(\n",
    "        df_base.select(\"location_id\", \"datetime\", target_col),\n",
    "        [\"location_id\", \"datetime\"],\n",
    "        \"inner\"\n",
    "    ).filter(F.col(target_col).isNotNull()) \\\n",
    "     .withColumnRenamed(target_col, \"target_value\") \\\n",
    "     .cache()\n",
    "    \n",
    "    final_count = result_df.count()\n",
    "    retention_rate = (final_count / records_after_layer1) * 100\n",
    "    \n",
    "    print(f\"      [SUCCESS] Final: {final_count:,} records ({retention_rate:.1f}% retained)\")\n",
    "    \n",
    "    # Cleanup\n",
    "    df_base.unpersist()\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "print(\"\\n[DATA] Creating sequences for each model...\")\n",
    "\n",
    "# Create CNN1D-BLSTM sequences\n",
    "print(f\"\\n[MODEL] CNN1D-BLSTM-Attention ({CNN_SEQUENCE_LENGTH} timesteps):\")\n",
    "try:\n",
    "    cnn_train_clean = create_sequences_optimized(dl_train, dl_input_features, target_feature, CNN_SEQUENCE_LENGTH)\n",
    "    cnn_val_clean = create_sequences_optimized(dl_val, dl_input_features, target_feature, CNN_SEQUENCE_LENGTH)\n",
    "    cnn_test_clean = create_sequences_optimized(dl_test, dl_input_features, target_feature, CNN_SEQUENCE_LENGTH)\n",
    "    print(f\"    [SUCCESS] CNN sequences created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"    [ERROR] CNN sequence creation failed: {str(e)[:100]}...\")\n",
    "    cnn_train_clean = cnn_val_clean = cnn_test_clean = None\n",
    "\n",
    "# Create LSTM sequences  \n",
    "print(f\"\\n[PROCESSING] LSTM ({LSTM_SEQUENCE_LENGTH} timesteps):\")\n",
    "try:\n",
    "    lstm_train_clean = create_sequences_optimized(dl_train, dl_input_features, target_feature, LSTM_SEQUENCE_LENGTH)\n",
    "    lstm_val_clean = create_sequences_optimized(dl_val, dl_input_features, target_feature, LSTM_SEQUENCE_LENGTH)\n",
    "    lstm_test_clean = create_sequences_optimized(dl_test, dl_input_features, target_feature, LSTM_SEQUENCE_LENGTH)\n",
    "    print(f\"    [SUCCESS] LSTM sequences created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"    [ERROR] LSTM sequence creation failed: {str(e)[:100]}...\")\n",
    "    lstm_train_clean = lstm_val_clean = lstm_test_clean = None\n",
    "\n",
    "print(f\"\\n[SUCCESS] Sequence data preparation completed!\")\n",
    "print(f\"\\n[METADATA] Data Quality Guarantee:\")\n",
    "print(f\"   [OK] Layer 1: No incomplete history (first {CNN_SEQUENCE_LENGTH}/{LSTM_SEQUENCE_LENGTH} records dropped)\")\n",
    "print(f\"   [OK] Layer 2: No data gaps in middle (nulls filtered out)\")\n",
    "print(f\"   [OK] Result: 100% clean sequences with ZERO nulls\")\n",
    "print(f\"   [OK] Ready for high-quality model training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:28:53.961991Z",
     "iopub.status.busy": "2025-12-01T16:28:53.961762Z",
     "iopub.status.idle": "2025-12-01T16:30:23.977602Z",
     "shell.execute_reply": "2025-12-01T16:30:23.976293Z",
     "shell.execute_reply.started": "2025-12-01T16:28:53.961973Z"
    },
    "papermill": {
     "duration": 75.76105,
     "end_time": "2025-11-20T10:51:19.919263",
     "exception": false,
     "start_time": "2025-11-20T10:50:04.158213",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INSTALL] Step 10: Exporting Final Datasets to Disk...\n",
      "[KAGGLE] Kaggle mode: Saving to /kaggle/working/processed\n",
      "   [WARNING]  Files will be auto-saved when you commit notebook\n",
      "\n",
      "[DATA] Dataset Status:\n",
      "  CNN1D-BLSTM: [SUCCESS] Ready\n",
      "  LSTM: [SUCCESS] Ready\n",
      "  XGBoost: [SUCCESS] Ready\n",
      "\n",
      "[SAVE] Exporting datasets to Parquet format...\n",
      "\n",
      "  [MODEL] Exporting CNN1D-BLSTM datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/01 16:28:58 WARN DAGScheduler: Broadcasting large task binary with size 1874.8 KiB\n",
      "25/12/01 16:29:32 WARN DAGScheduler: Broadcasting large task binary with size 1672.3 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     [SUCCESS] Saved to: /kaggle/working/processed/cnn_sequences/\n",
      "        - train: 201,146 records\n",
      "        - val:   42,389 records\n",
      "        - test:  42,598 records\n",
      "\n",
      "  [PROCESSING] Exporting LSTM datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/01 16:29:46 WARN DAGScheduler: Broadcasting large task binary with size 1472.9 KiB\n",
      "25/12/01 16:29:55 WARN DAGScheduler: Broadcasting large task binary with size 1025.6 KiB\n",
      "25/12/01 16:30:00 WARN DAGScheduler: Broadcasting large task binary with size 1025.6 KiB\n",
      "25/12/01 16:30:05 WARN DAGScheduler: Broadcasting large task binary with size 1270.4 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     [SUCCESS] Saved to: /kaggle/working/processed/lstm_sequences/\n",
      "        - train: 201,482 records\n",
      "        - val:   42,725 records\n",
      "        - test:  42,934 records\n",
      "\n",
      "  [DATA] Exporting XGBoost datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     [SUCCESS] Saved to: /kaggle/working/processed/xgboost/\n",
      "        - train: 201,818 records\n",
      "        - val:   43,061 records\n",
      "        - test:  43,270 records\n",
      "\n",
      "[SAVE] Saving metadata...\n",
      "   [SUCCESS] Metadata saved to: /kaggle/working/processed/datasets_ready.json\n",
      "   [SUCCESS] Scaler params saved to: /kaggle/working/processed/scaler_params.json\n",
      "   [SUCCESS] Feature metadata saved to: /kaggle/working/processed/feature_metadata.json\n",
      "\n",
      "================================================================================\n",
      "[SUCCESS] DATA PREPROCESSING & EXPORT COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "[KAGGLE] KAGGLE OUTPUT:\n",
      "   [?] Location: /kaggle/working/processed/\n",
      "   [?] To save permanently:\n",
      "      1. Click 'Save Version' (top right)\n",
      "      2. Choose 'Save & Run All' (recommended)\n",
      "      3. Wait for completion (~20-30 min)\n",
      "      4. Output will appear in 'Output' tab\n",
      "      5. Use as dataset: '+ Add Data' -> Your Output\n",
      "\n",
      "[?] Exported Directory Structure:\n",
      "   /kaggle/working/processed/\n",
      "   ‚îú‚îÄ‚îÄ cnn_sequences/\n",
      "   ‚îÇ   ‚îú‚îÄ‚îÄ train/  (201,146 records)\n",
      "   ‚îÇ   ‚îú‚îÄ‚îÄ val/    (42,389 records)\n",
      "   ‚îÇ   ‚îî‚îÄ‚îÄ test/   (42,598 records)\n",
      "   ‚îú‚îÄ‚îÄ lstm_sequences/\n",
      "   ‚îÇ   ‚îú‚îÄ‚îÄ train/  (201,482 records)\n",
      "   ‚îÇ   ‚îú‚îÄ‚îÄ val/    (42,725 records)\n",
      "   ‚îÇ   ‚îî‚îÄ‚îÄ test/   (42,934 records)\n",
      "   ‚îú‚îÄ‚îÄ xgboost/\n",
      "   ‚îÇ   ‚îú‚îÄ‚îÄ train/  (201,818 records)\n",
      "   ‚îÇ   ‚îú‚îÄ‚îÄ val/    (43,061 records)\n",
      "   ‚îÇ   ‚îî‚îÄ‚îÄ test/   (43,270 records)\n",
      "   ‚îú‚îÄ‚îÄ scaler_params.json\n",
      "   ‚îú‚îÄ‚îÄ feature_metadata.json\n",
      "   ‚îî‚îÄ‚îÄ datasets_ready.json\n",
      "\n",
      "[DATA] Total Dataset Sizes:\n",
      "   - CNN1D-BLSTM: 286,133 records (48 timesteps, 18 features)\n",
      "   - LSTM:        287,141 records (24 timesteps, 18 features)\n",
      "   - XGBoost:     288,149 records (71 features)\n",
      "\n",
      "[RUN] Ready for Model Training Phase!\n",
      "   [?] Next: Create new notebook, add this output as dataset\n",
      "   [?] Load: spark.read.parquet('/kaggle/input/<output-name>/processed/...')\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Export Final Datasets to Local then Upload to HDFS\n",
    "print(\"\\n[SAVE] Step 10: Saving Processed Data...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import json\n",
    "\n",
    "# Local output path for staging\n",
    "LOCAL_PROCESSED_PATH = os.path.join(TEMP_DIR, \"processed\")\n",
    "os.makedirs(LOCAL_PROCESSED_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"[LOCAL] Staging directory: {LOCAL_PROCESSED_PATH}\")\n",
    "print(f\"[HDFS] Final destination: {HDFS_PROCESSED_DATA_PATH}\")\n",
    "\n",
    "# Check dataset availability\n",
    "datasets_ready = {\n",
    "    \"cnn\": cnn_train_clean is not None and cnn_val_clean is not None and cnn_test_clean is not None,\n",
    "    \"lstm\": lstm_train_clean is not None and lstm_val_clean is not None and lstm_test_clean is not None,\n",
    "    \"xgb\": xgb_train is not None and xgb_val is not None and xgb_test is not None\n",
    "}\n",
    "\n",
    "print(f\"\\n[DATA] Dataset Status:\")\n",
    "for model, ready in datasets_ready.items():\n",
    "    model_name = {\"cnn\": \"CNN1D-BLSTM\", \"lstm\": \"LSTM\", \"xgb\": \"XGBoost\"}[model]\n",
    "    status = \"‚úì Ready\" if ready else \"‚úó Not Ready\"\n",
    "    print(f\"  {model_name}: {status}\")\n",
    "\n",
    "# ========================================\n",
    "# SAVE DATASETS TO LOCAL PARQUET\n",
    "# ========================================\n",
    "print(f\"\\n[SAVE] Saving datasets to local Parquet files...\")\n",
    "\n",
    "export_summary = {\n",
    "    \"cnn\": {\"train\": 0, \"val\": 0, \"test\": 0},\n",
    "    \"lstm\": {\"train\": 0, \"val\": 0, \"test\": 0},\n",
    "    \"xgb\": {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "}\n",
    "\n",
    "# Save CNN1D-BLSTM datasets\n",
    "if datasets_ready[\"cnn\"]:\n",
    "    print(f\"\\n  [CNN] Saving CNN1D-BLSTM datasets...\")\n",
    "    cnn_local_path = os.path.join(LOCAL_PROCESSED_PATH, \"cnn_sequences\")\n",
    "    \n",
    "    cnn_train_clean.coalesce(4).write.mode(\"overwrite\").parquet(f\"{cnn_local_path}/train\")\n",
    "    cnn_val_clean.coalesce(2).write.mode(\"overwrite\").parquet(f\"{cnn_local_path}/val\")\n",
    "    cnn_test_clean.coalesce(2).write.mode(\"overwrite\").parquet(f\"{cnn_local_path}/test\")\n",
    "    \n",
    "    export_summary[\"cnn\"][\"train\"] = cnn_train_clean.count()\n",
    "    export_summary[\"cnn\"][\"val\"] = cnn_val_clean.count()\n",
    "    export_summary[\"cnn\"][\"test\"] = cnn_test_clean.count()\n",
    "    \n",
    "    print(f\"     ‚úì Saved locally: {cnn_local_path}\")\n",
    "    print(f\"       - train: {export_summary['cnn']['train']:,} records\")\n",
    "    print(f\"       - val:   {export_summary['cnn']['val']:,} records\")\n",
    "    print(f\"       - test:  {export_summary['cnn']['test']:,} records\")\n",
    "\n",
    "# Save LSTM datasets\n",
    "if datasets_ready[\"lstm\"]:\n",
    "    print(f\"\\n  [LSTM] Saving LSTM datasets...\")\n",
    "    lstm_local_path = os.path.join(LOCAL_PROCESSED_PATH, \"lstm_sequences\")\n",
    "    \n",
    "    lstm_train_clean.coalesce(4).write.mode(\"overwrite\").parquet(f\"{lstm_local_path}/train\")\n",
    "    lstm_val_clean.coalesce(2).write.mode(\"overwrite\").parquet(f\"{lstm_local_path}/val\")\n",
    "    lstm_test_clean.coalesce(2).write.mode(\"overwrite\").parquet(f\"{lstm_local_path}/test\")\n",
    "    \n",
    "    export_summary[\"lstm\"][\"train\"] = lstm_train_clean.count()\n",
    "    export_summary[\"lstm\"][\"val\"] = lstm_val_clean.count()\n",
    "    export_summary[\"lstm\"][\"test\"] = lstm_test_clean.count()\n",
    "    \n",
    "    print(f\"     ‚úì Saved locally: {lstm_local_path}\")\n",
    "    print(f\"       - train: {export_summary['lstm']['train']:,} records\")\n",
    "    print(f\"       - val:   {export_summary['lstm']['val']:,} records\")\n",
    "    print(f\"       - test:  {export_summary['lstm']['test']:,} records\")\n",
    "\n",
    "# Save XGBoost datasets\n",
    "if datasets_ready[\"xgb\"]:\n",
    "    print(f\"\\n  [XGB] Saving XGBoost datasets...\")\n",
    "    xgb_local_path = os.path.join(LOCAL_PROCESSED_PATH, \"xgboost\")\n",
    "    \n",
    "    xgb_train.coalesce(4).write.mode(\"overwrite\").parquet(f\"{xgb_local_path}/train\")\n",
    "    xgb_val.coalesce(2).write.mode(\"overwrite\").parquet(f\"{xgb_local_path}/val\")\n",
    "    xgb_test.coalesce(2).write.mode(\"overwrite\").parquet(f\"{xgb_local_path}/test\")\n",
    "    \n",
    "    export_summary[\"xgb\"][\"train\"] = xgb_train.count()\n",
    "    export_summary[\"xgb\"][\"val\"] = xgb_val.count()\n",
    "    export_summary[\"xgb\"][\"test\"] = xgb_test.count()\n",
    "    \n",
    "    print(f\"     ‚úì Saved locally: {xgb_local_path}\")\n",
    "    print(f\"       - train: {export_summary['xgb']['train']:,} records\")\n",
    "    print(f\"       - val:   {export_summary['xgb']['val']:,} records\")\n",
    "    print(f\"       - test:  {export_summary['xgb']['test']:,} records\")\n",
    "\n",
    "# ========================================\n",
    "# SAVE METADATA TO LOCAL\n",
    "# ========================================\n",
    "print(f\"\\n[SAVE] Saving metadata files...\")\n",
    "\n",
    "# Create comprehensive metadata\n",
    "final_metadata = {\n",
    "    \"project\": \"PM2.5 Prediction with HDFS\",\n",
    "    \"preprocessing_completed\": True,\n",
    "    \"export_timestamp\": str(pd.Timestamp.now()),\n",
    "    \"environment\": \"local_hdfs_hybrid\",\n",
    "    \"hdfs_destination\": HDFS_PROCESSED_DATA_PATH,\n",
    "    \"models\": {\n",
    "        \"cnn1d_blstm\": {\n",
    "            \"sequence_length\": CNN_SEQUENCE_LENGTH,\n",
    "            \"features\": len(dl_input_features),\n",
    "            \"ready\": datasets_ready[\"cnn\"],\n",
    "            \"record_counts\": export_summary[\"cnn\"]\n",
    "        },\n",
    "        \"lstm\": {\n",
    "            \"sequence_length\": LSTM_SEQUENCE_LENGTH, \n",
    "            \"features\": len(dl_input_features),\n",
    "            \"ready\": datasets_ready[\"lstm\"],\n",
    "            \"record_counts\": export_summary[\"lstm\"]\n",
    "        },\n",
    "        \"xgboost\": {\n",
    "            \"features\": len(xgb_input_features),\n",
    "            \"lag_steps\": LAG_STEPS,\n",
    "            \"ready\": datasets_ready[\"xgb\"],\n",
    "            \"record_counts\": export_summary[\"xgb\"]\n",
    "        }\n",
    "    },\n",
    "    \"feature_details\": {\n",
    "        \"deep_learning_features\": dl_input_features,\n",
    "        \"xgboost_features\": xgb_input_features,\n",
    "        \"target\": target_feature\n",
    "    },\n",
    "    \"data_format\": \"parquet\",\n",
    "    \"null_handling\": {\n",
    "        \"strategy\": \"2-layer protection\",\n",
    "        \"layer1\": f\"Dropped first {CNN_SEQUENCE_LENGTH}/{LSTM_SEQUENCE_LENGTH} records per location\",\n",
    "        \"layer2\": \"Filtered records with nulls in sequence history\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metadata as JSON\n",
    "metadata_path = os.path.join(LOCAL_PROCESSED_PATH, \"datasets_ready.json\")\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(final_metadata, f, indent=2)\n",
    "print(f\"   ‚úì Metadata saved: datasets_ready.json\")\n",
    "\n",
    "# Save scaler params\n",
    "scaler_json = {\n",
    "    col: {\"min\": float(params[\"min\"]), \"max\": float(params[\"max\"])} \n",
    "    for col, params in scaler_params.items()\n",
    "}\n",
    "scaler_path = os.path.join(LOCAL_PROCESSED_PATH, \"scaler_params.json\")\n",
    "with open(scaler_path, 'w') as f:\n",
    "    json.dump(scaler_json, f, indent=2)\n",
    "print(f\"   ‚úì Scaler params saved: scaler_params.json\")\n",
    "\n",
    "print(f\"\\n[SUCCESS] All datasets saved locally to: {LOCAL_PROCESSED_PATH}\")\n",
    "\n",
    "# ========================================\n",
    "# UPLOAD TO HDFS\n",
    "# ========================================\n",
    "print(f\"\\n[UPLOAD] Uploading processed data to HDFS...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    hdfs_upload_directory(LOCAL_PROCESSED_PATH, HDFS_PROCESSED_DATA_PATH)\n",
    "    print(f\"\\n[SUCCESS] All data uploaded to HDFS: {HDFS_PROCESSED_DATA_PATH}\")\n",
    "    print(f\"[INFO] Verify at NameNode UI: http://localhost:9870\")\n",
    "    \n",
    "    # Verify upload\n",
    "    result = subprocess.run(\n",
    "        [\"docker\", \"exec\", \"hdfs-namenode\", \"hdfs\", \"dfs\", \"-ls\", \"-R\", HDFS_PROCESSED_DATA_PATH],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    print(f\"\\n[VERIFY] HDFS Contents:\")\n",
    "    print(result.stdout)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Upload failed: {str(e)}\")\n",
    "    print(f\"[INFO] Data is still available locally at: {LOCAL_PROCESSED_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:30:23.979099Z",
     "iopub.status.busy": "2025-12-01T16:30:23.978772Z",
     "iopub.status.idle": "2025-12-01T16:30:36.495834Z",
     "shell.execute_reply": "2025-12-01T16:30:36.494515Z",
     "shell.execute_reply.started": "2025-12-01T16:30:23.979072Z"
    },
    "papermill": {
     "duration": 10.483321,
     "end_time": "2025-11-20T10:51:30.609726",
     "exception": false,
     "start_time": "2025-11-20T10:51:20.126405",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[?] Loading Preprocessed Data with Pandas...\n",
      "================================================================================\n",
      "[KAGGLE] Kaggle mode: Loading from /kaggle/input/\n",
      "   Replace <your-dataset-name> with actual dataset name\n",
      "\n",
      "[INSTALL] Loading datasets...\n",
      "\n",
      "[MODEL] CNN1D-BLSTM-Attention:\n",
      "   [SUCCESS] Train: (201146, 21) | Val: (42389, 21) | Test: (42598, 21)\n",
      "\n",
      "[PROCESSING] LSTM:\n",
      "   [SUCCESS] Train: (201482, 21) | Val: (42725, 21) | Test: (42934, 21)\n",
      "\n",
      "[DATA] XGBoost:\n",
      "   [SUCCESS] Train: (201818, 74) | Val: (43061, 74) | Test: (43270, 74)\n",
      "\n",
      "[SUCCESS] All datasets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cleanup: Stop Spark and remove temp directory\n",
    "print(\"\\n[CLEANUP] Cleaning up resources...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Stop Spark\n",
    "spark.stop()\n",
    "print(\"[OK] Spark session stopped\")\n",
    "\n",
    "# Optional: Keep or remove temp directory\n",
    "print(f\"\\n[INFO] Temporary files location: {TEMP_DIR}\")\n",
    "print(f\"[INFO] You can safely delete this directory after verifying HDFS upload\")\n",
    "\n",
    "# Uncomment to auto-cleanup:\n",
    "# shutil.rmtree(TEMP_DIR, ignore_errors=True)\n",
    "# print(f\"[OK] Temporary directory cleaned up\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPROCESSING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n‚úì Data Source: HDFS {HDFS_RAW_DATA_PATH}\")\n",
    "print(f\"‚úì Data Destination: HDFS {HDFS_PROCESSED_DATA_PATH}\")\n",
    "print(f\"‚úì Verify: http://localhost:9870\")\n",
    "print(f\"\\n‚úì Ready for model training!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:30:36.501805Z",
     "iopub.status.busy": "2025-12-01T16:30:36.501428Z",
     "iopub.status.idle": "2025-12-01T16:30:36.676061Z",
     "shell.execute_reply": "2025-12-01T16:30:36.674839Z",
     "shell.execute_reply.started": "2025-12-01T16:30:36.501780Z"
    },
    "papermill": {
     "duration": 0.162933,
     "end_time": "2025-11-20T10:51:30.822005",
     "exception": false,
     "start_time": "2025-11-20T10:51:30.659072",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>PM10_scaled_sequence</th>\n",
       "      <th>NO2_scaled_sequence</th>\n",
       "      <th>SO2_scaled_sequence</th>\n",
       "      <th>temperature_2m_scaled_sequence</th>\n",
       "      <th>relative_humidity_2m_scaled_sequence</th>\n",
       "      <th>wind_speed_10m_scaled_sequence</th>\n",
       "      <th>surface_pressure_scaled_sequence</th>\n",
       "      <th>precipitation_scaled_sequence</th>\n",
       "      <th>...</th>\n",
       "      <th>hour_cos_sequence</th>\n",
       "      <th>month_sin_sequence</th>\n",
       "      <th>month_cos_sequence</th>\n",
       "      <th>day_of_week_sin_sequence</th>\n",
       "      <th>day_of_week_cos_sequence</th>\n",
       "      <th>wind_direction_sin_sequence</th>\n",
       "      <th>wind_direction_cos_sequence</th>\n",
       "      <th>is_weekend_sequence</th>\n",
       "      <th>PM2_5_log_scaled_sequence</th>\n",
       "      <th>target_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-05 16:00:00</td>\n",
       "      <td>[0.010953447846651731, 0.03360716952949963, 0....</td>\n",
       "      <td>[0.2112954186413902, 0.12796208530805686, 0.09...</td>\n",
       "      <td>[0.01834862385321101, 0.014416775884665795, 0....</td>\n",
       "      <td>[0.5221518987341772, 0.5379746835443038, 0.553...</td>\n",
       "      <td>[0.9404761904761905, 0.8928571428571429, 0.857...</td>\n",
       "      <td>[0.14094775212636695, 0.20534629404617252, 0.2...</td>\n",
       "      <td>[0.6020761245674737, 0.6280276816608994, 0.621...</td>\n",
       "      <td>[0.009398496240601503, 0.013157894736842105, 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-0.25881904510252063, -0.9659258262890683, -0...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-0.9749279121818236, -0.9749279121818236, -0....</td>\n",
       "      <td>[-0.2225209339563146, -0.2225209339563146, -0....</td>\n",
       "      <td>[0.9961946980917455, 0.9993908270190958, 0.994...</td>\n",
       "      <td>[-0.08715574274765824, -0.03489949670250073, -...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.2658888298411102, 0.40379514201938105, 0.41...</td>\n",
       "      <td>0.575244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-05 21:00:00</td>\n",
       "      <td>[0.03659447348767737, 0.04530744336569579, 0.0...</td>\n",
       "      <td>[0.052132701421800945, 0.046603475513428125, 0...</td>\n",
       "      <td>[0.02490170380078637, 0.030144167758846655, 0....</td>\n",
       "      <td>[0.5284810126582279, 0.5348101265822784, 0.534...</td>\n",
       "      <td>[0.9523809523809523, 0.9523809523809523, 0.952...</td>\n",
       "      <td>[0.11421628189550426, 0.12636695018226005, 0.1...</td>\n",
       "      <td>[0.6591695501730094, 0.6678200692041512, 0.676...</td>\n",
       "      <td>[0.011278195488721804, 0.007518796992481203, 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.5000000000000001, 0.7071067811865474, 0.866...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-0.9749279121818236, -0.9749279121818236, -0....</td>\n",
       "      <td>[-0.2225209339563146, -0.2225209339563146, -0....</td>\n",
       "      <td>[1.0, 0.9993908270190958, 0.9925461516413221, ...</td>\n",
       "      <td>[6.123233995736766e-17, 0.03489949670250108, -...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.5026014988639874, 0.5500357285414436, 0.559...</td>\n",
       "      <td>0.490291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-06 20:00:00</td>\n",
       "      <td>[0.05402041324371421, 0.06123973114264377, 0.0...</td>\n",
       "      <td>[0.10979462875197474, 0.08412322274881517, 0.1...</td>\n",
       "      <td>[0.02883355176933159, 0.030144167758846655, 0....</td>\n",
       "      <td>[0.5727848101265823, 0.5727848101265823, 0.563...</td>\n",
       "      <td>[0.8452380952380952, 0.8452380952380952, 0.857...</td>\n",
       "      <td>[0.15188335358444716, 0.14823815309842042, 0.1...</td>\n",
       "      <td>[0.6747404844290663, 0.6782006920415219, 0.685...</td>\n",
       "      <td>[0.0037593984962406013, 0.0018796992481203006,...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-0.7071067811865479, -0.5000000000000004, -0....</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-0.7818314824680299, -0.7818314824680299, -0....</td>\n",
       "      <td>[0.6234898018587334, 0.6234898018587334, 0.623...</td>\n",
       "      <td>[0.754709580222772, 0.8290375725550417, 0.7880...</td>\n",
       "      <td>[0.6560590289905073, 0.5591929034707468, 0.615...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0.5416701480187393, 0.5657633372272116, 0.558...</td>\n",
       "      <td>0.537972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-07 07:00:00</td>\n",
       "      <td>[0.029126213592233007, 0.025889967637540454, 0...</td>\n",
       "      <td>[0.05726698262243286, 0.08609794628751975, 0.0...</td>\n",
       "      <td>[0.01179554390563565, 0.014416775884665795, 0....</td>\n",
       "      <td>[0.4746835443037974, 0.46518987341772156, 0.45...</td>\n",
       "      <td>[0.8809523809523809, 0.8690476190476191, 0.857...</td>\n",
       "      <td>[0.15795868772782504, 0.15795868772782504, 0.1...</td>\n",
       "      <td>[0.7404844290657437, 0.7439446366782013, 0.740...</td>\n",
       "      <td>[0.0, 0.0, 0.0018796992481203006, 0.0, 0.0, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.8660254037844384, 0.9659258262890681, 1.0, ...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-0.7818314824680299, -0.7818314824680299, -2....</td>\n",
       "      <td>[0.6234898018587334, 0.6234898018587334, 1.0, ...</td>\n",
       "      <td>[0.6946583704589973, 0.6946583704589973, 0.694...</td>\n",
       "      <td>[0.7193398003386512, 0.7193398003386512, 0.719...</td>\n",
       "      <td>[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0.422640218551877, 0.3856687264826328, 0.3716...</td>\n",
       "      <td>0.502601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-07 14:00:00</td>\n",
       "      <td>[0.04605426935524023, 0.0495394573064476, 0.05...</td>\n",
       "      <td>[0.09913112164297, 0.13349131121642968, 0.1733...</td>\n",
       "      <td>[0.01703800786369594, 0.01834862385321101, 0.0...</td>\n",
       "      <td>[0.42721518987341767, 0.42088607594936706, 0.4...</td>\n",
       "      <td>[0.8571428571428571, 0.8571428571428571, 0.857...</td>\n",
       "      <td>[0.17982989064398544, 0.17739975698663427, 0.1...</td>\n",
       "      <td>[0.7249134948096888, 0.7352941176470594, 0.745...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0018796992481203006, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.25881904510252074, 6.123233995736766e-17, -...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-2.4492935982947064e-16, -2.4492935982947064e...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>[0.7771459614569708, 0.788010753606722, 0.7986...</td>\n",
       "      <td>[0.6293203910498375, 0.6156614753256583, 0.601...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0.4902908942160261, 0.43142220079168486, 0.48...</td>\n",
       "      <td>0.542887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-07 15:00:00</td>\n",
       "      <td>[0.0495394573064476, 0.05526512322628827, 0.05...</td>\n",
       "      <td>[0.13349131121642968, 0.17338072669826224, 0.1...</td>\n",
       "      <td>[0.01834862385321101, 0.02490170380078637, 0.0...</td>\n",
       "      <td>[0.42088607594936706, 0.4177215189873418, 0.44...</td>\n",
       "      <td>[0.8571428571428571, 0.8571428571428571, 0.845...</td>\n",
       "      <td>[0.17739975698663427, 0.17496962332928312, 0.1...</td>\n",
       "      <td>[0.7352941176470594, 0.7456747404844281, 0.759...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0018796992481203006, 0.0, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>[6.123233995736766e-17, -0.25881904510252063, ...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-2.4492935982947064e-16, -2.4492935982947064e...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>[0.788010753606722, 0.7986355100472928, 0.4999...</td>\n",
       "      <td>[0.6156614753256583, 0.6018150231520484, 0.866...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0.43142220079168486, 0.48218324911068233, 0.5...</td>\n",
       "      <td>0.526439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-07 16:00:00</td>\n",
       "      <td>[0.05526512322628827, 0.05899925317401045, 0.0...</td>\n",
       "      <td>[0.17338072669826224, 0.19075829383886256, 0.1...</td>\n",
       "      <td>[0.02490170380078637, 0.027522935779816515, 0....</td>\n",
       "      <td>[0.4177215189873418, 0.4430379746835442, 0.455...</td>\n",
       "      <td>[0.8571428571428571, 0.8452380952380952, 0.809...</td>\n",
       "      <td>[0.17496962332928312, 0.13122721749696234, 0.1...</td>\n",
       "      <td>[0.7456747404844281, 0.7595155709342563, 0.771...</td>\n",
       "      <td>[0.0, 0.0, 0.0018796992481203006, 0.0, 0.0, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>[-0.25881904510252063, -0.4999999999999998, -0...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-2.4492935982947064e-16, -2.4492935982947064e...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>[0.7986355100472928, 0.49999999999999994, 0.51...</td>\n",
       "      <td>[0.6018150231520484, 0.8660254037844387, 0.857...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0.48218324911068233, 0.5155826260477044, 0.52...</td>\n",
       "      <td>0.531649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-07 17:00:00</td>\n",
       "      <td>[0.05899925317401045, 0.04480955937266617, 0.0...</td>\n",
       "      <td>[0.19075829383886256, 0.1844391785150079, 0.17...</td>\n",
       "      <td>[0.027522935779816515, 0.022280471821756225, 0...</td>\n",
       "      <td>[0.4430379746835442, 0.45569620253164556, 0.48...</td>\n",
       "      <td>[0.8452380952380952, 0.8095238095238095, 0.761...</td>\n",
       "      <td>[0.13122721749696234, 0.13730255164034022, 0.1...</td>\n",
       "      <td>[0.7595155709342563, 0.7716262975778537, 0.771...</td>\n",
       "      <td>[0.0, 0.0018796992481203006, 0.0, 0.0, 0.0, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>[-0.4999999999999998, -0.7071067811865475, -0....</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-2.4492935982947064e-16, -2.4492935982947064e...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>[0.49999999999999994, 0.5150380749100542, 0.57...</td>\n",
       "      <td>[0.8660254037844387, 0.8571673007021123, 0.819...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0.5155826260477044, 0.5251145754621727, 0.501...</td>\n",
       "      <td>0.558058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-08 00:00:00</td>\n",
       "      <td>[0.05526512322628827, 0.05750560119492159, 0.0...</td>\n",
       "      <td>[0.14296998420221171, 0.11690363349131122, 0.0...</td>\n",
       "      <td>[0.03145478374836173, 0.02490170380078637, 0.0...</td>\n",
       "      <td>[0.5474683544303797, 0.5284810126582279, 0.509...</td>\n",
       "      <td>[0.7261904761904762, 0.7380952380952381, 0.773...</td>\n",
       "      <td>[0.1822600243013366, 0.13001215066828675, 0.11...</td>\n",
       "      <td>[0.7145328719723181, 0.7197231833910025, 0.723...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-0.5000000000000004, -0.25881904510252063, -1...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-2.4492935982947064e-16, -2.4492935982947064e...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.623...</td>\n",
       "      <td>[0.6691306063588582, 0.5446390350150271, 0.469...</td>\n",
       "      <td>[0.7431448254773942, 0.838670567945424, 0.8829...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0.5752438812449654, 0.5731764946609242, 0.567...</td>\n",
       "      <td>0.478844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-08 02:00:00</td>\n",
       "      <td>[0.05974607916355489, 0.05725665919840677, 0.0...</td>\n",
       "      <td>[0.09083728278041074, 0.12006319115323855, 0.0...</td>\n",
       "      <td>[0.022280471821756225, 0.022280471821756225, 0...</td>\n",
       "      <td>[0.509493670886076, 0.490506329113924, 0.47151...</td>\n",
       "      <td>[0.7738095238095238, 0.7738095238095238, 0.833...</td>\n",
       "      <td>[0.11907654921020658, 0.13973268529769137, 0.1...</td>\n",
       "      <td>[0.72318339100346, 0.7318339100346019, 0.75259...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00375939...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-1.8369701987210297e-16, 0.2588190451025203, ...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-2.4492935982947064e-16, -2.4492935982947064e...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6234898018587...</td>\n",
       "      <td>[0.4694715627858908, 0.5299192642332049, 0.358...</td>\n",
       "      <td>[0.882947592858927, 0.848048096156426, 0.93358...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0.5679102402506214, 0.5500357285414436, 0.480...</td>\n",
       "      <td>0.522438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows √ó 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  location_id            datetime  \\\n",
       "0      233335 2022-11-05 16:00:00   \n",
       "1      233335 2022-11-05 21:00:00   \n",
       "2      233335 2022-11-06 20:00:00   \n",
       "3      233335 2022-11-07 07:00:00   \n",
       "4      233335 2022-11-07 14:00:00   \n",
       "5      233335 2022-11-07 15:00:00   \n",
       "6      233335 2022-11-07 16:00:00   \n",
       "7      233335 2022-11-07 17:00:00   \n",
       "8      233335 2022-11-08 00:00:00   \n",
       "9      233335 2022-11-08 02:00:00   \n",
       "\n",
       "                                PM10_scaled_sequence  \\\n",
       "0  [0.010953447846651731, 0.03360716952949963, 0....   \n",
       "1  [0.03659447348767737, 0.04530744336569579, 0.0...   \n",
       "2  [0.05402041324371421, 0.06123973114264377, 0.0...   \n",
       "3  [0.029126213592233007, 0.025889967637540454, 0...   \n",
       "4  [0.04605426935524023, 0.0495394573064476, 0.05...   \n",
       "5  [0.0495394573064476, 0.05526512322628827, 0.05...   \n",
       "6  [0.05526512322628827, 0.05899925317401045, 0.0...   \n",
       "7  [0.05899925317401045, 0.04480955937266617, 0.0...   \n",
       "8  [0.05526512322628827, 0.05750560119492159, 0.0...   \n",
       "9  [0.05974607916355489, 0.05725665919840677, 0.0...   \n",
       "\n",
       "                                 NO2_scaled_sequence  \\\n",
       "0  [0.2112954186413902, 0.12796208530805686, 0.09...   \n",
       "1  [0.052132701421800945, 0.046603475513428125, 0...   \n",
       "2  [0.10979462875197474, 0.08412322274881517, 0.1...   \n",
       "3  [0.05726698262243286, 0.08609794628751975, 0.0...   \n",
       "4  [0.09913112164297, 0.13349131121642968, 0.1733...   \n",
       "5  [0.13349131121642968, 0.17338072669826224, 0.1...   \n",
       "6  [0.17338072669826224, 0.19075829383886256, 0.1...   \n",
       "7  [0.19075829383886256, 0.1844391785150079, 0.17...   \n",
       "8  [0.14296998420221171, 0.11690363349131122, 0.0...   \n",
       "9  [0.09083728278041074, 0.12006319115323855, 0.0...   \n",
       "\n",
       "                                 SO2_scaled_sequence  \\\n",
       "0  [0.01834862385321101, 0.014416775884665795, 0....   \n",
       "1  [0.02490170380078637, 0.030144167758846655, 0....   \n",
       "2  [0.02883355176933159, 0.030144167758846655, 0....   \n",
       "3  [0.01179554390563565, 0.014416775884665795, 0....   \n",
       "4  [0.01703800786369594, 0.01834862385321101, 0.0...   \n",
       "5  [0.01834862385321101, 0.02490170380078637, 0.0...   \n",
       "6  [0.02490170380078637, 0.027522935779816515, 0....   \n",
       "7  [0.027522935779816515, 0.022280471821756225, 0...   \n",
       "8  [0.03145478374836173, 0.02490170380078637, 0.0...   \n",
       "9  [0.022280471821756225, 0.022280471821756225, 0...   \n",
       "\n",
       "                      temperature_2m_scaled_sequence  \\\n",
       "0  [0.5221518987341772, 0.5379746835443038, 0.553...   \n",
       "1  [0.5284810126582279, 0.5348101265822784, 0.534...   \n",
       "2  [0.5727848101265823, 0.5727848101265823, 0.563...   \n",
       "3  [0.4746835443037974, 0.46518987341772156, 0.45...   \n",
       "4  [0.42721518987341767, 0.42088607594936706, 0.4...   \n",
       "5  [0.42088607594936706, 0.4177215189873418, 0.44...   \n",
       "6  [0.4177215189873418, 0.4430379746835442, 0.455...   \n",
       "7  [0.4430379746835442, 0.45569620253164556, 0.48...   \n",
       "8  [0.5474683544303797, 0.5284810126582279, 0.509...   \n",
       "9  [0.509493670886076, 0.490506329113924, 0.47151...   \n",
       "\n",
       "                relative_humidity_2m_scaled_sequence  \\\n",
       "0  [0.9404761904761905, 0.8928571428571429, 0.857...   \n",
       "1  [0.9523809523809523, 0.9523809523809523, 0.952...   \n",
       "2  [0.8452380952380952, 0.8452380952380952, 0.857...   \n",
       "3  [0.8809523809523809, 0.8690476190476191, 0.857...   \n",
       "4  [0.8571428571428571, 0.8571428571428571, 0.857...   \n",
       "5  [0.8571428571428571, 0.8571428571428571, 0.845...   \n",
       "6  [0.8571428571428571, 0.8452380952380952, 0.809...   \n",
       "7  [0.8452380952380952, 0.8095238095238095, 0.761...   \n",
       "8  [0.7261904761904762, 0.7380952380952381, 0.773...   \n",
       "9  [0.7738095238095238, 0.7738095238095238, 0.833...   \n",
       "\n",
       "                      wind_speed_10m_scaled_sequence  \\\n",
       "0  [0.14094775212636695, 0.20534629404617252, 0.2...   \n",
       "1  [0.11421628189550426, 0.12636695018226005, 0.1...   \n",
       "2  [0.15188335358444716, 0.14823815309842042, 0.1...   \n",
       "3  [0.15795868772782504, 0.15795868772782504, 0.1...   \n",
       "4  [0.17982989064398544, 0.17739975698663427, 0.1...   \n",
       "5  [0.17739975698663427, 0.17496962332928312, 0.1...   \n",
       "6  [0.17496962332928312, 0.13122721749696234, 0.1...   \n",
       "7  [0.13122721749696234, 0.13730255164034022, 0.1...   \n",
       "8  [0.1822600243013366, 0.13001215066828675, 0.11...   \n",
       "9  [0.11907654921020658, 0.13973268529769137, 0.1...   \n",
       "\n",
       "                    surface_pressure_scaled_sequence  \\\n",
       "0  [0.6020761245674737, 0.6280276816608994, 0.621...   \n",
       "1  [0.6591695501730094, 0.6678200692041512, 0.676...   \n",
       "2  [0.6747404844290663, 0.6782006920415219, 0.685...   \n",
       "3  [0.7404844290657437, 0.7439446366782013, 0.740...   \n",
       "4  [0.7249134948096888, 0.7352941176470594, 0.745...   \n",
       "5  [0.7352941176470594, 0.7456747404844281, 0.759...   \n",
       "6  [0.7456747404844281, 0.7595155709342563, 0.771...   \n",
       "7  [0.7595155709342563, 0.7716262975778537, 0.771...   \n",
       "8  [0.7145328719723181, 0.7197231833910025, 0.723...   \n",
       "9  [0.72318339100346, 0.7318339100346019, 0.75259...   \n",
       "\n",
       "                       precipitation_scaled_sequence  ...  \\\n",
       "0  [0.009398496240601503, 0.013157894736842105, 0...  ...   \n",
       "1  [0.011278195488721804, 0.007518796992481203, 0...  ...   \n",
       "2  [0.0037593984962406013, 0.0018796992481203006,...  ...   \n",
       "3  [0.0, 0.0, 0.0018796992481203006, 0.0, 0.0, 0....  ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0018796992481203006, 0....  ...   \n",
       "5  [0.0, 0.0, 0.0, 0.0018796992481203006, 0.0, 0....  ...   \n",
       "6  [0.0, 0.0, 0.0018796992481203006, 0.0, 0.0, 0....  ...   \n",
       "7  [0.0, 0.0018796992481203006, 0.0, 0.0, 0.0, 0....  ...   \n",
       "8  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  ...   \n",
       "9  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00375939...  ...   \n",
       "\n",
       "                                   hour_cos_sequence  \\\n",
       "0  [-0.25881904510252063, -0.9659258262890683, -0...   \n",
       "1  [0.5000000000000001, 0.7071067811865474, 0.866...   \n",
       "2  [-0.7071067811865479, -0.5000000000000004, -0....   \n",
       "3  [0.8660254037844384, 0.9659258262890681, 1.0, ...   \n",
       "4  [0.25881904510252074, 6.123233995736766e-17, -...   \n",
       "5  [6.123233995736766e-17, -0.25881904510252063, ...   \n",
       "6  [-0.25881904510252063, -0.4999999999999998, -0...   \n",
       "7  [-0.4999999999999998, -0.7071067811865475, -0....   \n",
       "8  [-0.5000000000000004, -0.25881904510252063, -1...   \n",
       "9  [-1.8369701987210297e-16, 0.2588190451025203, ...   \n",
       "\n",
       "                                  month_sin_sequence  \\\n",
       "0  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "1  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "2  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "3  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "4  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "5  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "6  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "7  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "8  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "9  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "\n",
       "                                  month_cos_sequence  \\\n",
       "0  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "1  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "2  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "3  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "4  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "5  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "6  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "7  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "8  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "9  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "\n",
       "                            day_of_week_sin_sequence  \\\n",
       "0  [-0.9749279121818236, -0.9749279121818236, -0....   \n",
       "1  [-0.9749279121818236, -0.9749279121818236, -0....   \n",
       "2  [-0.7818314824680299, -0.7818314824680299, -0....   \n",
       "3  [-0.7818314824680299, -0.7818314824680299, -2....   \n",
       "4  [-2.4492935982947064e-16, -2.4492935982947064e...   \n",
       "5  [-2.4492935982947064e-16, -2.4492935982947064e...   \n",
       "6  [-2.4492935982947064e-16, -2.4492935982947064e...   \n",
       "7  [-2.4492935982947064e-16, -2.4492935982947064e...   \n",
       "8  [-2.4492935982947064e-16, -2.4492935982947064e...   \n",
       "9  [-2.4492935982947064e-16, -2.4492935982947064e...   \n",
       "\n",
       "                            day_of_week_cos_sequence  \\\n",
       "0  [-0.2225209339563146, -0.2225209339563146, -0....   \n",
       "1  [-0.2225209339563146, -0.2225209339563146, -0....   \n",
       "2  [0.6234898018587334, 0.6234898018587334, 0.623...   \n",
       "3  [0.6234898018587334, 0.6234898018587334, 1.0, ...   \n",
       "4  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n",
       "5  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n",
       "6  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n",
       "7  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n",
       "8  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.623...   \n",
       "9  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6234898018587...   \n",
       "\n",
       "                         wind_direction_sin_sequence  \\\n",
       "0  [0.9961946980917455, 0.9993908270190958, 0.994...   \n",
       "1  [1.0, 0.9993908270190958, 0.9925461516413221, ...   \n",
       "2  [0.754709580222772, 0.8290375725550417, 0.7880...   \n",
       "3  [0.6946583704589973, 0.6946583704589973, 0.694...   \n",
       "4  [0.7771459614569708, 0.788010753606722, 0.7986...   \n",
       "5  [0.788010753606722, 0.7986355100472928, 0.4999...   \n",
       "6  [0.7986355100472928, 0.49999999999999994, 0.51...   \n",
       "7  [0.49999999999999994, 0.5150380749100542, 0.57...   \n",
       "8  [0.6691306063588582, 0.5446390350150271, 0.469...   \n",
       "9  [0.4694715627858908, 0.5299192642332049, 0.358...   \n",
       "\n",
       "                         wind_direction_cos_sequence  \\\n",
       "0  [-0.08715574274765824, -0.03489949670250073, -...   \n",
       "1  [6.123233995736766e-17, 0.03489949670250108, -...   \n",
       "2  [0.6560590289905073, 0.5591929034707468, 0.615...   \n",
       "3  [0.7193398003386512, 0.7193398003386512, 0.719...   \n",
       "4  [0.6293203910498375, 0.6156614753256583, 0.601...   \n",
       "5  [0.6156614753256583, 0.6018150231520484, 0.866...   \n",
       "6  [0.6018150231520484, 0.8660254037844387, 0.857...   \n",
       "7  [0.8660254037844387, 0.8571673007021123, 0.819...   \n",
       "8  [0.7431448254773942, 0.838670567945424, 0.8829...   \n",
       "9  [0.882947592858927, 0.848048096156426, 0.93358...   \n",
       "\n",
       "                                 is_weekend_sequence  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "5  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "6  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "7  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "8  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "9  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                           PM2_5_log_scaled_sequence target_value  \n",
       "0  [0.2658888298411102, 0.40379514201938105, 0.41...     0.575244  \n",
       "1  [0.5026014988639874, 0.5500357285414436, 0.559...     0.490291  \n",
       "2  [0.5416701480187393, 0.5657633372272116, 0.558...     0.537972  \n",
       "3  [0.422640218551877, 0.3856687264826328, 0.3716...     0.502601  \n",
       "4  [0.4902908942160261, 0.43142220079168486, 0.48...     0.542887  \n",
       "5  [0.43142220079168486, 0.48218324911068233, 0.5...     0.526439  \n",
       "6  [0.48218324911068233, 0.5155826260477044, 0.52...     0.531649  \n",
       "7  [0.5155826260477044, 0.5251145754621727, 0.501...     0.558058  \n",
       "8  [0.5752438812449654, 0.5731764946609242, 0.567...     0.478844  \n",
       "9  [0.5679102402506214, 0.5500357285414436, 0.480...     0.522438  \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:34:53.214038Z",
     "iopub.status.busy": "2025-12-01T16:34:53.213715Z",
     "iopub.status.idle": "2025-12-01T16:34:53.222779Z",
     "shell.execute_reply": "2025-12-01T16:34:53.221472Z",
     "shell.execute_reply.started": "2025-12-01T16:34:53.214017Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5026015 , 0.55003573, 0.55917734, 0.54886235, 0.53921268,\n",
       "       0.54768189, 0.56897491, 0.57317649, 0.56359258, 0.55693172,\n",
       "       0.59386427, 0.62088433, 0.62570843, 0.58913362, 0.52108626,\n",
       "       0.49029089, 0.49654315, 0.52243828, 0.51972482, 0.54167015,\n",
       "       0.56576334, 0.55805776, 0.55579916, 0.53797225, 0.49959581,\n",
       "       0.49029089, 0.42264022, 0.38566873, 0.37160754, 0.3745019 ,\n",
       "       0.4612311 , 0.50556165, 0.53293027, 0.49029089, 0.4314222 ,\n",
       "       0.48218325, 0.51558263, 0.52511458, 0.50110443, 0.53035962,\n",
       "       0.48052055, 0.13541754, 0.17103701, 0.57524388, 0.57317649,\n",
       "       0.56791024, 0.55003573, 0.48052055])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_train.iloc[1]['PM2_5_log_scaled_sequence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:30:36.677765Z",
     "iopub.status.busy": "2025-12-01T16:30:36.677402Z",
     "iopub.status.idle": "2025-12-01T16:30:36.774780Z",
     "shell.execute_reply": "2025-12-01T16:30:36.773456Z",
     "shell.execute_reply.started": "2025-12-01T16:30:36.677742Z"
    },
    "papermill": {
     "duration": 0.121219,
     "end_time": "2025-11-20T10:51:30.993810",
     "exception": false,
     "start_time": "2025-11-20T10:51:30.872591",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>PM10_scaled_sequence</th>\n",
       "      <th>NO2_scaled_sequence</th>\n",
       "      <th>SO2_scaled_sequence</th>\n",
       "      <th>temperature_2m_scaled_sequence</th>\n",
       "      <th>relative_humidity_2m_scaled_sequence</th>\n",
       "      <th>wind_speed_10m_scaled_sequence</th>\n",
       "      <th>surface_pressure_scaled_sequence</th>\n",
       "      <th>precipitation_scaled_sequence</th>\n",
       "      <th>...</th>\n",
       "      <th>hour_cos_sequence</th>\n",
       "      <th>month_sin_sequence</th>\n",
       "      <th>month_cos_sequence</th>\n",
       "      <th>day_of_week_sin_sequence</th>\n",
       "      <th>day_of_week_cos_sequence</th>\n",
       "      <th>wind_direction_sin_sequence</th>\n",
       "      <th>wind_direction_cos_sequence</th>\n",
       "      <th>is_weekend_sequence</th>\n",
       "      <th>PM2_5_log_scaled_sequence</th>\n",
       "      <th>target_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-04 15:00:00</td>\n",
       "      <td>[0.010953447846651731, 0.03360716952949963, 0....</td>\n",
       "      <td>[0.2112954186413902, 0.12796208530805686, 0.09...</td>\n",
       "      <td>[0.01834862385321101, 0.014416775884665795, 0....</td>\n",
       "      <td>[0.5221518987341772, 0.5379746835443038, 0.553...</td>\n",
       "      <td>[0.9404761904761905, 0.8928571428571429, 0.857...</td>\n",
       "      <td>[0.14094775212636695, 0.20534629404617252, 0.2...</td>\n",
       "      <td>[0.6020761245674737, 0.6280276816608994, 0.621...</td>\n",
       "      <td>[0.009398496240601503, 0.013157894736842105, 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-0.25881904510252063, -0.9659258262890683, -0...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-0.9749279121818236, -0.9749279121818236, -0....</td>\n",
       "      <td>[-0.2225209339563146, -0.2225209339563146, -0....</td>\n",
       "      <td>[0.9961946980917455, 0.9993908270190958, 0.994...</td>\n",
       "      <td>[-0.08715574274765824, -0.03489949670250073, -...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.2658888298411102, 0.40379514201938105, 0.41...</td>\n",
       "      <td>0.541670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-04 18:00:00</td>\n",
       "      <td>[0.026387851630570076, 0.02414737366193677, 0....</td>\n",
       "      <td>[0.08056872037914692, 0.07187993680884676, 0.0...</td>\n",
       "      <td>[0.01179554390563565, 0.01834862385321101, 0.0...</td>\n",
       "      <td>[0.5474683544303797, 0.5158227848101264, 0.528...</td>\n",
       "      <td>[0.8928571428571429, 0.9642857142857143, 0.952...</td>\n",
       "      <td>[0.1968408262454435, 0.1057108140947752, 0.114...</td>\n",
       "      <td>[0.6193771626297575, 0.6435986159169543, 0.659...</td>\n",
       "      <td>[0.06203007518796992, 0.015037593984962405, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>[-0.7071067811865479, 0.2588190451025203, 0.50...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-0.9749279121818236, -0.9749279121818236, -0....</td>\n",
       "      <td>[-0.2225209339563146, -0.2225209339563146, -0....</td>\n",
       "      <td>[0.9781476007338057, 0.7071067811865475, 1.0, ...</td>\n",
       "      <td>[-0.20791169081775912, 0.7071067811865476, 6.1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.39878547755870103, 0.4575097227627197, 0.50...</td>\n",
       "      <td>0.555799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-04 20:00:00</td>\n",
       "      <td>[0.03659447348767737, 0.04530744336569579, 0.0...</td>\n",
       "      <td>[0.052132701421800945, 0.046603475513428125, 0...</td>\n",
       "      <td>[0.02490170380078637, 0.030144167758846655, 0....</td>\n",
       "      <td>[0.5284810126582279, 0.5348101265822784, 0.534...</td>\n",
       "      <td>[0.9523809523809523, 0.9523809523809523, 0.952...</td>\n",
       "      <td>[0.11421628189550426, 0.12636695018226005, 0.1...</td>\n",
       "      <td>[0.6591695501730094, 0.6678200692041512, 0.676...</td>\n",
       "      <td>[0.011278195488721804, 0.007518796992481203, 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.5000000000000001, 0.7071067811865474, 0.866...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-0.9749279121818236, -0.9749279121818236, -0....</td>\n",
       "      <td>[-0.2225209339563146, -0.2225209339563146, -0....</td>\n",
       "      <td>[1.0, 0.9993908270190958, 0.9925461516413221, ...</td>\n",
       "      <td>[6.123233995736766e-17, 0.03489949670250108, -...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.5026014988639874, 0.5500357285414436, 0.559...</td>\n",
       "      <td>0.499596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-04 23:00:00</td>\n",
       "      <td>[0.045058501369180985, 0.041573313417973615, 0...</td>\n",
       "      <td>[0.12085308056872039, 0.17890995260663506, 0.1...</td>\n",
       "      <td>[0.02883355176933159, 0.0327653997378768, 0.03...</td>\n",
       "      <td>[0.5379746835443038, 0.531645569620253, 0.5221...</td>\n",
       "      <td>[0.9523809523809523, 0.9642857142857143, 0.964...</td>\n",
       "      <td>[0.14094775212636695, 0.11057108140947752, 0.1...</td>\n",
       "      <td>[0.6747404844290663, 0.66955017301038, 0.66435...</td>\n",
       "      <td>[0.015037593984962405, 0.03759398496240601, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.9659258262890681, 1.0, 0.9659258262890683, ...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-0.9749279121818236, -0.7818314824680299, -0....</td>\n",
       "      <td>[-0.2225209339563146, 0.6234898018587334, 0.62...</td>\n",
       "      <td>[0.9925461516413221, 0.9876883405951378, 0.819...</td>\n",
       "      <td>[-0.12186934340514737, 0.15643446504023092, 0....</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.5488623543891202, 0.5392126790240658, 0.547...</td>\n",
       "      <td>0.385669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-05 02:00:00</td>\n",
       "      <td>[0.058252427184466014, 0.05899925317401045, 0....</td>\n",
       "      <td>[0.14139020537124802, 0.13112164296998421, 0.1...</td>\n",
       "      <td>[0.03669724770642202, 0.03669724770642202, 0.0...</td>\n",
       "      <td>[0.5221518987341772, 0.5253164556962026, 0.525...</td>\n",
       "      <td>[0.9642857142857143, 0.9523809523809523, 0.928...</td>\n",
       "      <td>[0.13001215066828675, 0.16403402187120292, 0.1...</td>\n",
       "      <td>[0.6608996539792381, 0.6522491349480962, 0.653...</td>\n",
       "      <td>[0.013157894736842105, 0.005639097744360902, 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.8660254037844387, 0.7071067811865476, 0.500...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-0.7818314824680299, -0.7818314824680299, -0....</td>\n",
       "      <td>[0.6234898018587334, 0.6234898018587334, 0.623...</td>\n",
       "      <td>[0.9396926207859083, 0.9876883405951378, 0.970...</td>\n",
       "      <td>[0.3420201433256688, 0.15643446504023092, 0.24...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.5689749088750778, 0.5731764946609242, 0.563...</td>\n",
       "      <td>0.461231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-05 16:00:00</td>\n",
       "      <td>[0.05402041324371421, 0.06123973114264377, 0.0...</td>\n",
       "      <td>[0.10979462875197474, 0.08412322274881517, 0.1...</td>\n",
       "      <td>[0.02883355176933159, 0.030144167758846655, 0....</td>\n",
       "      <td>[0.5727848101265823, 0.5727848101265823, 0.563...</td>\n",
       "      <td>[0.8452380952380952, 0.8452380952380952, 0.857...</td>\n",
       "      <td>[0.15188335358444716, 0.14823815309842042, 0.1...</td>\n",
       "      <td>[0.6747404844290663, 0.6782006920415219, 0.685...</td>\n",
       "      <td>[0.0037593984962406013, 0.0018796992481203006,...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-0.7071067811865479, -0.5000000000000004, -0....</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-0.7818314824680299, -0.7818314824680299, -0....</td>\n",
       "      <td>[0.6234898018587334, 0.6234898018587334, 0.623...</td>\n",
       "      <td>[0.754709580222772, 0.8290375725550417, 0.7880...</td>\n",
       "      <td>[0.6560590289905073, 0.5591929034707468, 0.615...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0.5416701480187393, 0.5657633372272116, 0.558...</td>\n",
       "      <td>0.575244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-05 21:00:00</td>\n",
       "      <td>[0.04655215334826985, 0.03410505352252925, 0.0...</td>\n",
       "      <td>[0.062401263823064775, 0.0703001579778831, 0.0...</td>\n",
       "      <td>[0.01834862385321101, 0.020969855832241157, 0....</td>\n",
       "      <td>[0.4841772151898734, 0.4810126582278481, 0.474...</td>\n",
       "      <td>[0.8809523809523809, 0.8928571428571429, 0.880...</td>\n",
       "      <td>[0.1543134872417983, 0.1543134872417983, 0.157...</td>\n",
       "      <td>[0.7214532871972312, 0.7318339100346019, 0.740...</td>\n",
       "      <td>[0.0018796992481203006, 0.0, 0.0, 0.0, 0.00187...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.5000000000000001, 0.7071067811865474, 0.866...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-0.7818314824680299, -0.7818314824680299, -0....</td>\n",
       "      <td>[0.6234898018587334, 0.6234898018587334, 0.623...</td>\n",
       "      <td>[0.6819983600624985, 0.6819983600624985, 0.694...</td>\n",
       "      <td>[0.7313537016191706, 0.7313537016191706, 0.719...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0.4995958055932217, 0.4902908942160261, 0.422...</td>\n",
       "      <td>0.490291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-06 20:00:00</td>\n",
       "      <td>[0.05526512322628827, 0.05750560119492159, 0.0...</td>\n",
       "      <td>[0.14296998420221171, 0.11690363349131122, 0.0...</td>\n",
       "      <td>[0.03145478374836173, 0.02490170380078637, 0.0...</td>\n",
       "      <td>[0.5474683544303797, 0.5284810126582279, 0.509...</td>\n",
       "      <td>[0.7261904761904762, 0.7380952380952381, 0.773...</td>\n",
       "      <td>[0.1822600243013366, 0.13001215066828675, 0.11...</td>\n",
       "      <td>[0.7145328719723181, 0.7197231833910025, 0.723...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-0.5000000000000004, -0.25881904510252063, -1...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-2.4492935982947064e-16, -2.4492935982947064e...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.623...</td>\n",
       "      <td>[0.6691306063588582, 0.5446390350150271, 0.469...</td>\n",
       "      <td>[0.7431448254773942, 0.838670567945424, 0.8829...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0.5752438812449654, 0.5731764946609242, 0.567...</td>\n",
       "      <td>0.537972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-07 07:00:00</td>\n",
       "      <td>[0.05526512322628827, 0.05601194921583271, 0.0...</td>\n",
       "      <td>[0.11690363349131122, 0.11216429699842022, 0.1...</td>\n",
       "      <td>[0.01703800786369594, 0.020969855832241157, 0....</td>\n",
       "      <td>[0.46202531645569617, 0.45569620253164556, 0.4...</td>\n",
       "      <td>[0.8214285714285714, 0.8214285714285714, 0.833...</td>\n",
       "      <td>[0.1275820170109356, 0.10814094775212638, 0.11...</td>\n",
       "      <td>[0.761245674740485, 0.7595155709342563, 0.7560...</td>\n",
       "      <td>[0.0, 0.0, 0.0037593984962406013, 0.0018796992...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.9659258262890681, 1.0, 0.9659258262890683, ...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-2.4492935982947064e-16, 0.7818314824680298, ...</td>\n",
       "      <td>[1.0, 0.6234898018587336, 0.6234898018587336, ...</td>\n",
       "      <td>[0.6560590289905072, 0.5299192642332049, 0.629...</td>\n",
       "      <td>[0.7547095802227721, 0.848048096156426, 0.7771...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0.5793141578229432, 0.5591773357499615, 0.521...</td>\n",
       "      <td>0.502601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-07 14:00:00</td>\n",
       "      <td>[0.05850136918098083, 0.04680109534478467, 0.0...</td>\n",
       "      <td>[0.1358609794628752, 0.18957345971563982, 0.17...</td>\n",
       "      <td>[0.014416775884665795, 0.01834862385321101, 0....</td>\n",
       "      <td>[0.4525316455696202, 0.4683544303797468, 0.487...</td>\n",
       "      <td>[0.8690476190476191, 0.8571428571428571, 0.797...</td>\n",
       "      <td>[0.1069258809234508, 0.11907654921020658, 0.13...</td>\n",
       "      <td>[0.7785467128027688, 0.7716262975778537, 0.750...</td>\n",
       "      <td>[0.015037593984962405, 0.06954887218045112, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>[-0.8660254037844387, -0.9659258262890682, -1....</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[0.7818314824680298, 0.7818314824680298, 0.781...</td>\n",
       "      <td>[0.6234898018587336, 0.6234898018587336, 0.623...</td>\n",
       "      <td>[0.8191520442889918, 0.8090169943749475, 0.719...</td>\n",
       "      <td>[0.5735764363510462, 0.5877852522924731, 0.694...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[0.5155826260477044, 0.5303596209261283, 0.522...</td>\n",
       "      <td>0.542887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows √ó 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  location_id            datetime  \\\n",
       "0      233335 2022-11-04 15:00:00   \n",
       "1      233335 2022-11-04 18:00:00   \n",
       "2      233335 2022-11-04 20:00:00   \n",
       "3      233335 2022-11-04 23:00:00   \n",
       "4      233335 2022-11-05 02:00:00   \n",
       "5      233335 2022-11-05 16:00:00   \n",
       "6      233335 2022-11-05 21:00:00   \n",
       "7      233335 2022-11-06 20:00:00   \n",
       "8      233335 2022-11-07 07:00:00   \n",
       "9      233335 2022-11-07 14:00:00   \n",
       "\n",
       "                                PM10_scaled_sequence  \\\n",
       "0  [0.010953447846651731, 0.03360716952949963, 0....   \n",
       "1  [0.026387851630570076, 0.02414737366193677, 0....   \n",
       "2  [0.03659447348767737, 0.04530744336569579, 0.0...   \n",
       "3  [0.045058501369180985, 0.041573313417973615, 0...   \n",
       "4  [0.058252427184466014, 0.05899925317401045, 0....   \n",
       "5  [0.05402041324371421, 0.06123973114264377, 0.0...   \n",
       "6  [0.04655215334826985, 0.03410505352252925, 0.0...   \n",
       "7  [0.05526512322628827, 0.05750560119492159, 0.0...   \n",
       "8  [0.05526512322628827, 0.05601194921583271, 0.0...   \n",
       "9  [0.05850136918098083, 0.04680109534478467, 0.0...   \n",
       "\n",
       "                                 NO2_scaled_sequence  \\\n",
       "0  [0.2112954186413902, 0.12796208530805686, 0.09...   \n",
       "1  [0.08056872037914692, 0.07187993680884676, 0.0...   \n",
       "2  [0.052132701421800945, 0.046603475513428125, 0...   \n",
       "3  [0.12085308056872039, 0.17890995260663506, 0.1...   \n",
       "4  [0.14139020537124802, 0.13112164296998421, 0.1...   \n",
       "5  [0.10979462875197474, 0.08412322274881517, 0.1...   \n",
       "6  [0.062401263823064775, 0.0703001579778831, 0.0...   \n",
       "7  [0.14296998420221171, 0.11690363349131122, 0.0...   \n",
       "8  [0.11690363349131122, 0.11216429699842022, 0.1...   \n",
       "9  [0.1358609794628752, 0.18957345971563982, 0.17...   \n",
       "\n",
       "                                 SO2_scaled_sequence  \\\n",
       "0  [0.01834862385321101, 0.014416775884665795, 0....   \n",
       "1  [0.01179554390563565, 0.01834862385321101, 0.0...   \n",
       "2  [0.02490170380078637, 0.030144167758846655, 0....   \n",
       "3  [0.02883355176933159, 0.0327653997378768, 0.03...   \n",
       "4  [0.03669724770642202, 0.03669724770642202, 0.0...   \n",
       "5  [0.02883355176933159, 0.030144167758846655, 0....   \n",
       "6  [0.01834862385321101, 0.020969855832241157, 0....   \n",
       "7  [0.03145478374836173, 0.02490170380078637, 0.0...   \n",
       "8  [0.01703800786369594, 0.020969855832241157, 0....   \n",
       "9  [0.014416775884665795, 0.01834862385321101, 0....   \n",
       "\n",
       "                      temperature_2m_scaled_sequence  \\\n",
       "0  [0.5221518987341772, 0.5379746835443038, 0.553...   \n",
       "1  [0.5474683544303797, 0.5158227848101264, 0.528...   \n",
       "2  [0.5284810126582279, 0.5348101265822784, 0.534...   \n",
       "3  [0.5379746835443038, 0.531645569620253, 0.5221...   \n",
       "4  [0.5221518987341772, 0.5253164556962026, 0.525...   \n",
       "5  [0.5727848101265823, 0.5727848101265823, 0.563...   \n",
       "6  [0.4841772151898734, 0.4810126582278481, 0.474...   \n",
       "7  [0.5474683544303797, 0.5284810126582279, 0.509...   \n",
       "8  [0.46202531645569617, 0.45569620253164556, 0.4...   \n",
       "9  [0.4525316455696202, 0.4683544303797468, 0.487...   \n",
       "\n",
       "                relative_humidity_2m_scaled_sequence  \\\n",
       "0  [0.9404761904761905, 0.8928571428571429, 0.857...   \n",
       "1  [0.8928571428571429, 0.9642857142857143, 0.952...   \n",
       "2  [0.9523809523809523, 0.9523809523809523, 0.952...   \n",
       "3  [0.9523809523809523, 0.9642857142857143, 0.964...   \n",
       "4  [0.9642857142857143, 0.9523809523809523, 0.928...   \n",
       "5  [0.8452380952380952, 0.8452380952380952, 0.857...   \n",
       "6  [0.8809523809523809, 0.8928571428571429, 0.880...   \n",
       "7  [0.7261904761904762, 0.7380952380952381, 0.773...   \n",
       "8  [0.8214285714285714, 0.8214285714285714, 0.833...   \n",
       "9  [0.8690476190476191, 0.8571428571428571, 0.797...   \n",
       "\n",
       "                      wind_speed_10m_scaled_sequence  \\\n",
       "0  [0.14094775212636695, 0.20534629404617252, 0.2...   \n",
       "1  [0.1968408262454435, 0.1057108140947752, 0.114...   \n",
       "2  [0.11421628189550426, 0.12636695018226005, 0.1...   \n",
       "3  [0.14094775212636695, 0.11057108140947752, 0.1...   \n",
       "4  [0.13001215066828675, 0.16403402187120292, 0.1...   \n",
       "5  [0.15188335358444716, 0.14823815309842042, 0.1...   \n",
       "6  [0.1543134872417983, 0.1543134872417983, 0.157...   \n",
       "7  [0.1822600243013366, 0.13001215066828675, 0.11...   \n",
       "8  [0.1275820170109356, 0.10814094775212638, 0.11...   \n",
       "9  [0.1069258809234508, 0.11907654921020658, 0.13...   \n",
       "\n",
       "                    surface_pressure_scaled_sequence  \\\n",
       "0  [0.6020761245674737, 0.6280276816608994, 0.621...   \n",
       "1  [0.6193771626297575, 0.6435986159169543, 0.659...   \n",
       "2  [0.6591695501730094, 0.6678200692041512, 0.676...   \n",
       "3  [0.6747404844290663, 0.66955017301038, 0.66435...   \n",
       "4  [0.6608996539792381, 0.6522491349480962, 0.653...   \n",
       "5  [0.6747404844290663, 0.6782006920415219, 0.685...   \n",
       "6  [0.7214532871972312, 0.7318339100346019, 0.740...   \n",
       "7  [0.7145328719723181, 0.7197231833910025, 0.723...   \n",
       "8  [0.761245674740485, 0.7595155709342563, 0.7560...   \n",
       "9  [0.7785467128027688, 0.7716262975778537, 0.750...   \n",
       "\n",
       "                       precipitation_scaled_sequence  ...  \\\n",
       "0  [0.009398496240601503, 0.013157894736842105, 0...  ...   \n",
       "1  [0.06203007518796992, 0.015037593984962405, 0....  ...   \n",
       "2  [0.011278195488721804, 0.007518796992481203, 0...  ...   \n",
       "3  [0.015037593984962405, 0.03759398496240601, 0....  ...   \n",
       "4  [0.013157894736842105, 0.005639097744360902, 0...  ...   \n",
       "5  [0.0037593984962406013, 0.0018796992481203006,...  ...   \n",
       "6  [0.0018796992481203006, 0.0, 0.0, 0.0, 0.00187...  ...   \n",
       "7  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  ...   \n",
       "8  [0.0, 0.0, 0.0037593984962406013, 0.0018796992...  ...   \n",
       "9  [0.015037593984962405, 0.06954887218045112, 0....  ...   \n",
       "\n",
       "                                   hour_cos_sequence  \\\n",
       "0  [-0.25881904510252063, -0.9659258262890683, -0...   \n",
       "1  [-0.7071067811865479, 0.2588190451025203, 0.50...   \n",
       "2  [0.5000000000000001, 0.7071067811865474, 0.866...   \n",
       "3  [0.9659258262890681, 1.0, 0.9659258262890683, ...   \n",
       "4  [0.8660254037844387, 0.7071067811865476, 0.500...   \n",
       "5  [-0.7071067811865479, -0.5000000000000004, -0....   \n",
       "6  [0.5000000000000001, 0.7071067811865474, 0.866...   \n",
       "7  [-0.5000000000000004, -0.25881904510252063, -1...   \n",
       "8  [0.9659258262890681, 1.0, 0.9659258262890683, ...   \n",
       "9  [-0.8660254037844387, -0.9659258262890682, -1....   \n",
       "\n",
       "                                  month_sin_sequence  \\\n",
       "0  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "1  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "2  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "3  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "4  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "5  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "6  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "7  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "8  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "9  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "\n",
       "                                  month_cos_sequence  \\\n",
       "0  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "1  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "2  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "3  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "4  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "5  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "6  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "7  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "8  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "9  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "\n",
       "                            day_of_week_sin_sequence  \\\n",
       "0  [-0.9749279121818236, -0.9749279121818236, -0....   \n",
       "1  [-0.9749279121818236, -0.9749279121818236, -0....   \n",
       "2  [-0.9749279121818236, -0.9749279121818236, -0....   \n",
       "3  [-0.9749279121818236, -0.7818314824680299, -0....   \n",
       "4  [-0.7818314824680299, -0.7818314824680299, -0....   \n",
       "5  [-0.7818314824680299, -0.7818314824680299, -0....   \n",
       "6  [-0.7818314824680299, -0.7818314824680299, -0....   \n",
       "7  [-2.4492935982947064e-16, -2.4492935982947064e...   \n",
       "8  [-2.4492935982947064e-16, 0.7818314824680298, ...   \n",
       "9  [0.7818314824680298, 0.7818314824680298, 0.781...   \n",
       "\n",
       "                            day_of_week_cos_sequence  \\\n",
       "0  [-0.2225209339563146, -0.2225209339563146, -0....   \n",
       "1  [-0.2225209339563146, -0.2225209339563146, -0....   \n",
       "2  [-0.2225209339563146, -0.2225209339563146, -0....   \n",
       "3  [-0.2225209339563146, 0.6234898018587334, 0.62...   \n",
       "4  [0.6234898018587334, 0.6234898018587334, 0.623...   \n",
       "5  [0.6234898018587334, 0.6234898018587334, 0.623...   \n",
       "6  [0.6234898018587334, 0.6234898018587334, 0.623...   \n",
       "7  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.623...   \n",
       "8  [1.0, 0.6234898018587336, 0.6234898018587336, ...   \n",
       "9  [0.6234898018587336, 0.6234898018587336, 0.623...   \n",
       "\n",
       "                         wind_direction_sin_sequence  \\\n",
       "0  [0.9961946980917455, 0.9993908270190958, 0.994...   \n",
       "1  [0.9781476007338057, 0.7071067811865475, 1.0, ...   \n",
       "2  [1.0, 0.9993908270190958, 0.9925461516413221, ...   \n",
       "3  [0.9925461516413221, 0.9876883405951378, 0.819...   \n",
       "4  [0.9396926207859083, 0.9876883405951378, 0.970...   \n",
       "5  [0.754709580222772, 0.8290375725550417, 0.7880...   \n",
       "6  [0.6819983600624985, 0.6819983600624985, 0.694...   \n",
       "7  [0.6691306063588582, 0.5446390350150271, 0.469...   \n",
       "8  [0.6560590289905072, 0.5299192642332049, 0.629...   \n",
       "9  [0.8191520442889918, 0.8090169943749475, 0.719...   \n",
       "\n",
       "                         wind_direction_cos_sequence  \\\n",
       "0  [-0.08715574274765824, -0.03489949670250073, -...   \n",
       "1  [-0.20791169081775912, 0.7071067811865476, 6.1...   \n",
       "2  [6.123233995736766e-17, 0.03489949670250108, -...   \n",
       "3  [-0.12186934340514737, 0.15643446504023092, 0....   \n",
       "4  [0.3420201433256688, 0.15643446504023092, 0.24...   \n",
       "5  [0.6560590289905073, 0.5591929034707468, 0.615...   \n",
       "6  [0.7313537016191706, 0.7313537016191706, 0.719...   \n",
       "7  [0.7431448254773942, 0.838670567945424, 0.8829...   \n",
       "8  [0.7547095802227721, 0.848048096156426, 0.7771...   \n",
       "9  [0.5735764363510462, 0.5877852522924731, 0.694...   \n",
       "\n",
       "                                 is_weekend_sequence  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "5  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...   \n",
       "6  [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "7  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "8  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "9  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...   \n",
       "\n",
       "                           PM2_5_log_scaled_sequence target_value  \n",
       "0  [0.2658888298411102, 0.40379514201938105, 0.41...     0.541670  \n",
       "1  [0.39878547755870103, 0.4575097227627197, 0.50...     0.555799  \n",
       "2  [0.5026014988639874, 0.5500357285414436, 0.559...     0.499596  \n",
       "3  [0.5488623543891202, 0.5392126790240658, 0.547...     0.385669  \n",
       "4  [0.5689749088750778, 0.5731764946609242, 0.563...     0.461231  \n",
       "5  [0.5416701480187393, 0.5657633372272116, 0.558...     0.575244  \n",
       "6  [0.4995958055932217, 0.4902908942160261, 0.422...     0.490291  \n",
       "7  [0.5752438812449654, 0.5731764946609242, 0.567...     0.537972  \n",
       "8  [0.5793141578229432, 0.5591773357499615, 0.521...     0.502601  \n",
       "9  [0.5155826260477044, 0.5303596209261283, 0.522...     0.542887  \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:30:36.776572Z",
     "iopub.status.busy": "2025-12-01T16:30:36.776190Z",
     "iopub.status.idle": "2025-12-01T16:30:36.807767Z",
     "shell.execute_reply": "2025-12-01T16:30:36.806412Z",
     "shell.execute_reply.started": "2025-12-01T16:30:36.776543Z"
    },
    "papermill": {
     "duration": 0.081881,
     "end_time": "2025-11-20T10:51:31.126678",
     "exception": false,
     "start_time": "2025-11-20T10:51:31.044797",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>PM2_5_log_scaled</th>\n",
       "      <th>PM10_scaled</th>\n",
       "      <th>NO2_scaled</th>\n",
       "      <th>SO2_scaled</th>\n",
       "      <th>temperature_2m_scaled</th>\n",
       "      <th>relative_humidity_2m_scaled</th>\n",
       "      <th>wind_speed_10m_scaled</th>\n",
       "      <th>surface_pressure_scaled</th>\n",
       "      <th>...</th>\n",
       "      <th>surface_pressure_lag3_scaled</th>\n",
       "      <th>surface_pressure_lag6_scaled</th>\n",
       "      <th>surface_pressure_lag12_scaled</th>\n",
       "      <th>surface_pressure_lag24_scaled</th>\n",
       "      <th>precipitation_lag1_scaled</th>\n",
       "      <th>precipitation_lag2_scaled</th>\n",
       "      <th>precipitation_lag3_scaled</th>\n",
       "      <th>precipitation_lag6_scaled</th>\n",
       "      <th>precipitation_lag12_scaled</th>\n",
       "      <th>precipitation_lag24_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7727</td>\n",
       "      <td>2022-11-02 02:00:00</td>\n",
       "      <td>0.280367</td>\n",
       "      <td>0.019417</td>\n",
       "      <td>0.183649</td>\n",
       "      <td>0.065531</td>\n",
       "      <td>0.490506</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.397327</td>\n",
       "      <td>0.532872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557093</td>\n",
       "      <td>0.555363</td>\n",
       "      <td>0.576125</td>\n",
       "      <td>0.572664</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.016917</td>\n",
       "      <td>0.033835</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7727</td>\n",
       "      <td>2022-11-02 04:00:00</td>\n",
       "      <td>0.051257</td>\n",
       "      <td>0.012198</td>\n",
       "      <td>0.182859</td>\n",
       "      <td>0.066841</td>\n",
       "      <td>0.468354</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.391252</td>\n",
       "      <td>0.522491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.553633</td>\n",
       "      <td>0.562284</td>\n",
       "      <td>0.562284</td>\n",
       "      <td>0.565744</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.016917</td>\n",
       "      <td>0.067669</td>\n",
       "      <td>0.005639</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7727</td>\n",
       "      <td>2022-11-02 06:00:00</td>\n",
       "      <td>0.541670</td>\n",
       "      <td>0.039582</td>\n",
       "      <td>0.170221</td>\n",
       "      <td>0.068152</td>\n",
       "      <td>0.462025</td>\n",
       "      <td>0.702381</td>\n",
       "      <td>0.385176</td>\n",
       "      <td>0.525952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546713</td>\n",
       "      <td>0.560554</td>\n",
       "      <td>0.536332</td>\n",
       "      <td>0.550173</td>\n",
       "      <td>0.009398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.016917</td>\n",
       "      <td>0.001880</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7727</td>\n",
       "      <td>2022-11-02 07:00:00</td>\n",
       "      <td>0.518354</td>\n",
       "      <td>0.040578</td>\n",
       "      <td>0.159163</td>\n",
       "      <td>0.070773</td>\n",
       "      <td>0.462025</td>\n",
       "      <td>0.702381</td>\n",
       "      <td>0.381531</td>\n",
       "      <td>0.538062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.532872</td>\n",
       "      <td>0.557093</td>\n",
       "      <td>0.534602</td>\n",
       "      <td>0.539792</td>\n",
       "      <td>0.005639</td>\n",
       "      <td>0.009398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7727</td>\n",
       "      <td>2022-11-02 10:00:00</td>\n",
       "      <td>0.356455</td>\n",
       "      <td>0.021409</td>\n",
       "      <td>0.095182</td>\n",
       "      <td>0.065531</td>\n",
       "      <td>0.477848</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.393682</td>\n",
       "      <td>0.584775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.522491</td>\n",
       "      <td>0.553633</td>\n",
       "      <td>0.536332</td>\n",
       "      <td>0.536332</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.005639</td>\n",
       "      <td>0.009398</td>\n",
       "      <td>0.016917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7727</td>\n",
       "      <td>2022-11-02 11:00:00</td>\n",
       "      <td>0.362658</td>\n",
       "      <td>0.024396</td>\n",
       "      <td>0.084123</td>\n",
       "      <td>0.070773</td>\n",
       "      <td>0.490506</td>\n",
       "      <td>0.773810</td>\n",
       "      <td>0.398542</td>\n",
       "      <td>0.581315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525952</td>\n",
       "      <td>0.546713</td>\n",
       "      <td>0.541522</td>\n",
       "      <td>0.534602</td>\n",
       "      <td>0.016917</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.005639</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7727</td>\n",
       "      <td>2022-11-02 12:00:00</td>\n",
       "      <td>0.424873</td>\n",
       "      <td>0.028379</td>\n",
       "      <td>0.079384</td>\n",
       "      <td>0.066841</td>\n",
       "      <td>0.493671</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.407047</td>\n",
       "      <td>0.570934</td>\n",
       "      <td>...</td>\n",
       "      <td>0.538062</td>\n",
       "      <td>0.532872</td>\n",
       "      <td>0.555363</td>\n",
       "      <td>0.544983</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.016917</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7727</td>\n",
       "      <td>2022-11-02 13:00:00</td>\n",
       "      <td>0.403795</td>\n",
       "      <td>0.022654</td>\n",
       "      <td>0.090442</td>\n",
       "      <td>0.065531</td>\n",
       "      <td>0.484177</td>\n",
       "      <td>0.773810</td>\n",
       "      <td>0.403402</td>\n",
       "      <td>0.557093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.584775</td>\n",
       "      <td>0.522491</td>\n",
       "      <td>0.562284</td>\n",
       "      <td>0.555363</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.016917</td>\n",
       "      <td>0.009398</td>\n",
       "      <td>0.067669</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7727</td>\n",
       "      <td>2022-11-02 14:00:00</td>\n",
       "      <td>0.368670</td>\n",
       "      <td>0.022903</td>\n",
       "      <td>0.058057</td>\n",
       "      <td>0.068152</td>\n",
       "      <td>0.493671</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.400972</td>\n",
       "      <td>0.544983</td>\n",
       "      <td>...</td>\n",
       "      <td>0.581315</td>\n",
       "      <td>0.525952</td>\n",
       "      <td>0.560554</td>\n",
       "      <td>0.581315</td>\n",
       "      <td>0.001880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.005639</td>\n",
       "      <td>0.016917</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7727</td>\n",
       "      <td>2022-11-02 15:00:00</td>\n",
       "      <td>0.350049</td>\n",
       "      <td>0.021907</td>\n",
       "      <td>0.058057</td>\n",
       "      <td>0.068152</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.386391</td>\n",
       "      <td>0.539792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570934</td>\n",
       "      <td>0.538062</td>\n",
       "      <td>0.557093</td>\n",
       "      <td>0.591696</td>\n",
       "      <td>0.001880</td>\n",
       "      <td>0.001880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.033835</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows √ó 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  location_id            datetime  PM2_5_log_scaled  PM10_scaled  NO2_scaled  \\\n",
       "0        7727 2022-11-02 02:00:00          0.280367     0.019417    0.183649   \n",
       "1        7727 2022-11-02 04:00:00          0.051257     0.012198    0.182859   \n",
       "2        7727 2022-11-02 06:00:00          0.541670     0.039582    0.170221   \n",
       "3        7727 2022-11-02 07:00:00          0.518354     0.040578    0.159163   \n",
       "4        7727 2022-11-02 10:00:00          0.356455     0.021409    0.095182   \n",
       "5        7727 2022-11-02 11:00:00          0.362658     0.024396    0.084123   \n",
       "6        7727 2022-11-02 12:00:00          0.424873     0.028379    0.079384   \n",
       "7        7727 2022-11-02 13:00:00          0.403795     0.022654    0.090442   \n",
       "8        7727 2022-11-02 14:00:00          0.368670     0.022903    0.058057   \n",
       "9        7727 2022-11-02 15:00:00          0.350049     0.021907    0.058057   \n",
       "\n",
       "   SO2_scaled  temperature_2m_scaled  relative_humidity_2m_scaled  \\\n",
       "0    0.065531               0.490506                     0.666667   \n",
       "1    0.066841               0.468354                     0.678571   \n",
       "2    0.068152               0.462025                     0.702381   \n",
       "3    0.070773               0.462025                     0.702381   \n",
       "4    0.065531               0.477848                     0.785714   \n",
       "5    0.070773               0.490506                     0.773810   \n",
       "6    0.066841               0.493671                     0.761905   \n",
       "7    0.065531               0.484177                     0.773810   \n",
       "8    0.068152               0.493671                     0.761905   \n",
       "9    0.068152               0.500000                     0.750000   \n",
       "\n",
       "   wind_speed_10m_scaled  surface_pressure_scaled  ...  \\\n",
       "0               0.397327                 0.532872  ...   \n",
       "1               0.391252                 0.522491  ...   \n",
       "2               0.385176                 0.525952  ...   \n",
       "3               0.381531                 0.538062  ...   \n",
       "4               0.393682                 0.584775  ...   \n",
       "5               0.398542                 0.581315  ...   \n",
       "6               0.407047                 0.570934  ...   \n",
       "7               0.403402                 0.557093  ...   \n",
       "8               0.400972                 0.544983  ...   \n",
       "9               0.386391                 0.539792  ...   \n",
       "\n",
       "   surface_pressure_lag3_scaled  surface_pressure_lag6_scaled  \\\n",
       "0                      0.557093                      0.555363   \n",
       "1                      0.553633                      0.562284   \n",
       "2                      0.546713                      0.560554   \n",
       "3                      0.532872                      0.557093   \n",
       "4                      0.522491                      0.553633   \n",
       "5                      0.525952                      0.546713   \n",
       "6                      0.538062                      0.532872   \n",
       "7                      0.584775                      0.522491   \n",
       "8                      0.581315                      0.525952   \n",
       "9                      0.570934                      0.538062   \n",
       "\n",
       "   surface_pressure_lag12_scaled  surface_pressure_lag24_scaled  \\\n",
       "0                       0.576125                       0.572664   \n",
       "1                       0.562284                       0.565744   \n",
       "2                       0.536332                       0.550173   \n",
       "3                       0.534602                       0.539792   \n",
       "4                       0.536332                       0.536332   \n",
       "5                       0.541522                       0.534602   \n",
       "6                       0.555363                       0.544983   \n",
       "7                       0.562284                       0.555363   \n",
       "8                       0.560554                       0.581315   \n",
       "9                       0.557093                       0.591696   \n",
       "\n",
       "   precipitation_lag1_scaled  precipitation_lag2_scaled  \\\n",
       "0                   0.003759                   0.016917   \n",
       "1                   0.000000                   0.003759   \n",
       "2                   0.009398                   0.000000   \n",
       "3                   0.005639                   0.009398   \n",
       "4                   0.007519                   0.005639   \n",
       "5                   0.016917                   0.007519   \n",
       "6                   0.003759                   0.016917   \n",
       "7                   0.000000                   0.003759   \n",
       "8                   0.001880                   0.000000   \n",
       "9                   0.001880                   0.001880   \n",
       "\n",
       "   precipitation_lag3_scaled  precipitation_lag6_scaled  \\\n",
       "0                   0.033835                   0.013158   \n",
       "1                   0.016917                   0.067669   \n",
       "2                   0.003759                   0.016917   \n",
       "3                   0.000000                   0.033835   \n",
       "4                   0.009398                   0.016917   \n",
       "5                   0.005639                   0.003759   \n",
       "6                   0.007519                   0.000000   \n",
       "7                   0.016917                   0.009398   \n",
       "8                   0.003759                   0.005639   \n",
       "9                   0.000000                   0.007519   \n",
       "\n",
       "   precipitation_lag12_scaled  precipitation_lag24_scaled  \n",
       "0                    0.000000                         0.0  \n",
       "1                    0.005639                         0.0  \n",
       "2                    0.001880                         0.0  \n",
       "3                    0.000000                         0.0  \n",
       "4                    0.000000                         0.0  \n",
       "5                    0.000000                         0.0  \n",
       "6                    0.013158                         0.0  \n",
       "7                    0.067669                         0.0  \n",
       "8                    0.016917                         0.0  \n",
       "9                    0.033835                         0.0  \n",
       "\n",
       "[10 rows x 74 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Preprocessing Ho√†n T·∫•t v·ªõi HDFS!\n",
    "\n",
    "### üìä T·ªïng K·∫øt\n",
    "\n",
    "**Ngu·ªìn D·ªØ Li·ªáu:**\n",
    "- ‚úÖ Downloaded t·ª´ HDFS: `hdfs://namenode:9000/data/raw/`\n",
    "- ‚úÖ 28 files (14 pollutant + 14 weather) t·ª´ 14 ƒë·ªãa ƒëi·ªÉm\n",
    "\n",
    "**X·ª≠ L√Ω:**\n",
    "- ‚úÖ Data cleaning (outliers, missing values)\n",
    "- ‚úÖ Feature engineering (time features, lags)\n",
    "- ‚úÖ Scaling (MinMax 0-1)\n",
    "- ‚úÖ Sequence generation (CNN: 24h, LSTM: 48h)\n",
    "\n",
    "**K·∫øt Qu·∫£ L∆∞u Tr√™n HDFS:**\n",
    "- ‚úÖ Uploaded to: `hdfs://namenode:9000/data/processed/`\n",
    "- ‚úÖ 3 model datasets (CNN, LSTM, XGBoost)\n",
    "- ‚úÖ Metadata + Scaler parameters\n",
    "\n",
    "### üîç Verify Tr√™n HDFS\n",
    "\n",
    "#### Option 1: Web UI\n",
    "M·ªü browser: **http://localhost:9870**\n",
    "- Utilities ‚Üí Browse the file system\n",
    "- Navigate: `/data/processed/`\n",
    "- Xem: cnn_sequences/, lstm_sequences/, xgboost/, *.json\n",
    "\n",
    "#### Option 2: Command Line\n",
    "```powershell\n",
    "# List t·∫•t c·∫£ files\n",
    "docker exec hdfs-namenode hdfs dfs -ls -R /data/processed\n",
    "\n",
    "# Xem k√≠ch th∆∞·ªõc\n",
    "docker exec hdfs-namenode hdfs dfs -du -h /data/processed\n",
    "\n",
    "# ƒê·ªçc metadata\n",
    "docker exec hdfs-namenode hdfs dfs -cat /data/processed/datasets_ready.json\n",
    "```\n",
    "\n",
    "### üìÅ Local Staging Files\n",
    "D·ªØ li·ªáu t·∫°m c≈©ng c√≥ t·∫°i local (c√≥ th·ªÉ x√≥a sau khi verify HDFS):\n",
    "```python\n",
    "print(f\"Temp directory: {TEMP_DIR}\")\n",
    "```\n",
    "\n",
    "### üéì Demo Cho M√¥n Big Data\n",
    "\n",
    "**ƒê√£ ho√†n th√†nh:**\n",
    "1. ‚úÖ Setup HDFS cluster (NameNode + 3 DataNodes)\n",
    "2. ‚úÖ Upload d·ªØ li·ªáu l√™n HDFS distributed storage\n",
    "3. ‚úÖ X·ª≠ l√Ω v·ªõi PySpark (distributed processing framework)\n",
    "4. ‚úÖ L∆∞u k·∫øt qu·∫£ v·ªÅ HDFS (persistent storage)\n",
    "5. ‚úÖ Verify replication v√† fault tolerance\n",
    "\n",
    "**C√≥ th·ªÉ present:**\n",
    "- HDFS architecture (blocks, replication)\n",
    "- Data locality trong processing\n",
    "- Scalability v·ªõi nhi·ªÅu DataNodes\n",
    "- Fault tolerance v·ªõi replication factor = 2\n",
    "\n",
    "### üîÑ Next Steps\n",
    "\n",
    "**ƒê·ªÉ train models:**\n",
    "```python\n",
    "# Download processed data t·ª´ HDFS v·ªÅ local\n",
    "docker exec hdfs-namenode hdfs dfs -get /data/processed ./data/\n",
    "\n",
    "# Sau ƒë√≥ d√πng notebooks training nh∆∞ b√¨nh th∆∞·ªùng\n",
    "# 02-cnn-bilstm-attention-model.ipynb\n",
    "# ...\n",
    "```\n",
    "\n",
    "**Ho·∫∑c scale l√™n:**\n",
    "- Th√™m DataNodes trong docker-compose.yml\n",
    "- TƒÉng replication factor\n",
    "- Process tr√™n Spark cluster th·∫≠t thay v√¨ local mode"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8787588,
     "sourceId": 13801744,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1402.87466,
   "end_time": "2025-11-20T10:51:34.096961",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-20T10:28:11.222301",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5caea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# ========================================\n",
    "# DATA QUALITY ASSESSMENT TOOLKIT\n",
    "# ========================================\n",
    "\n",
    "class DataQualityChecker:\n",
    "    \"\"\"\n",
    "    Comprehensive data quality checker for time series datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X_train, y_train, X_val, y_val, X_test, y_test, \n",
    "                 feature_names=None, target_name='PM2_5_log_scaled'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X_train, X_val, X_test: Shape (samples, timesteps, features)\n",
    "            y_train, y_val, y_test: Shape (samples,)\n",
    "            feature_names: List of feature names\n",
    "            target_name: Name of target variable\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.X_val = X_val\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_val = y_val\n",
    "        self.y_test = y_test\n",
    "        self.feature_names = feature_names\n",
    "        self.target_name = target_name\n",
    "        \n",
    "        if feature_names is None:\n",
    "            self.feature_names = [f'Feature_{i}' for i in range(X_train.shape[2])]\n",
    "    \n",
    "    def check_all(self):\n",
    "        \"\"\"Run all quality checks\"\"\"\n",
    "        print(\"=\"*80)\n",
    "        print(\"DATA QUALITY ASSESSMENT REPORT\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        self.check_basic_info()\n",
    "        self.check_missing_values()\n",
    "        self.check_duplicates()\n",
    "        self.check_distribution()\n",
    "        self.check_outliers()\n",
    "        self.check_data_balance()\n",
    "        self.check_feature_ranges()\n",
    "        self.check_temporal_consistency()\n",
    "        self.check_correlations()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"QUALITY ASSESSMENT COMPLETED\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    def check_basic_info(self):\n",
    "        \"\"\"Check basic dataset information\"\"\"\n",
    "        print(\"\\n[1] BASIC INFORMATION\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"Training samples:   {len(self.X_train):,}\")\n",
    "        print(f\"Validation samples: {len(self.X_val):,}\")\n",
    "        print(f\"Test samples:       {len(self.X_test):,}\")\n",
    "        print(f\"Total samples:      {len(self.X_train) + len(self.X_val) + len(self.X_test):,}\")\n",
    "        print(f\"\\nSequence length:    {self.X_train.shape[1]}\")\n",
    "        print(f\"Number of features: {self.X_train.shape[2]}\")\n",
    "        print(f\"Features: {', '.join(self.feature_names)}\")\n",
    "        \n",
    "        # Split ratio\n",
    "        total = len(self.X_train) + len(self.X_val) + len(self.X_test)\n",
    "        print(f\"\\nSplit ratios:\")\n",
    "        print(f\"  Train: {len(self.X_train)/total*100:.1f}%\")\n",
    "        print(f\"  Val:   {len(self.X_val)/total*100:.1f}%\")\n",
    "        print(f\"  Test:  {len(self.X_test)/total*100:.1f}%\")\n",
    "    \n",
    "    def check_missing_values(self):\n",
    "        \"\"\"Check for missing values\"\"\"\n",
    "        print(\"\\n[2] MISSING VALUES CHECK\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        datasets = {\n",
    "            'Train': (self.X_train, self.y_train),\n",
    "            'Val': (self.X_val, self.y_val),\n",
    "            'Test': (self.X_test, self.y_test)\n",
    "        }\n",
    "        \n",
    "        has_missing = False\n",
    "        for name, (X, y) in datasets.items():\n",
    "            nan_count_X = np.isnan(X).sum()\n",
    "            inf_count_X = np.isinf(X).sum()\n",
    "            nan_count_y = np.isnan(y).sum()\n",
    "            inf_count_y = np.isinf(y).sum()\n",
    "            \n",
    "            if nan_count_X > 0 or inf_count_X > 0 or nan_count_y > 0 or inf_count_y > 0:\n",
    "                has_missing = True\n",
    "                print(f\"‚ö†Ô∏è  {name} set:\")\n",
    "                if nan_count_X > 0:\n",
    "                    print(f\"   X: {nan_count_X:,} NaN values ({nan_count_X/X.size*100:.4f}%)\")\n",
    "                if inf_count_X > 0:\n",
    "                    print(f\"   X: {inf_count_X:,} Inf values ({inf_count_X/X.size*100:.4f}%)\")\n",
    "                if nan_count_y > 0:\n",
    "                    print(f\"   y: {nan_count_y:,} NaN values ({nan_count_y/y.size*100:.4f}%)\")\n",
    "                if inf_count_y > 0:\n",
    "                    print(f\"   y: {inf_count_y:,} Inf values ({inf_count_y/y.size*100:.4f}%)\")\n",
    "        \n",
    "        if not has_missing:\n",
    "            print(\"‚úÖ No missing values (NaN/Inf) detected in any dataset\")\n",
    "    \n",
    "    def check_duplicates(self):\n",
    "        \"\"\"Check for duplicate sequences\"\"\"\n",
    "        print(\"\\n[3] DUPLICATE SEQUENCES CHECK\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Reshape to 2D for duplicate detection\n",
    "        X_train_2d = self.X_train.reshape(len(self.X_train), -1)\n",
    "        X_val_2d = self.X_val.reshape(len(self.X_val), -1)\n",
    "        X_test_2d = self.X_test.reshape(len(self.X_test), -1)\n",
    "        \n",
    "        # Check duplicates within each set\n",
    "        train_df = pd.DataFrame(X_train_2d)\n",
    "        val_df = pd.DataFrame(X_val_2d)\n",
    "        test_df = pd.DataFrame(X_test_2d)\n",
    "        \n",
    "        train_dups = train_df.duplicated().sum()\n",
    "        val_dups = val_df.duplicated().sum()\n",
    "        test_dups = test_df.duplicated().sum()\n",
    "        \n",
    "        print(f\"Train duplicates: {train_dups:,} ({train_dups/len(self.X_train)*100:.2f}%)\")\n",
    "        print(f\"Val duplicates:   {val_dups:,} ({val_dups/len(self.X_val)*100:.2f}%)\")\n",
    "        print(f\"Test duplicates:  {test_dups:,} ({test_dups/len(self.X_test)*100:.2f}%)\")\n",
    "        \n",
    "        if train_dups + val_dups + test_dups == 0:\n",
    "            print(\"‚úÖ No duplicate sequences found\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Duplicates detected - consider removing them\")\n",
    "    \n",
    "    def check_distribution(self):\n",
    "        \"\"\"Check target distribution\"\"\"\n",
    "        print(\"\\n[4] TARGET DISTRIBUTION\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        datasets = {\n",
    "            'Train': self.y_train,\n",
    "            'Val': self.y_val,\n",
    "            'Test': self.y_test\n",
    "        }\n",
    "        \n",
    "        for name, y in datasets.items():\n",
    "            print(f\"\\n{name} set ({self.target_name}):\")\n",
    "            print(f\"  Mean:   {np.mean(y):.6f}\")\n",
    "            print(f\"  Std:    {np.std(y):.6f}\")\n",
    "            print(f\"  Min:    {np.min(y):.6f}\")\n",
    "            print(f\"  25%:    {np.percentile(y, 25):.6f}\")\n",
    "            print(f\"  Median: {np.median(y):.6f}\")\n",
    "            print(f\"  75%:    {np.percentile(y, 75):.6f}\")\n",
    "            print(f\"  Max:    {np.max(y):.6f}\")\n",
    "            \n",
    "            # Skewness and Kurtosis\n",
    "            skewness = stats.skew(y)\n",
    "            kurtosis = stats.kurtosis(y)\n",
    "            print(f\"  Skewness: {skewness:.4f} {'(right-skewed)' if skewness > 0 else '(left-skewed)'}\")\n",
    "            print(f\"  Kurtosis: {kurtosis:.4f} {'(heavy-tailed)' if kurtosis > 0 else '(light-tailed)'}\")\n",
    "        \n",
    "        # Check distribution consistency\n",
    "        print(\"\\nüìä Distribution Consistency Check:\")\n",
    "        train_mean, val_mean, test_mean = np.mean(self.y_train), np.mean(self.y_val), np.mean(self.y_test)\n",
    "        train_std, val_std, test_std = np.std(self.y_train), np.std(self.y_val), np.std(self.y_test)\n",
    "        \n",
    "        mean_diff_val = abs(train_mean - val_mean) / train_mean * 100\n",
    "        mean_diff_test = abs(train_mean - test_mean) / train_mean * 100\n",
    "        std_diff_val = abs(train_std - val_std) / train_std * 100\n",
    "        std_diff_test = abs(train_std - test_std) / train_std * 100\n",
    "        \n",
    "        print(f\"  Train vs Val mean difference:  {mean_diff_val:.2f}%\")\n",
    "        print(f\"  Train vs Test mean difference: {mean_diff_test:.2f}%\")\n",
    "        print(f\"  Train vs Val std difference:   {std_diff_val:.2f}%\")\n",
    "        print(f\"  Train vs Test std difference:  {std_diff_test:.2f}%\")\n",
    "        \n",
    "        if mean_diff_val > 10 or mean_diff_test > 10:\n",
    "            print(\"  ‚ö†Ô∏è  Large mean difference detected - possible distribution shift\")\n",
    "        else:\n",
    "            print(\"  ‚úÖ Distribution is consistent across splits\")\n",
    "    \n",
    "    def check_outliers(self):\n",
    "        \"\"\"Check for outliers in target variable\"\"\"\n",
    "        print(\"\\n[5] OUTLIER DETECTION (Target Variable)\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        datasets = {\n",
    "            'Train': self.y_train,\n",
    "            'Val': self.y_val,\n",
    "            'Test': self.y_test\n",
    "        }\n",
    "        \n",
    "        for name, y in datasets.items():\n",
    "            Q1 = np.percentile(y, 25)\n",
    "            Q3 = np.percentile(y, 75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            outliers_low = np.sum(y < lower_bound)\n",
    "            outliers_high = np.sum(y > upper_bound)\n",
    "            total_outliers = outliers_low + outliers_high\n",
    "            \n",
    "            print(f\"\\n{name} set:\")\n",
    "            print(f\"  IQR: {IQR:.6f}\")\n",
    "            print(f\"  Lower bound: {lower_bound:.6f}\")\n",
    "            print(f\"  Upper bound: {upper_bound:.6f}\")\n",
    "            print(f\"  Outliers (low):  {outliers_low:,} ({outliers_low/len(y)*100:.2f}%)\")\n",
    "            print(f\"  Outliers (high): {outliers_high:,} ({outliers_high/len(y)*100:.2f}%)\")\n",
    "            print(f\"  Total outliers:  {total_outliers:,} ({total_outliers/len(y)*100:.2f}%)\")\n",
    "            \n",
    "            if total_outliers/len(y) > 0.05:\n",
    "                print(f\"  ‚ö†Ô∏è  High percentage of outliers (>{5}%)\")\n",
    "    \n",
    "    def check_data_balance(self):\n",
    "        \"\"\"Check data balance across value ranges\"\"\"\n",
    "        print(\"\\n[6] DATA BALANCE (Target Variable Distribution)\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Create bins for target variable\n",
    "        bins = np.percentile(self.y_train, [0, 25, 50, 75, 100])\n",
    "        bin_labels = ['Q1 (Low)', 'Q2', 'Q3', 'Q4 (High)']\n",
    "        \n",
    "        datasets = {\n",
    "            'Train': self.y_train,\n",
    "            'Val': self.y_val,\n",
    "            'Test': self.y_test\n",
    "        }\n",
    "        \n",
    "        for name, y in datasets.items():\n",
    "            digitized = np.digitize(y, bins[1:-1])\n",
    "            counts = [np.sum(digitized == i) for i in range(4)]\n",
    "            \n",
    "            print(f\"\\n{name} set distribution:\")\n",
    "            for i, (label, count) in enumerate(zip(bin_labels, counts)):\n",
    "                percentage = count / len(y) * 100\n",
    "                bar = '‚ñà' * int(percentage / 2)\n",
    "                print(f\"  {label}: {count:6,} ({percentage:5.2f}%) {bar}\")\n",
    "            \n",
    "            # Check balance\n",
    "            min_count = min(counts)\n",
    "            max_count = max(counts)\n",
    "            imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')\n",
    "            \n",
    "            if imbalance_ratio > 2:\n",
    "                print(f\"  ‚ö†Ô∏è  Imbalanced data detected (ratio: {imbalance_ratio:.2f}:1)\")\n",
    "            else:\n",
    "                print(f\"  ‚úÖ Data is reasonably balanced (ratio: {imbalance_ratio:.2f}:1)\")\n",
    "    \n",
    "    def check_feature_ranges(self):\n",
    "        \"\"\"Check feature value ranges\"\"\"\n",
    "        print(\"\\n[7] FEATURE VALUE RANGES\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Get last timestep for each sequence (most recent)\n",
    "        X_train_last = self.X_train[:, -1, :]\n",
    "        \n",
    "        print(f\"\\n{'Feature':<30} {'Min':>10} {'Max':>10} {'Mean':>10} {'Std':>10}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for i, name in enumerate(self.feature_names):\n",
    "            feature_values = X_train_last[:, i]\n",
    "            print(f\"{name:<30} {np.min(feature_values):>10.4f} {np.max(feature_values):>10.4f} \"\n",
    "                  f\"{np.mean(feature_values):>10.4f} {np.std(feature_values):>10.4f}\")\n",
    "        \n",
    "        # Check if any features are constant\n",
    "        print(\"\\nüîç Constant Features Check:\")\n",
    "        constant_features = []\n",
    "        for i, name in enumerate(self.feature_names):\n",
    "            feature_values = X_train_last[:, i]\n",
    "            if np.std(feature_values) < 1e-6:\n",
    "                constant_features.append(name)\n",
    "        \n",
    "        if constant_features:\n",
    "            print(f\"  ‚ö†Ô∏è  Constant features detected: {', '.join(constant_features)}\")\n",
    "        else:\n",
    "            print(\"  ‚úÖ No constant features detected\")\n",
    "    \n",
    "    def check_temporal_consistency(self):\n",
    "        \"\"\"Check temporal consistency in sequences\"\"\"\n",
    "        print(\"\\n[8] TEMPORAL CONSISTENCY\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Check if sequences have temporal trends\n",
    "        # Calculate average difference between consecutive timesteps\n",
    "        diffs = np.diff(self.X_train, axis=1)\n",
    "        avg_diff = np.mean(np.abs(diffs), axis=(0, 1))\n",
    "        \n",
    "        print(f\"\\nAverage temporal change per feature:\")\n",
    "        print(f\"{'Feature':<30} {'Avg Change':>15}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for i, name in enumerate(self.feature_names):\n",
    "            print(f\"{name:<30} {avg_diff[i]:>15.6f}\")\n",
    "        \n",
    "        # Check for sequences with no temporal variation\n",
    "        zero_variance_count = 0\n",
    "        for seq in self.X_train:\n",
    "            if np.all(np.std(seq, axis=0) < 1e-6):\n",
    "                zero_variance_count += 1\n",
    "        \n",
    "        print(f\"\\nüîç Zero-variance sequences: {zero_variance_count:,} ({zero_variance_count/len(self.X_train)*100:.2f}%)\")\n",
    "        \n",
    "        if zero_variance_count > 0:\n",
    "            print(\"  ‚ö†Ô∏è  Some sequences have no temporal variation\")\n",
    "        else:\n",
    "            print(\"  ‚úÖ All sequences have temporal variation\")\n",
    "    \n",
    "    def check_correlations(self):\n",
    "        \"\"\"Check feature correlations\"\"\"\n",
    "        print(\"\\n[9] FEATURE CORRELATIONS\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Use last timestep for correlation analysis\n",
    "        X_train_last = self.X_train[:, -1, :]\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        corr_matrix = np.corrcoef(X_train_last.T)\n",
    "        \n",
    "        # Find highly correlated features (>0.9)\n",
    "        high_corr_pairs = []\n",
    "        for i in range(len(self.feature_names)):\n",
    "            for j in range(i+1, len(self.feature_names)):\n",
    "                if abs(corr_matrix[i, j]) > 0.9:\n",
    "                    high_corr_pairs.append((\n",
    "                        self.feature_names[i], \n",
    "                        self.feature_names[j], \n",
    "                        corr_matrix[i, j]\n",
    "                    ))\n",
    "        \n",
    "        if high_corr_pairs:\n",
    "            print(\"\\n‚ö†Ô∏è  Highly correlated feature pairs (|r| > 0.9):\")\n",
    "            for feat1, feat2, corr in high_corr_pairs:\n",
    "                print(f\"  {feat1} <-> {feat2}: {corr:.4f}\")\n",
    "            print(\"\\n  Consider removing one feature from each pair to reduce redundancy\")\n",
    "        else:\n",
    "            print(\"\\n‚úÖ No highly correlated feature pairs detected (threshold: |r| > 0.9)\")\n",
    "        \n",
    "        # Correlation with target\n",
    "        print(\"\\nüìä Feature correlation with target:\")\n",
    "        y_train_expanded = np.repeat(self.y_train.reshape(-1, 1), X_train_last.shape[1], axis=1)\n",
    "        target_corr = [np.corrcoef(X_train_last[:, i], self.y_train)[0, 1] \n",
    "                       for i in range(X_train_last.shape[1])]\n",
    "        \n",
    "        # Sort by absolute correlation\n",
    "        sorted_indices = np.argsort(np.abs(target_corr))[::-1]\n",
    "        \n",
    "        print(f\"\\n{'Feature':<30} {'Correlation':>15}\")\n",
    "        print(\"-\" * 50)\n",
    "        for idx in sorted_indices[:10]:  # Top 10\n",
    "            print(f\"{self.feature_names[idx]:<30} {target_corr[idx]:>15.4f}\")\n",
    "    \n",
    "    def plot_quality_report(self):\n",
    "        \"\"\"Generate visualization report\"\"\"\n",
    "        print(\"\\n[10] GENERATING VISUALIZATIONS...\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        fig = plt.figure(figsize=(16, 12))\n",
    "        \n",
    "        # 1. Target distribution\n",
    "        ax1 = plt.subplot(3, 3, 1)\n",
    "        ax1.hist([self.y_train, self.y_val, self.y_test], \n",
    "                 bins=50, label=['Train', 'Val', 'Test'], alpha=0.7)\n",
    "        ax1.set_xlabel(self.target_name)\n",
    "        ax1.set_ylabel('Frequency')\n",
    "        ax1.set_title('Target Distribution')\n",
    "        ax1.legend()\n",
    "        ax1.grid(alpha=0.3)\n",
    "        \n",
    "        # 2. Box plots\n",
    "        ax2 = plt.subplot(3, 3, 2)\n",
    "        data_to_plot = [self.y_train, self.y_val, self.y_test]\n",
    "        ax2.boxplot(data_to_plot, labels=['Train', 'Val', 'Test'])\n",
    "        ax2.set_ylabel(self.target_name)\n",
    "        ax2.set_title('Target Distribution (Box Plot)')\n",
    "        ax2.grid(alpha=0.3)\n",
    "        \n",
    "        # 3. Q-Q plot for normality\n",
    "        ax3 = plt.subplot(3, 3, 3)\n",
    "        stats.probplot(self.y_train, dist=\"norm\", plot=ax3)\n",
    "        ax3.set_title('Q-Q Plot (Train Set)')\n",
    "        ax3.grid(alpha=0.3)\n",
    "        \n",
    "        # 4. Feature distributions (first 6 features)\n",
    "        for i in range(min(6, len(self.feature_names))):\n",
    "            ax = plt.subplot(3, 3, i+4)\n",
    "            feature_data = self.X_train[:, -1, i]  # Last timestep\n",
    "            ax.hist(feature_data, bins=50, alpha=0.7, edgecolor='black')\n",
    "            ax.set_xlabel(self.feature_names[i])\n",
    "            ax.set_ylabel('Frequency')\n",
    "            ax.set_title(f'{self.feature_names[i]} Distribution')\n",
    "            ax.grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('data_quality_report.png', dpi=150, bbox_inches='tight')\n",
    "        print(\"‚úÖ Visualization saved as 'data_quality_report.png'\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# USAGE EXAMPLE\n",
    "# ========================================\n",
    "\n",
    "# Initialize checker\n",
    "checker = DataQualityChecker(\n",
    "    X_train=X_train,\n",
    "    X_val=X_val,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_val=y_val,\n",
    "    y_test=y_test,\n",
    "    feature_names=[\n",
    "        \"PM10_scaled\", \"NO2_scaled\", \"SO2_scaled\",\n",
    "        \"temperature_2m_scaled\", \"relative_humidity_2m_scaled\",\n",
    "        \"wind_speed_10m_scaled\", \"surface_pressure_scaled\",\n",
    "        \"precipitation_scaled\", \"hour_sin\", \"hour_cos\",\n",
    "        \"month_sin\", \"month_cos\", \"day_of_week_sin\",\n",
    "        \"day_of_week_cos\", \"wind_direction_sin\",\n",
    "        \"wind_direction_cos\", \"is_weekend\"\n",
    "    ],\n",
    "    target_name='PM2_5_log_scaled'\n",
    ")\n",
    "\n",
    "# Run all quality checks\n",
    "checker.check_all()\n",
    "\n",
    "# Generate visualization report\n",
    "checker.plot_quality_report()\n",
    "\n",
    "# ========================================\n",
    "# ADDITIONAL CHECKS\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ADDITIONAL SANITY CHECKS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check data types\n",
    "print(\"\\n[11] DATA TYPE CHECK\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"X_train dtype: {X_train.dtype}\")\n",
    "print(f\"y_train dtype: {y_train.dtype}\")\n",
    "if X_train.dtype != np.float32 and X_train.dtype != np.float64:\n",
    "    print(\"‚ö†Ô∏è  X_train is not float type - may cause issues\")\n",
    "if y_train.dtype != np.float32 and y_train.dtype != np.float64:\n",
    "    print(\"‚ö†Ô∏è  y_train is not float type - may cause issues\")\n",
    "\n",
    "# Check for data leakage\n",
    "print(\"\\n[12] DATA LEAKAGE CHECK\")\n",
    "print(\"-\" * 80)\n",
    "X_train_2d = X_train.reshape(len(X_train), -1)\n",
    "X_test_2d = X_test.reshape(len(X_test), -1)\n",
    "\n",
    "# Check if any test samples appear in training\n",
    "train_df = pd.DataFrame(X_train_2d)\n",
    "test_df = pd.DataFrame(X_test_2d)\n",
    "common = pd.merge(train_df, test_df, how='inner')\n",
    "\n",
    "if len(common) > 0:\n",
    "    print(f\"‚ö†Ô∏è  POSSIBLE DATA LEAKAGE: {len(common)} test samples found in training set!\")\n",
    "else:\n",
    "    print(\"‚úÖ No data leakage detected\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA QUALITY ASSESSMENT COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ecbf08",
   "metadata": {
    "papermill": {
     "duration": 0.009172,
     "end_time": "2025-11-09T05:38:15.730376",
     "exception": false,
     "start_time": "2025-11-09T05:38:15.721204",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PM2.5 Prediction - Data Preprocessing\n",
    "\n",
    "Notebook này thực hiện tiền xử lý dữ liệu với PySpark:\n",
    "1. Kết nối Spark cluster\n",
    "2. Đọc và khám phá dữ liệu\n",
    "3. Tổng quan về dataset\n",
    "4. **Làm sạch dữ liệu** (Outlier Removal -> Missing Value Imputation)\n",
    "5. Feature engineering\n",
    "6. Data summary & statistics\n",
    "7. Lưu dữ liệu đã xử lý"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa45156c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T05:38:15.747607Z",
     "iopub.status.busy": "2025-11-09T05:38:15.746540Z",
     "iopub.status.idle": "2025-11-09T05:38:25.770491Z",
     "shell.execute_reply": "2025-11-09T05:38:25.769210Z"
    },
    "papermill": {
     "duration": 10.034171,
     "end_time": "2025-11-09T05:38:25.772317",
     "exception": false,
     "start_time": "2025-11-09T05:38:15.738146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KAGGLE] Running on Kaggle\n",
      "[INSTALL] Installing PySpark...\n",
      "[OK] PySpark installed\n",
      "[OK] Java: /usr/lib/jvm/java-11-openjdk-amd64\n",
      "[OK] All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Detect environment (Kaggle vs Colab vs Local)\n",
    "IN_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    print(\"[KAGGLE] Running on Kaggle\")\n",
    "    \n",
    "    # Kaggle has Java pre-installed, just set JAVA_HOME\n",
    "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "    os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ.get(\"PATH\", \"\")\n",
    "    \n",
    "    # Install PySpark\n",
    "    print(\"[INSTALL] Installing PySpark...\")\n",
    "    !pip install -q pyspark\n",
    "    print(\"[OK] PySpark installed\")\n",
    "    print(f\"[OK] Java: {os.environ['JAVA_HOME']}\")\n",
    "    \n",
    "elif IN_COLAB:\n",
    "    print(\"[COLAB] Running on Google Colab\")\n",
    "    \n",
    "    # Install Java 11 (required for PySpark)\n",
    "    print(\"[INSTALL] Installing Java 11...\")\n",
    "    !apt-get install -y openjdk-11-jdk-headless -qq > /dev/null\n",
    "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "    os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ.get(\"PATH\", \"\")\n",
    "    print(f\"[OK] Java installed: {os.environ['JAVA_HOME']}\")\n",
    "    \n",
    "    # Install PySpark\n",
    "    print(\"[INSTALL] Installing PySpark...\")\n",
    "    !pip install -q pyspark\n",
    "    print(\"[OK] PySpark installed\")\n",
    "    \n",
    "    # Mount Google Drive (optional - if data is in Drive)\n",
    "    # from google.colab import drive\n",
    "    # drive.mount('/content/drive')\n",
    "    \n",
    "else:\n",
    "    print(\"[LOCAL] Running on Local Machine\")\n",
    "    \n",
    "    # Set Java 21 for PySpark (local only)\n",
    "    os.environ['JAVA_HOME'] = r'C:\\Program Files\\Java\\jdk-21'\n",
    "    os.environ['PATH'] = os.environ['JAVA_HOME'] + r'\\bin;' + os.environ.get('PATH', '')\n",
    "    print(f\"[OK] Using Java: {os.environ['JAVA_HOME']}\")\n",
    "\n",
    "# Common imports\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"[OK] All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f19612b",
   "metadata": {
    "papermill": {
     "duration": 0.006615,
     "end_time": "2025-11-09T05:38:25.785844",
     "exception": false,
     "start_time": "2025-11-09T05:38:25.779229",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Kết nối Spark Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b219f8da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T05:38:25.801142Z",
     "iopub.status.busy": "2025-11-09T05:38:25.800559Z",
     "iopub.status.idle": "2025-11-09T05:38:35.340106Z",
     "shell.execute_reply": "2025-11-09T05:38:35.338432Z"
    },
    "papermill": {
     "duration": 9.549642,
     "end_time": "2025-11-09T05:38:35.341988",
     "exception": false,
     "start_time": "2025-11-09T05:38:25.792346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/09 05:38:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KAGGLE] Spark running on Kaggle (4 cores, 12GB total)\n",
      "[OK] Spark version: 3.5.1\n",
      "[OK] Spark mode: local[4]\n",
      "[OK] Application ID: local-1762666712539\n",
      "[OK] Cores: 8\n",
      "[OK] Parallelism: 8\n"
     ]
    }
   ],
   "source": [
    "# Tạo Spark Session với cấu hình tùy theo môi trường\n",
    "if IN_KAGGLE:\n",
    "    # Kaggle configuration - Balanced (4 cores, 16GB RAM)\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PM25-Preprocessing\") \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"6g\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "        .config(\"spark.default.parallelism\", \"8\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(\"[KAGGLE] Spark running on Kaggle (4 cores, 12GB total)\")\n",
    "    \n",
    "elif IN_COLAB:\n",
    "    # Colab configuration - Lighter settings (2 cores, 12GB RAM)\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PM25-Preprocessing\") \\\n",
    "        .master(\"local[2]\") \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .config(\"spark.executor.memory\", \"2g\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(\"[COLAB] Spark running on Colab (2 cores, 4GB total)\")\n",
    "    \n",
    "else:\n",
    "    # Local configuration - OPTIMIZED for 8-core AMD Ryzen + 15.7GB RAM\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PM25-Preprocessing\") \\\n",
    "        .master(\"local[8]\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"16\") \\\n",
    "        .config(\"spark.default.parallelism\", \"16\") \\\n",
    "        .config(\"spark.python.worker.timeout\", \"600\") \\\n",
    "        .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "        .config(\"spark.network.timeout\", \"600s\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(\"[LOCAL] Spark running on Local (8 cores, 12GB total - OPTIMIZED)\")\n",
    "\n",
    "print(f\"[OK] Spark version: {spark.version}\")\n",
    "print(f\"[OK] Spark mode: {spark.sparkContext.master}\")\n",
    "print(f\"[OK] Application ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"[OK] Cores: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"[OK] Parallelism: {spark.conf.get('spark.default.parallelism', 'default')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754e6ac9",
   "metadata": {
    "papermill": {
     "duration": 0.007376,
     "end_time": "2025-11-09T05:38:35.356486",
     "exception": false,
     "start_time": "2025-11-09T05:38:35.349110",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Định nghĩa Schema và Scan Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bfb7fe9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T05:38:35.373658Z",
     "iopub.status.busy": "2025-11-09T05:38:35.372827Z",
     "iopub.status.idle": "2025-11-09T05:38:35.381615Z",
     "shell.execute_reply": "2025-11-09T05:38:35.380463Z"
    },
    "papermill": {
     "duration": 0.019386,
     "end_time": "2025-11-09T05:38:35.383506",
     "exception": false,
     "start_time": "2025-11-09T05:38:35.364120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Schemas defined\n"
     ]
    }
   ],
   "source": [
    "# Schema cho dữ liệu OpenAQ\n",
    "openaq_schema = StructType([\n",
    "    StructField(\"location_id\", StringType(), True),\n",
    "    StructField(\"sensors_id\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"datetime\", TimestampType(), True),\n",
    "    StructField(\"lat\", DoubleType(), True),\n",
    "    StructField(\"lon\", DoubleType(), True),\n",
    "    StructField(\"parameter\", StringType(), True),\n",
    "    StructField(\"units\", StringType(), True),\n",
    "    StructField(\"value\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Schema cho dữ liệu Weather\n",
    "weather_schema = StructType([\n",
    "    StructField(\"time\", TimestampType(), True),\n",
    "    StructField(\"temperature_2m\", DoubleType(), True),\n",
    "    StructField(\"relative_humidity_2m\", DoubleType(), True),\n",
    "    StructField(\"wind_speed_10m\", DoubleType(), True),\n",
    "    StructField(\"wind_direction_10m\", DoubleType(), True),\n",
    "    StructField(\"surface_pressure\", DoubleType(), True),\n",
    "    StructField(\"precipitation\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "print(\"[OK] Schemas defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83616e4",
   "metadata": {
    "papermill": {
     "duration": 0.008599,
     "end_time": "2025-11-09T05:38:35.404180",
     "exception": false,
     "start_time": "2025-11-09T05:38:35.395581",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.1 Scan và Map Files theo Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f0c86b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T05:38:35.420451Z",
     "iopub.status.busy": "2025-11-09T05:38:35.420032Z",
     "iopub.status.idle": "2025-11-09T05:38:35.453047Z",
     "shell.execute_reply": "2025-11-09T05:38:35.451620Z"
    },
    "papermill": {
     "duration": 0.043568,
     "end_time": "2025-11-09T05:38:35.454766",
     "exception": false,
     "start_time": "2025-11-09T05:38:35.411198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KAGGLE] Using Kaggle dataset: /kaggle/input/hongkong-pollutant-dataset\n",
      "[FILES] Found 14 pollutant files:\n",
      "  [OK] Location 233335: pollutant_location_233335.csv + weather_location_233335.csv\n",
      "  [OK] Location 7728: pollutant_location_7728.csv + weather_location_7728.csv\n",
      "  [OK] Location 7735: pollutant_location_7735.csv + weather_location_7735.csv\n",
      "  [OK] Location 7742: pollutant_location_7742.csv + weather_location_7742.csv\n",
      "  [OK] Location 7734: pollutant_location_7734.csv + weather_location_7734.csv\n",
      "  [OK] Location 7740: pollutant_location_7740.csv + weather_location_7740.csv\n",
      "  [OK] Location 7736: pollutant_location_7736.csv + weather_location_7736.csv\n",
      "  [OK] Location 7739: pollutant_location_7739.csv + weather_location_7739.csv\n",
      "  [OK] Location 7733: pollutant_location_7733.csv + weather_location_7733.csv\n",
      "  [OK] Location 7732: pollutant_location_7732.csv + weather_location_7732.csv\n",
      "  [OK] Location 7730: pollutant_location_7730.csv + weather_location_7730.csv\n",
      "  [OK] Location 7737: pollutant_location_7737.csv + weather_location_7737.csv\n",
      "  [OK] Location 7727: pollutant_location_7727.csv + weather_location_7727.csv\n",
      "  [OK] Location 233336: pollutant_location_233336.csv + weather_location_233336.csv\n",
      "\n",
      "[SUCCESS] Total locations to process: 14\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# ========================================\n",
    "# KAGGLE: Sử dụng đường dẫn Kaggle dataset\n",
    "# ========================================\n",
    "if IN_KAGGLE:\n",
    "    # [KAGGLE] Kaggle paths\n",
    "    # Format: /kaggle/input/{dataset-name}/\n",
    "    raw_data_path = Path(\"/kaggle/input/hongkong-pollutant-dataset\")  # ← Thay tên dataset của bạn\n",
    "    print(f\"[KAGGLE] Using Kaggle dataset: {raw_data_path}\")\n",
    "    \n",
    "elif IN_COLAB:\n",
    "    # [COLAB] Colab: Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    raw_data_path = Path(\"/content/drive/MyDrive/pm25-data/raw\")  # ← Thay đường dẫn Drive của bạn\n",
    "    print(f\"[COLAB] Using Google Drive: {raw_data_path}\")\n",
    "    \n",
    "else:\n",
    "    # [LOCAL] Local path (giữ nguyên)\n",
    "    raw_data_path = Path(\"../data/raw\")\n",
    "    print(f\"[LOCAL] Using local path: {raw_data_path}\")\n",
    "\n",
    "# Tìm tất cả các file pollutant\n",
    "pollutant_files = list(raw_data_path.glob(\"pollutant_location_*.csv\"))\n",
    "\n",
    "print(f\"[FILES] Found {len(pollutant_files)} pollutant files:\")\n",
    "\n",
    "# Tạo mapping giữa pollutant và weather files\n",
    "location_mapping = {}\n",
    "\n",
    "for pollutant_file in pollutant_files:\n",
    "    # Extract location_id từ tên file: pollutant_location_7727.csv -> 7727\n",
    "    match = re.search(r'pollutant_location_(\\d+)\\.csv', pollutant_file.name)\n",
    "    \n",
    "    if match:\n",
    "        location_id = match.group(1)\n",
    "        weather_file = raw_data_path / f\"weather_location_{location_id}.csv\"\n",
    "        \n",
    "        # Kiểm tra file weather tương ứng có tồn tại không\n",
    "        if weather_file.exists():\n",
    "            location_mapping[location_id] = {\n",
    "                'pollutant': str(pollutant_file),\n",
    "                'weather': str(weather_file)\n",
    "            }\n",
    "            print(f\"  [OK] Location {location_id}: {pollutant_file.name} + {weather_file.name}\")\n",
    "        else:\n",
    "            print(f\"  [WARNING]  Location {location_id}: Missing weather file!\")\n",
    "\n",
    "print(f\"\\n[SUCCESS] Total locations to process: {len(location_mapping)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6ef314",
   "metadata": {
    "papermill": {
     "duration": 0.007053,
     "end_time": "2025-11-09T05:38:35.469178",
     "exception": false,
     "start_time": "2025-11-09T05:38:35.462125",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.2 Xử lý từng Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3d7bb59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T05:38:35.486643Z",
     "iopub.status.busy": "2025-11-09T05:38:35.485814Z",
     "iopub.status.idle": "2025-11-09T05:39:16.201879Z",
     "shell.execute_reply": "2025-11-09T05:39:16.200753Z"
    },
    "papermill": {
     "duration": 40.72645,
     "end_time": "2025-11-09T05:39:16.203636",
     "exception": false,
     "start_time": "2025-11-09T05:38:35.477186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PROCESSING] Processing Location 233335...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [DATA] Air quality (PM2.5, PM10, SO2, NO2): 83,970 records\n",
      "  [?]  Weather: 25,560 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] After join: 21,679 records\n",
      "\n",
      "[PROCESSING] Processing Location 7728...\n",
      "  [DATA] Air quality (PM2.5, PM10, SO2, NO2): 84,638 records\n",
      "  [?]  Weather: 25,560 records\n",
      "  [OK] After join: 21,676 records\n",
      "\n",
      "[PROCESSING] Processing Location 7735...\n",
      "  [DATA] Air quality (PM2.5, PM10, SO2, NO2): 83,373 records\n",
      "  [?]  Weather: 25,560 records\n",
      "  [OK] After join: 21,270 records\n",
      "\n",
      "[PROCESSING] Processing Location 7742...\n",
      "  [DATA] Air quality (PM2.5, PM10, SO2, NO2): 82,234 records\n",
      "  [?]  Weather: 25,560 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] After join: 21,087 records\n",
      "\n",
      "[PROCESSING] Processing Location 7734...\n",
      "  [DATA] Air quality (PM2.5, PM10, SO2, NO2): 82,570 records\n",
      "  [?]  Weather: 25,560 records\n",
      "  [OK] After join: 21,247 records\n",
      "\n",
      "[PROCESSING] Processing Location 7740...\n",
      "  [DATA] Air quality (PM2.5, PM10, SO2, NO2): 84,504 records\n",
      "  [?]  Weather: 25,560 records\n",
      "  [OK] After join: 21,687 records\n",
      "\n",
      "[PROCESSING] Processing Location 7736...\n",
      "  [DATA] Air quality (PM2.5, PM10, SO2, NO2): 83,295 records\n",
      "  [?]  Weather: 25,560 records\n",
      "  [OK] After join: 21,274 records\n",
      "\n",
      "[PROCESSING] Processing Location 7739...\n",
      "  [DATA] Air quality (PM2.5, PM10, SO2, NO2): 84,686 records\n",
      "  [?]  Weather: 25,560 records\n",
      "  [OK] After join: 21,721 records\n",
      "\n",
      "[PROCESSING] Processing Location 7733...\n",
      "  [DATA] Air quality (PM2.5, PM10, SO2, NO2): 83,047 records\n",
      "  [?]  Weather: 25,560 records\n",
      "  [OK] After join: 21,228 records\n",
      "\n",
      "[PROCESSING] Processing Location 7732...\n",
      "  [DATA] Air quality (PM2.5, PM10, SO2, NO2): 83,386 records\n",
      "  [?]  Weather: 25,560 records\n",
      "  [OK] After join: 21,299 records\n",
      "\n",
      "[PROCESSING] Processing Location 7730...\n",
      "  [DATA] Air quality (PM2.5, PM10, SO2, NO2): 82,776 records\n",
      "  [?]  Weather: 25,560 records\n",
      "  [OK] After join: 21,267 records\n",
      "\n",
      "[PROCESSING] Processing Location 7737...\n",
      "  [DATA] Air quality (PM2.5, PM10, SO2, NO2): 83,079 records\n",
      "  [?]  Weather: 25,560 records\n",
      "  [OK] After join: 21,268 records\n",
      "\n",
      "[PROCESSING] Processing Location 7727...\n",
      "  [DATA] Air quality (PM2.5, PM10, SO2, NO2): 86,684 records\n",
      "  [?]  Weather: 25,560 records\n",
      "  [OK] After join: 22,360 records\n",
      "\n",
      "[PROCESSING] Processing Location 233336...\n",
      "  [DATA] Air quality (PM2.5, PM10, SO2, NO2): 83,818 records\n",
      "  [?]  Weather: 25,560 records\n",
      "  [OK] After join: 21,255 records\n",
      "\n",
      "[SUCCESS] Processed 14 locations successfully!\n"
     ]
    }
   ],
   "source": [
    "# List để chứa dataframes của từng location\n",
    "all_locations_data = []\n",
    "\n",
    "for location_id, files in location_mapping.items():\n",
    "    print(f\"\\n[PROCESSING] Processing Location {location_id}...\")\n",
    "    \n",
    "    # Đọc pollutant data\n",
    "    df_air = spark.read.csv(\n",
    "        files['pollutant'],\n",
    "        header=True,\n",
    "        schema=openaq_schema\n",
    "    )\n",
    "    \n",
    "    # [?] LỌC CHỈ LẤY CÁC CHỈ SỐ QUAN TÂM: PM2.5, PM10, SO2, NO2\n",
    "    df_air = df_air.filter(\n",
    "        F.col(\"parameter\").isin([\"pm25\", \"pm10\", \"so2\", \"no2\"])\n",
    "    )\n",
    "    \n",
    "    # Đọc weather data\n",
    "    df_weather = spark.read.csv(\n",
    "        files['weather'],\n",
    "        header=True,\n",
    "        schema=weather_schema\n",
    "    )\n",
    "    \n",
    "    print(f\"  [DATA] Air quality (PM2.5, PM10, SO2, NO2): {df_air.count():,} records\")\n",
    "    print(f\"  [?]  Weather: {df_weather.count():,} records\")\n",
    "    \n",
    "    # Weather data - drop missing (ít missing)\n",
    "    df_weather_clean = df_weather.na.drop()\n",
    "    \n",
    "    # Pivot pollutant data\n",
    "    df_air_pivot = df_air.groupBy(\n",
    "        \"location_id\", \"location\", \"datetime\", \"lat\", \"lon\"\n",
    "    ).pivot(\"parameter\").agg(F.first(\"value\"))\n",
    "    \n",
    "    # Rename columns\n",
    "    column_mapping = {\n",
    "        \"pm25\": \"PM2_5\",\n",
    "        \"pm10\": \"PM10\",\n",
    "        \"no2\": \"NO2\",\n",
    "        \"so2\": \"SO2\"\n",
    "    }\n",
    "    \n",
    "    for old_name, new_name in column_mapping.items():\n",
    "        if old_name in df_air_pivot.columns:\n",
    "            df_air_pivot = df_air_pivot.withColumnRenamed(old_name, new_name)\n",
    "    \n",
    "    # Join với weather data (theo datetime)\n",
    "    df_location = df_air_pivot.join(\n",
    "        df_weather_clean,\n",
    "        df_air_pivot.datetime == df_weather_clean.time,\n",
    "        \"inner\"\n",
    "    ).drop(\"time\")\n",
    "    \n",
    "    print(f\"  [OK] After join: {df_location.count():,} records\")\n",
    "    \n",
    "    # Thêm vào list\n",
    "    all_locations_data.append(df_location)\n",
    "\n",
    "print(f\"\\n[SUCCESS] Processed {len(all_locations_data)} locations successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce778753",
   "metadata": {
    "papermill": {
     "duration": 0.009178,
     "end_time": "2025-11-09T05:39:16.222366",
     "exception": false,
     "start_time": "2025-11-09T05:39:16.213188",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.3 Gộp tất cả Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac9d3039",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T05:39:16.241828Z",
     "iopub.status.busy": "2025-11-09T05:39:16.241464Z",
     "iopub.status.idle": "2025-11-09T05:39:49.504072Z",
     "shell.execute_reply": "2025-11-09T05:39:49.501282Z"
    },
    "papermill": {
     "duration": 33.274848,
     "end_time": "2025-11-09T05:39:49.506017",
     "exception": false,
     "start_time": "2025-11-09T05:39:16.231169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROCESSING] Combining 14 locations...\n",
      "⏳ Computing combined dataset (this may take a moment)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Combined dataset: 300,318 total records\n",
      "[SUCCESS] Number of locations: 14\n",
      "\n",
      "[METADATA] Sample records (unsorted):\n",
      "+-----------+------------+-------------------+--------+---------+----+----+-----+---+--------------+--------------------+--------------+------------------+----------------+-------------+\n",
      "|location_id|location    |datetime           |lat     |lon      |NO2 |PM10|PM2_5|SO2|temperature_2m|relative_humidity_2m|wind_speed_10m|wind_direction_10m|surface_pressure|precipitation|\n",
      "+-----------+------------+-------------------+--------+---------+----+----+-----+---+--------------+--------------------+--------------+------------------+----------------+-------------+\n",
      "|233335     |North-245631|2022-11-02 06:00:00|22.49671|114.12824|42.5|15.4|11.3 |1.8|18.0          |80.0                |22.8          |18.0              |1005.4          |0.5          |\n",
      "|233335     |North-245631|2022-11-03 15:00:00|22.49671|114.12824|20.4|10.6|6.7  |0.9|22.2          |91.0                |16.2          |102.0             |1010.5          |3.3          |\n",
      "|233335     |North-245631|2022-11-04 16:00:00|22.49671|114.12824|21.3|24.6|17.1 |2.3|23.0          |87.0                |12.2          |56.0              |1013.9          |0.1          |\n",
      "|233335     |North-245631|2022-11-05 11:00:00|22.49671|114.12824|50.2|27.0|14.1 |1.9|21.1          |78.0                |13.1          |32.0              |1019.0          |0.0          |\n",
      "|233335     |North-245631|2022-11-06 13:00:00|22.49671|114.12824|37.3|17.8|14.5 |1.2|20.4          |82.0                |10.8          |37.0              |1017.3          |0.4          |\n",
      "|233335     |North-245631|2022-11-06 15:00:00|22.49671|114.12824|35.8|18.7|14.1 |1.3|19.8          |86.0                |10.2          |42.0              |1015.6          |0.7          |\n",
      "|233335     |North-245631|2022-11-06 03:00:00|22.49671|114.12824|27.4|NULL|NULL |1.6|18.5          |87.0                |11.2          |48.0              |1017.0          |0.0          |\n",
      "|233335     |North-245631|2022-11-06 19:00:00|22.49671|114.12824|23.6|21.2|17.2 |0.9|19.1          |91.0                |11.5          |46.0              |1015.9          |0.0          |\n",
      "|233335     |North-245631|2022-11-08 07:00:00|22.49671|114.12824|37.5|17.3|15.9 |2.6|19.5          |92.0                |9.7           |39.0              |1016.1          |0.2          |\n",
      "|233335     |North-245631|2022-11-08 05:00:00|22.49671|114.12824|35.6|20.8|19.4 |2.3|19.5          |93.0                |8.8           |35.0              |1014.8          |0.0          |\n",
      "+-----------+------------+-------------------+--------+---------+----+----+-----+---+--------------+--------------------+--------------+------------------+----------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gộp tất cả locations lại\n",
    "print(f\"[PROCESSING] Combining {len(all_locations_data)} locations...\")\n",
    "\n",
    "df_combined = all_locations_data[0]\n",
    "for df in all_locations_data[1:]:\n",
    "    df_combined = df_combined.union(df)\n",
    "\n",
    "# OPTIMIZE: Cache để tránh recompute nhiều lần\n",
    "df_combined = df_combined.cache()\n",
    "\n",
    "# OPTIMIZE: Trigger action 1 lần, tránh count() nhiều lần\n",
    "print(\"⏳ Computing combined dataset (this may take a moment)...\")\n",
    "total_records = df_combined.count()\n",
    "num_locations = df_combined.select('location_id').distinct().count()\n",
    "\n",
    "print(f\"[SUCCESS] Combined dataset: {total_records:,} total records\")\n",
    "print(f\"[SUCCESS] Number of locations: {num_locations}\")\n",
    "\n",
    "# OPTIMIZE: Chỉ show sample, không orderBy toàn bộ dataset (rất chậm!)\n",
    "print(\"\\n[METADATA] Sample records (unsorted):\")\n",
    "df_combined.show(10, truncate=False)\n",
    "\n",
    "# OPTIONAL: Nếu cần sort, chỉ sort 1 partition nhỏ để xem\n",
    "# df_combined.orderBy(\"location_id\", \"datetime\").limit(50).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ce9002",
   "metadata": {
    "papermill": {
     "duration": 0.022375,
     "end_time": "2025-11-09T05:39:49.555114",
     "exception": false,
     "start_time": "2025-11-09T05:39:49.532739",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Tổng quan Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35a7c42b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T05:39:49.605316Z",
     "iopub.status.busy": "2025-11-09T05:39:49.603662Z",
     "iopub.status.idle": "2025-11-09T05:39:56.331716Z",
     "shell.execute_reply": "2025-11-09T05:39:56.330467Z"
    },
    "papermill": {
     "duration": 6.75469,
     "end_time": "2025-11-09T05:39:56.334517",
     "exception": false,
     "start_time": "2025-11-09T05:39:49.579827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATA] Dataset Overview by Location:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----+\n",
      "|location_id|location            |count|\n",
      "+-----------+--------------------+-----+\n",
      "|233335     |North-245631        |21679|\n",
      "|233336     |Southern-245632     |21255|\n",
      "|7727       |Tung Chung-7727     |22360|\n",
      "|7728       |Mong Kok-7728       |21676|\n",
      "|7730       |Central/Western-7730|21267|\n",
      "|7732       |Causeway Bay-7732   |21299|\n",
      "|7733       |Sha Tin-7733        |21228|\n",
      "|7734       |Sham Shui Po-7734   |21247|\n",
      "|7735       |Kwun Tong-7735      |21270|\n",
      "|7736       |Kwai Chung-7736     |21274|\n",
      "|7737       |Tai Po-7737         |21268|\n",
      "|7739       |Yuen Long-7739      |21721|\n",
      "|7740       |Tsuen Wan-7740      |21687|\n",
      "|7742       |Tuen Mun-7742       |2368 |\n",
      "|7742       |Tuen Mun-932161     |18719|\n",
      "+-----------+--------------------+-----+\n",
      "\n",
      "\n",
      "[?] Time Range by Location:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 644:=================================================>   (105 + 5) / 112]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-------------------+-------+\n",
      "|location_id|start_date         |end_date           |records|\n",
      "+-----------+-------------------+-------------------+-------+\n",
      "|233335     |2022-11-01 00:00:00|2025-09-30 16:00:00|21679  |\n",
      "|233336     |2022-11-01 00:00:00|2025-09-30 16:00:00|21255  |\n",
      "|7727       |2022-11-01 00:00:00|2025-09-30 16:00:00|22360  |\n",
      "|7728       |2022-11-01 00:00:00|2025-09-30 16:00:00|21676  |\n",
      "|7730       |2022-11-01 00:00:00|2025-09-30 16:00:00|21267  |\n",
      "|7732       |2022-11-01 00:00:00|2025-09-30 16:00:00|21299  |\n",
      "|7733       |2022-11-01 00:00:00|2025-09-30 16:00:00|21228  |\n",
      "|7734       |2022-11-01 00:00:00|2025-09-30 16:00:00|21247  |\n",
      "|7735       |2022-11-01 00:00:00|2025-09-30 16:00:00|21270  |\n",
      "|7736       |2022-11-01 00:00:00|2025-09-30 16:00:00|21274  |\n",
      "|7737       |2022-11-01 00:00:00|2025-09-30 16:00:00|21268  |\n",
      "|7739       |2022-11-01 00:00:00|2025-09-30 16:00:00|21721  |\n",
      "|7740       |2022-11-01 00:00:00|2025-09-30 16:00:00|21687  |\n",
      "|7742       |2022-11-01 00:00:00|2025-09-30 16:00:00|21087  |\n",
      "+-----------+-------------------+-------------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Thống kê theo location\n",
    "print(\"[DATA] Dataset Overview by Location:\")\n",
    "df_combined.groupBy(\"location_id\", \"location\").count().orderBy(\"location_id\").show(truncate=False)\n",
    "\n",
    "# Time range của từng location\n",
    "print(\"\\n[?] Time Range by Location:\")\n",
    "df_combined.groupBy(\"location_id\").agg(\n",
    "    F.min(\"datetime\").alias(\"start_date\"),\n",
    "    F.max(\"datetime\").alias(\"end_date\"),\n",
    "    F.count(\"*\").alias(\"records\")\n",
    ").orderBy(\"location_id\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9f2dff1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T05:39:56.389377Z",
     "iopub.status.busy": "2025-11-09T05:39:56.388984Z",
     "iopub.status.idle": "2025-11-09T05:40:37.462090Z",
     "shell.execute_reply": "2025-11-09T05:40:37.461149Z"
    },
    "papermill": {
     "duration": 41.09938,
     "end_time": "2025-11-09T05:40:37.463617",
     "exception": false,
     "start_time": "2025-11-09T05:39:56.364237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING]  Missing Values Summary:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NO2                      :    7,612 (  2.53%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PM10                     :    3,391 (  1.13%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PM2_5                    :   11,161 (  3.72%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  SO2                      :    7,424 (  2.47%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Kiểm tra missing values\n",
    "print(\"[WARNING]  Missing Values Summary:\")\n",
    "for col_name in df_combined.columns:\n",
    "    null_count = df_combined.filter(F.col(col_name).isNull()).count()\n",
    "    total = df_combined.count()\n",
    "    pct = (null_count / total) * 100\n",
    "    if null_count > 0:  # Chỉ hiển thị cột có missing\n",
    "        print(f\"  {col_name:25s}: {null_count:8,} ({pct:6.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cae8505",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T05:40:37.500188Z",
     "iopub.status.busy": "2025-11-09T05:40:37.499796Z",
     "iopub.status.idle": "2025-11-09T05:40:39.850136Z",
     "shell.execute_reply": "2025-11-09T05:40:39.849231Z"
    },
    "papermill": {
     "duration": 2.375417,
     "end_time": "2025-11-09T05:40:39.856154",
     "exception": false,
     "start_time": "2025-11-09T05:40:37.480737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[?] Overall Statistics:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/09 05:40:37 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 2473:===========================================>         (91 + 4) / 112]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+-----------------+--------------------+------------------+------------------+\n",
      "|summary|             PM2_5|              PM10|               NO2|               SO2|   temperature_2m|relative_humidity_2m|    wind_speed_10m|     precipitation|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+--------------------+------------------+------------------+\n",
      "|  count|            289157|            296927|            292706|            292894|           300318|              300318|            300318|            300318|\n",
      "|   mean|15.684073703904803|25.418589417600963|  39.2034498780346|3.7197607325517086|23.17203064751364|   79.55055974000892|12.503190950925351|0.2767030281235236|\n",
      "| stddev|10.897151413820698| 19.75489546779464|26.134835003178324|2.4372186299768455|5.550822884231528|   15.46350485998542| 6.256373957792993|1.1696306615920375|\n",
      "|    min|               0.0|               0.0|               0.0|               0.0|              1.9|                16.0|               0.0|               0.0|\n",
      "|    max|             182.5|             401.7|             292.6|              76.9|             36.5|               100.0|              86.6|              53.2|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+--------------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Statistics tổng quan\n",
    "print(\"[?] Overall Statistics:\")\n",
    "df_combined.select(\n",
    "    \"PM2_5\", \"PM10\", \"NO2\", \"SO2\",\n",
    "    \"temperature_2m\", \"relative_humidity_2m\", \"wind_speed_10m\", \"precipitation\"\n",
    ").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19aab62",
   "metadata": {
    "papermill": {
     "duration": 0.017544,
     "end_time": "2025-11-09T05:40:39.902203",
     "exception": false,
     "start_time": "2025-11-09T05:40:39.884659",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Làm sạch Dữ liệu\n",
    "\n",
    "**Quy trình làm sạch:**\n",
    "1. **Loại bỏ Outliers trước** - Để tránh giá trị cực đoan ảnh hưởng đến tính toán statistics\n",
    "2. **Fill Missing Values sau** - Imputation dựa trên dữ liệu đã loại bỏ outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c34bc2f",
   "metadata": {
    "papermill": {
     "duration": 0.017653,
     "end_time": "2025-11-09T05:40:39.937104",
     "exception": false,
     "start_time": "2025-11-09T05:40:39.919451",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.1. Loại bỏ Outliers\n",
    "\n",
    "Loại bỏ các giá trị cực đoan trước khi imputation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d684c098",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T05:40:39.972953Z",
     "iopub.status.busy": "2025-11-09T05:40:39.972620Z",
     "iopub.status.idle": "2025-11-09T05:40:55.380227Z",
     "shell.execute_reply": "2025-11-09T05:40:55.379227Z"
    },
    "papermill": {
     "duration": 15.429133,
     "end_time": "2025-11-09T05:40:55.383157",
     "exception": false,
     "start_time": "2025-11-09T05:40:39.954024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATA] Outlier Removal:\n",
      "  Before: 300,318 records\n",
      "  After:  289,157 records\n",
      "  Removed: 11,161 records (3.72%)\n",
      "\n",
      "  [WARNING]  Note: Records with PM2.5 = null are REMOVED (target variable must have real values)\n",
      "\n",
      "[WARNING]  Missing values after outlier removal:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PM2_5     :        0 (  0.00%) [SUCCESS] (Target - must be 0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PM10      :      296 (  0.10%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NO2       :    7,361 (  2.55%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3063:===============================================>    (102 + 4) / 112]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  SO2       :    7,178 (  2.48%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Loại bỏ outliers theo WHO/EPA International Standards (cho dữ liệu Hong Kong)\n",
    "# [WARNING]  QUAN TRỌNG: PM2.5 là TARGET variable - PHẢI có giá trị thật!\n",
    "#     -> Records có PM2.5 = null sẽ BỊ LOẠI BỎ\n",
    "#     -> Chỉ các features khác (PM10, NO2, SO2) mới được phép null và impute sau\n",
    "\n",
    "df_no_outliers = df_combined.filter(\n",
    "    # [TARGET] TARGET: PM2.5 theo WHO Emergency threshold (không cho phép null)\n",
    "    (F.col(\"PM2_5\").isNotNull()) & \n",
    "    (F.col(\"PM2_5\") >= 0) & (F.col(\"PM2_5\") < 250) &  # WHO Emergency: 250 μg/m³\n",
    "    \n",
    "    # [DATA] FEATURES: WHO/EPA International Standards - Cho phép null, chỉ loại outliers\n",
    "    ((F.col(\"PM10\").isNull()) | ((F.col(\"PM10\") >= 0) & (F.col(\"PM10\") < 430))) &  # WHO Emergency: 430 μg/m³\n",
    "    ((F.col(\"NO2\").isNull()) | ((F.col(\"NO2\") >= 0) & (F.col(\"NO2\") < 400))) &     # WHO/EU: 400 μg/m³ (1-hour)\n",
    "    ((F.col(\"SO2\").isNull()) | ((F.col(\"SO2\") >= 0) & (F.col(\"SO2\") < 500))) &     # WHO/EU: 500 μg/m³ (10-min)\n",
    "    \n",
    "    # [?] WEATHER: WMO standards cho Hong Kong\n",
    "    (F.col(\"precipitation\") >= 0) & (F.col(\"precipitation\") < 100)  # WMO: 100mm/h extreme rain\n",
    ")\n",
    "\n",
    "records_before = df_combined.count()\n",
    "records_after = df_no_outliers.count()\n",
    "removed = records_before - records_after\n",
    "\n",
    "print(f\"[DATA] Outlier Removal:\")\n",
    "print(f\"  Before: {records_before:,} records\")\n",
    "print(f\"  After:  {records_after:,} records\")\n",
    "print(f\"  Removed: {removed:,} records ({removed/records_before*100:.2f}%)\")\n",
    "print(f\"\\n  [WARNING]  Note: Records with PM2.5 = null are REMOVED (target variable must have real values)\")\n",
    "\n",
    "# Kiểm tra missing values sau khi loại outliers\n",
    "print(\"\\n[WARNING]  Missing values after outlier removal:\")\n",
    "for col_name in [\"PM2_5\", \"PM10\", \"NO2\", \"SO2\"]:\n",
    "    if col_name in df_no_outliers.columns:\n",
    "        null_count = df_no_outliers.filter(F.col(col_name).isNull()).count()\n",
    "        total = df_no_outliers.count()\n",
    "        pct = (null_count / total) * 100\n",
    "        if null_count > 0:\n",
    "            print(f\"  {col_name:10s}: {null_count:8,} ({pct:6.2f}%)\")\n",
    "        elif col_name == \"PM2_5\":\n",
    "            print(f\"  {col_name:10s}: {null_count:8,} ({pct:6.2f}%) [SUCCESS] (Target - must be 0%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21804fa8",
   "metadata": {
    "papermill": {
     "duration": 0.019054,
     "end_time": "2025-11-09T05:40:55.434505",
     "exception": false,
     "start_time": "2025-11-09T05:40:55.415451",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.2. Xử lý Missing Values (Interpolation)\n",
    "\n",
    "**Chiến lược Imputation cho Time Series:**\n",
    "- **PM2.5**: Đã loại bỏ tất cả records có null (target variable)\n",
    "- **PM10, NO2, SO2**: Sử dụng **Linear Interpolation** (tốt nhất cho time series)\n",
    "  - Bước 1: **Linear Interpolation** - Nội suy tuyến tính dựa trên giá trị trước & sau\n",
    "  - Bước 2: **Forward Fill** - Xử lý missing ở cuối chuỗi (không có giá trị sau)\n",
    "  - Bước 3: **Backward Fill** - Xử lý missing ở đầu chuỗi (không có giá trị trước)\n",
    "  - Bước 4: **Mean** - Backup cuối cùng (nếu còn missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55e12c84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T05:40:55.475623Z",
     "iopub.status.busy": "2025-11-09T05:40:55.475199Z",
     "iopub.status.idle": "2025-11-09T05:41:05.871038Z",
     "shell.execute_reply": "2025-11-09T05:41:05.868257Z"
    },
    "papermill": {
     "duration": 10.419148,
     "end_time": "2025-11-09T05:41:05.872906",
     "exception": false,
     "start_time": "2025-11-09T05:40:55.453758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROCESSING] Time Series Imputation Strategy (PySpark Native):\n",
      "   1. True Linear Interpolation - y = y₁ + (y₂-y₁) × (t-t₁)/(t₂-t₁)\n",
      "   2. Forward Fill - If only prev value available\n",
      "   3. Backward Fill - If only next value available\n",
      "   4. Null - If no surrounding values (rare)\n",
      "\n",
      "   Columns to impute: ['PM10', 'NO2', 'SO2']\n",
      "   PM2.5 NOT imputed (target variable - already removed nulls)\n",
      "   [?] Safe: Window partitioned by location_id (no cross-location interpolation)\n",
      "\n",
      "[WARNING]  Missing values BEFORE interpolation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PM10      :      296 (  0.10%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NO2       :    7,361 (  2.55%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3446:=============================================>       (97 + 4) / 112]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  SO2       :    7,178 (  2.48%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Chiến lược Imputation cho Time Series Data\n",
    "# Sử dụng PySpark Window Functions - Nội suy tuyến tính dựa trên khoảng cách thời gian\n",
    "\n",
    "# List các cột FEATURES cần impute (KHÔNG bao gồm PM2.5 - target variable)\n",
    "pollutant_cols = [\"PM10\", \"NO2\", \"SO2\"]  # [WARNING] Không có PM2.5!\n",
    "\n",
    "print(f\"[PROCESSING] Time Series Imputation Strategy (PySpark Native):\")\n",
    "print(f\"   1. True Linear Interpolation - y = y₁ + (y₂-y₁) × (t-t₁)/(t₂-t₁)\")\n",
    "print(f\"   2. Forward Fill - If only prev value available\")\n",
    "print(f\"   3. Backward Fill - If only next value available\")\n",
    "print(f\"   4. Null - If no surrounding values (rare)\")\n",
    "print(f\"\\n   Columns to impute: {pollutant_cols}\")\n",
    "print(f\"   PM2.5 NOT imputed (target variable - already removed nulls)\")\n",
    "print(f\"   [?] Safe: Window partitioned by location_id (no cross-location interpolation)\\n\")\n",
    "\n",
    "# Cache để tăng performance\n",
    "df_filled = df_no_outliers.cache()\n",
    "\n",
    "# Kiểm tra missing TRƯỚC khi interpolate\n",
    "print(\"[WARNING]  Missing values BEFORE interpolation:\")\n",
    "for col_name in pollutant_cols:\n",
    "    if col_name in df_filled.columns:\n",
    "        null_count = df_filled.filter(F.col(col_name).isNull()).count()\n",
    "        total = df_filled.count()\n",
    "        pct = (null_count / total) * 100\n",
    "        if null_count > 0:\n",
    "            print(f\"  {col_name:10s}: {null_count:8,} ({pct:6.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e81f0645",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T05:41:05.917010Z",
     "iopub.status.busy": "2025-11-09T05:41:05.916566Z",
     "iopub.status.idle": "2025-11-09T05:51:01.790650Z",
     "shell.execute_reply": "2025-11-09T05:51:01.788731Z"
    },
    "papermill": {
     "duration": 595.89997,
     "end_time": "2025-11-09T05:51:01.793314",
     "exception": false,
     "start_time": "2025-11-09T05:41:05.893344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROCESSING] Applying true linear interpolation per location (PySpark native)...\n",
      "  ▶ Interpolating PM10... [OK]\n",
      "  ▶ Interpolating NO2... [OK]\n",
      "  ▶ Interpolating SO2... [OK]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3535:=================================================>      (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUCCESS] Linear interpolation completed! Total records: 289,157\n",
      "   [GEAR]  Method: True linear interpolation based on time distance (epoch)\n",
      "   [?] Safe: No cross-location interpolation (partitioned by location_id)\n",
      "   [RUN] Optimized: Native PySpark (no Pandas conversion)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Áp dụng True Linear Interpolation với PySpark (không dùng Pandas)\n",
    "# Nội suy tuyến tính dựa trên khoảng cách thời gian THỰC (epoch)\n",
    "# Window function đảm bảo KHÔNG nội suy chéo giữa các locations\n",
    "\n",
    "print(\"[PROCESSING] Applying true linear interpolation per location (PySpark native)...\")\n",
    "\n",
    "# Tạo cột epoch (timestamp dạng số) để tính toán khoảng cách thời gian\n",
    "df_filled = df_filled.withColumn(\"epoch\", F.col(\"datetime\").cast(\"long\"))\n",
    "\n",
    "# Định nghĩa Window cho từng location\n",
    "w_forward = (\n",
    "    Window.partitionBy(\"location_id\")\n",
    "    .orderBy(\"epoch\")\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    ")\n",
    "\n",
    "w_backward = (\n",
    "    Window.partitionBy(\"location_id\")\n",
    "    .orderBy(\"epoch\")\n",
    "    .rowsBetween(Window.currentRow, Window.unboundedFollowing)\n",
    ")\n",
    "\n",
    "# Xử lý từng pollutant column\n",
    "for col_name in pollutant_cols:\n",
    "    if col_name not in df_filled.columns:\n",
    "        continue\n",
    "    \n",
    "    print(f\"  ▶ Interpolating {col_name}...\", end=\" \", flush=True)\n",
    "    \n",
    "    # Bước 1: Tìm giá trị & timestamp TRƯỚC và SAU gần nhất (có giá trị non-null)\n",
    "    df_filled = (\n",
    "        df_filled\n",
    "        .withColumn(f\"{col_name}_prev_value\", F.last(col_name, True).over(w_forward))\n",
    "        .withColumn(f\"{col_name}_next_value\", F.first(col_name, True).over(w_backward))\n",
    "        .withColumn(f\"{col_name}_prev_time\", F.last(F.when(F.col(col_name).isNotNull(), F.col(\"epoch\")), True).over(w_forward))\n",
    "        .withColumn(f\"{col_name}_next_time\", F.first(F.when(F.col(col_name).isNotNull(), F.col(\"epoch\")), True).over(w_backward))\n",
    "    )\n",
    "    \n",
    "    # Bước 2: Tính toán Linear Interpolation theo công thức:\n",
    "    # y = y₁ + (y₂ - y₁) * (t - t₁) / (t₂ - t₁)\n",
    "    interpolated_value = (\n",
    "        F.col(f\"{col_name}_prev_value\") +\n",
    "        (F.col(f\"{col_name}_next_value\") - F.col(f\"{col_name}_prev_value\")) *\n",
    "        ((F.col(\"epoch\") - F.col(f\"{col_name}_prev_time\")) /\n",
    "         (F.col(f\"{col_name}_next_time\") - F.col(f\"{col_name}_prev_time\")))\n",
    "    )\n",
    "    \n",
    "    # Bước 3: Logic chọn giá trị cuối cùng với fallback\n",
    "    df_filled = df_filled.withColumn(\n",
    "        col_name,\n",
    "        F.when(F.col(col_name).isNotNull(), F.col(col_name))  # Giữ nguyên nếu có giá trị\n",
    "         .when(\n",
    "             # Linear interpolation nếu có cả prev & next và không chia 0\n",
    "             (F.col(f\"{col_name}_prev_value\").isNotNull()) &\n",
    "             (F.col(f\"{col_name}_next_value\").isNotNull()) &\n",
    "             ((F.col(f\"{col_name}_next_time\") - F.col(f\"{col_name}_prev_time\")) != 0),\n",
    "             interpolated_value\n",
    "         )\n",
    "         .when(F.col(f\"{col_name}_prev_value\").isNotNull(), F.col(f\"{col_name}_prev_value\"))  # Forward fill\n",
    "         .when(F.col(f\"{col_name}_next_value\").isNotNull(), F.col(f\"{col_name}_next_value\"))  # Backward fill\n",
    "         .otherwise(None)  # Vẫn null nếu không có data nào\n",
    "    )\n",
    "    \n",
    "    # Bước 4: Xóa các cột phụ để giảm memory\n",
    "    df_filled = df_filled.drop(\n",
    "        f\"{col_name}_prev_value\", f\"{col_name}_next_value\",\n",
    "        f\"{col_name}_prev_time\", f\"{col_name}_next_time\"\n",
    "    )\n",
    "    \n",
    "    print(\"[OK]\")\n",
    "\n",
    "# Cache kết quả sau khi interpolation\n",
    "df_filled = df_filled.cache()\n",
    "\n",
    "# Trigger computation và đếm records\n",
    "count = df_filled.count()\n",
    "print(f\"\\n[SUCCESS] Linear interpolation completed! Total records: {count:,}\")\n",
    "print(f\"   [GEAR]  Method: True linear interpolation based on time distance (epoch)\")\n",
    "print(f\"   [?] Safe: No cross-location interpolation (partitioned by location_id)\")\n",
    "print(f\"   [RUN] Optimized: Native PySpark (no Pandas conversion)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7226022e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T05:51:01.870206Z",
     "iopub.status.busy": "2025-11-09T05:51:01.869876Z",
     "iopub.status.idle": "2025-11-09T05:51:06.902198Z",
     "shell.execute_reply": "2025-11-09T05:51:06.901296Z"
    },
    "papermill": {
     "duration": 5.072413,
     "end_time": "2025-11-09T05:51:06.904291",
     "exception": false,
     "start_time": "2025-11-09T05:51:01.831878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[METADATA] Final Missing Values Check (After Interpolation):\n",
      "  PM2_5 (Target): 0 nulls [SUCCESS] (Must be 0)\n",
      "  PM10      : 0 nulls [SUCCESS]\n",
      "  NO2       : 0 nulls [SUCCESS]\n",
      "  SO2       : 0 nulls [SUCCESS]\n",
      "\n",
      "  [SUCCESS] No missing values in any feature columns!\n",
      "\n",
      "[DATA] Final Verification:\n",
      "  PM2_5     : 0 nulls [SUCCESS]\n",
      "  PM10      : 0 nulls [SUCCESS]\n",
      "  NO2       : 0 nulls [SUCCESS]\n",
      "  SO2       : 0 nulls [SUCCESS]\n",
      "\n",
      "[SUCCESS] Data cleaning completed with True Linear Interpolation!\n",
      "   Final dataset: 289,157 records\n",
      "   [WARNING]  All records have REAL PM2.5 values (target variable)\n",
      "   [SUCCESS] Features interpolated smoothly (time-based linear interpolation)\n",
      "   [SUCCESS] Edge cases (no surrounding data) removed\n",
      "   [RUN] Performance: Native PySpark (no Pandas conversion)\n"
     ]
    }
   ],
   "source": [
    "# Verify: PM2.5 không có null, các features khác không có null\n",
    "print(\"\\n[METADATA] Final Missing Values Check (After Interpolation):\")\n",
    "\n",
    "# Kiểm tra PM2.5 (target)\n",
    "pm25_nulls = df_filled.filter(F.col(\"PM2_5\").isNull()).count()\n",
    "print(f\"  PM2_5 (Target): {pm25_nulls:,} nulls [SUCCESS] (Must be 0)\")\n",
    "\n",
    "# Kiểm tra features\n",
    "total_nulls = 0\n",
    "for col_name in pollutant_cols:\n",
    "    if col_name in df_filled.columns:\n",
    "        null_count = df_filled.filter(F.col(col_name).isNull()).count()\n",
    "        total_nulls += null_count\n",
    "        if null_count > 0:\n",
    "            print(f\"  {col_name:10s}: {null_count:,} nulls [WARNING]\")\n",
    "        else:\n",
    "            print(f\"  {col_name:10s}: {null_count:,} nulls [SUCCESS]\")\n",
    "\n",
    "# Xử lý edge case: Drop records còn null (không có giá trị xung quanh để interpolate)\n",
    "if total_nulls > 0:\n",
    "    print(f\"\\n[WARNING]  Found {total_nulls} remaining nulls (edge cases with no surrounding data)\")\n",
    "    print(f\"   -> Dropping these records to ensure data quality...\")\n",
    "    \n",
    "    records_before_drop = df_filled.count()\n",
    "    \n",
    "    # Drop records có bất kỳ feature nào còn null\n",
    "    for col_name in pollutant_cols:\n",
    "        df_filled = df_filled.filter(F.col(col_name).isNotNull())\n",
    "    \n",
    "    records_after_drop = df_filled.count()\n",
    "    dropped = records_before_drop - records_after_drop\n",
    "    \n",
    "    print(f\"   Before drop: {records_before_drop:,} records\")\n",
    "    print(f\"   After drop:  {records_after_drop:,} records\")\n",
    "    print(f\"   Dropped:     {dropped:,} records ({dropped/records_before_drop*100:.2f}%)\")\n",
    "    print(f\"\\n   [SUCCESS] All feature columns now have 0 nulls!\")\n",
    "else:\n",
    "    print(\"\\n  [SUCCESS] No missing values in any feature columns!\")\n",
    "\n",
    "# Xóa cột epoch (đã dùng xong)\n",
    "df_filled = df_filled.drop(\"epoch\")\n",
    "\n",
    "# Verify lần cuối\n",
    "print(f\"\\n[DATA] Final Verification:\")\n",
    "for col_name in [\"PM2_5\"] + pollutant_cols:\n",
    "    if col_name in df_filled.columns:\n",
    "        null_count = df_filled.filter(F.col(col_name).isNull()).count()\n",
    "        print(f\"  {col_name:10s}: {null_count:,} nulls [SUCCESS]\")\n",
    "\n",
    "print(f\"\\n[SUCCESS] Data cleaning completed with True Linear Interpolation!\")\n",
    "print(f\"   Final dataset: {df_filled.count():,} records\")\n",
    "print(f\"   [WARNING]  All records have REAL PM2.5 values (target variable)\")\n",
    "print(f\"   [SUCCESS] Features interpolated smoothly (time-based linear interpolation)\")\n",
    "print(f\"   [SUCCESS] Edge cases (no surrounding data) removed\")\n",
    "print(f\"   [RUN] Performance: Native PySpark (no Pandas conversion)\")\n",
    "\n",
    "# Cập nhật df_combined với dữ liệu đã clean và sắp xếp\n",
    "df_combined = df_filled.orderBy(\"location_id\", \"datetime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595a62e4",
   "metadata": {
    "papermill": {
     "duration": 0.022705,
     "end_time": "2025-11-09T05:51:06.961071",
     "exception": false,
     "start_time": "2025-11-09T05:51:06.938366",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Feature Engineering & Normalization\n",
    "\n",
    "**Quy trình ĐÚNG để tránh Data Leakage:**\n",
    "1. **Time Features** - Thêm cyclic encoding (sin/cos) và is_weekend (không cần normalize)\n",
    "2. **Temporal Split** - Chia train/validation/test theo thời gian (70/15/15)\n",
    "3. **Normalization** - Chuẩn hóa **CHỈ numerical GỐC** bằng Min-Max từ train set\n",
    "4. **Lag Features** - Tạo lag TỪ CÁC CỘT ĐÃ SCALE (giữ đúng scale relationship)\n",
    "5. **Model-Specific Datasets** - Chuẩn bị riêng cho Deep Learning và XGBoost\n",
    "6. **Null Handling** - Xử lý nulls trong lag features cuối cùng\n",
    "\n",
    "**[WARNING] QUAN TRỌNG:** Lag features phải tạo SAU khi normalize để giữ đúng mối quan hệ scale!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9e25f0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T05:51:07.005415Z",
     "iopub.status.busy": "2025-11-09T05:51:07.005140Z",
     "iopub.status.idle": "2025-11-09T05:51:07.844247Z",
     "shell.execute_reply": "2025-11-09T05:51:07.843275Z"
    },
    "papermill": {
     "duration": 0.864,
     "end_time": "2025-11-09T05:51:07.846196",
     "exception": false,
     "start_time": "2025-11-09T05:51:06.982196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROCESSING] Step 1: Adding Time Features (No normalization needed)...\n",
      "[OK] Time features added successfully!\n",
      "[OK] Total records: 289,157\n",
      "[OK] Total columns: 22\n",
      "\n",
      "[METADATA] Time Features Created:\n",
      "  Cyclic (sin/cos): hour, month, day_of_week -> Already in [-1, 1]\n",
      "  Binary: is_weekend -> Already in [0, 1]\n",
      "  [SUCCESS] No normalization needed for time features!\n"
     ]
    }
   ],
   "source": [
    "# Bước 1: Thêm Time Features từ dữ liệu đã clean\n",
    "print(\"[PROCESSING] Step 1: Adding Time Features (No normalization needed)...\")\n",
    "\n",
    "import math\n",
    "\n",
    "df_features = df_combined \\\n",
    "    .withColumn(\"hour\", F.hour(\"datetime\")) \\\n",
    "    .withColumn(\"month\", F.month(\"datetime\")) \\\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(\"datetime\"))\n",
    "\n",
    "# Cyclic encoding cho hour (24h cycle)\n",
    "df_features = df_features \\\n",
    "    .withColumn(\"hour_sin\", F.sin(2 * math.pi * F.col(\"hour\") / 24)) \\\n",
    "    .withColumn(\"hour_cos\", F.cos(2 * math.pi * F.col(\"hour\") / 24))\n",
    "\n",
    "# Cyclic encoding cho month (12 month cycle)\n",
    "df_features = df_features \\\n",
    "    .withColumn(\"month_sin\", F.sin(2 * math.pi * F.col(\"month\") / 12)) \\\n",
    "    .withColumn(\"month_cos\", F.cos(2 * math.pi * F.col(\"month\") / 12))\n",
    "\n",
    "# Cyclic encoding cho day_of_week (7 day cycle)\n",
    "df_features = df_features \\\n",
    "    .withColumn(\"day_of_week_sin\", F.sin(2 * math.pi * F.col(\"day_of_week\") / 7)) \\\n",
    "    .withColumn(\"day_of_week_cos\", F.cos(2 * math.pi * F.col(\"day_of_week\") / 7))\n",
    "\n",
    "# Binary feature: is_weekend\n",
    "df_features = df_features \\\n",
    "    .withColumn(\"is_weekend\", F.when(F.col(\"day_of_week\").isin([1, 7]), 1).otherwise(0))\n",
    "\n",
    "# ✅ FIX: Thêm cyclic encoding cho wind_direction\n",
    "df_features = df_features \\\n",
    "    .withColumn(\"wind_direction_sin\", F.sin(2 * math.pi * F.col(\"wind_direction_10m\") / 360)) \\\n",
    "    .withColumn(\"wind_direction_cos\", F.cos(2 * math.pi * F.col(\"wind_direction_10m\") / 360))\n",
    "\n",
    "# Xóa các cột trung gian\n",
    "df_features = df_features.drop(\"hour\", \"month\", \"day_of_week\", \"wind_direction_10m\")\n",
    "\n",
    "print(\"[OK] Time features added successfully!\")\n",
    "print(f\"[OK] Total records: {df_features.count():,}\")\n",
    "print(f\"[OK] Total columns: {len(df_features.columns)}\")\n",
    "\n",
    "print(\"\\n[METADATA] Time Features Created:\")\n",
    "print(\"  Cyclic (sin/cos): hour, month, day_of_week -> Already in [-1, 1]\")\n",
    "print(\"  Binary: is_weekend -> Already in [0, 1]\")\n",
    "print(\"  [SUCCESS] No normalization needed for time features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cee54f8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T05:51:07.898795Z",
     "iopub.status.busy": "2025-11-09T05:51:07.898445Z",
     "iopub.status.idle": "2025-11-09T05:51:10.719095Z",
     "shell.execute_reply": "2025-11-09T05:51:10.718166Z"
    },
    "papermill": {
     "duration": 2.846445,
     "end_time": "2025-11-09T05:51:10.721137",
     "exception": false,
     "start_time": "2025-11-09T05:51:07.874692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PROCESSING] Step 2: Temporal Train/Val/Test Split BEFORE Normalization...\n",
      "[?] Temporal Split (Avoiding Data Leakage):\n",
      "  [?] Train:      2022-11-01 -> 2024-11-14 (744 days)\n",
      "  [?] Validation: 2024-11-14 -> 2025-04-22 (159 days)\n",
      "  [?] Test:       2025-04-22 -> 2025-09-30 (161 days)\n",
      "\n",
      "[DATA] Split Results:\n",
      "  [?] Train:  187,587 (64.9%)\n",
      "  [?] Val:     49,956 (17.3%)\n",
      "  [?] Test:    51,614 (17.8%)\n",
      "\n",
      "[SUCCESS] Temporal split completed!\n",
      "   [WARNING]  Next: Normalize using TRAIN SET statistics ONLY\n"
     ]
    }
   ],
   "source": [
    "# Bước 2: TEMPORAL SPLIT TRƯỚC KHI NORMALIZE (Tránh Data Leakage)\n",
    "print(\"\\n[PROCESSING] Step 2: Temporal Train/Val/Test Split BEFORE Normalization...\")\n",
    "\n",
    "# Tính toán ngày chia dựa trên percentile thời gian  \n",
    "time_stats = df_features.select(\n",
    "    F.min(\"datetime\").alias(\"min_time\"),\n",
    "    F.max(\"datetime\").alias(\"max_time\")\n",
    ").collect()[0]\n",
    "\n",
    "min_time = time_stats[\"min_time\"]\n",
    "max_time = time_stats[\"max_time\"]\n",
    "total_days = (max_time - min_time).days\n",
    "\n",
    "# 70% train, 15% validation, 15% test\n",
    "train_days = int(total_days * 0.70)\n",
    "val_days = int(total_days * 0.15)\n",
    "\n",
    "train_end = min_time + pd.Timedelta(days=train_days)\n",
    "val_end = train_end + pd.Timedelta(days=val_days)\n",
    "\n",
    "print(f\"[?] Temporal Split (Avoiding Data Leakage):\")\n",
    "print(f\"  [?] Train:      {min_time.strftime('%Y-%m-%d')} -> {train_end.strftime('%Y-%m-%d')} ({(train_end - min_time).days} days)\")\n",
    "print(f\"  [?] Validation: {train_end.strftime('%Y-%m-%d')} -> {val_end.strftime('%Y-%m-%d')} ({(val_end - train_end).days} days)\")\n",
    "print(f\"  [?] Test:       {val_end.strftime('%Y-%m-%d')} -> {max_time.strftime('%Y-%m-%d')} ({(max_time - val_end).days} days)\")\n",
    "\n",
    "# Split data - Count BEFORE caching to trigger evaluation\n",
    "df_train_raw = df_features.filter(F.col(\"datetime\") < train_end)\n",
    "df_val_raw = df_features.filter((F.col(\"datetime\") >= train_end) & (F.col(\"datetime\") < val_end))\n",
    "df_test_raw = df_features.filter(F.col(\"datetime\") >= val_end)\n",
    "\n",
    "train_count = df_train_raw.count()\n",
    "val_count = df_val_raw.count() \n",
    "test_count = df_test_raw.count()\n",
    "total_count = train_count + val_count + test_count\n",
    "\n",
    "print(f\"\\n[DATA] Split Results:\")\n",
    "print(f\"  [?] Train: {train_count:8,} ({train_count/total_count*100:.1f}%)\")\n",
    "print(f\"  [?] Val:   {val_count:8,} ({val_count/total_count*100:.1f}%)\")\n",
    "print(f\"  [?] Test:  {test_count:8,} ({test_count/total_count*100:.1f}%)\")\n",
    "\n",
    "# Now cache after counting (avoids double computation)\n",
    "df_train_raw = df_train_raw.cache()\n",
    "df_val_raw = df_val_raw.cache()\n",
    "df_test_raw = df_test_raw.cache()\n",
    "\n",
    "print(f\"\\n[SUCCESS] Temporal split completed!\")\n",
    "print(f\"   [WARNING]  Next: Normalize using TRAIN SET statistics ONLY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf410815",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T05:51:10.795044Z",
     "iopub.status.busy": "2025-11-09T05:51:10.794632Z",
     "iopub.status.idle": "2025-11-09T05:51:24.591931Z",
     "shell.execute_reply": "2025-11-09T05:51:24.590432Z"
    },
    "papermill": {
     "duration": 13.830395,
     "end_time": "2025-11-09T05:51:24.593742",
     "exception": false,
     "start_time": "2025-11-09T05:51:10.763347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PROCESSING] Step 3: Normalize NUMERICAL BASE FEATURES using TRAIN SET ONLY...\n",
      "[DATA] Normalizing 9 BASE features (NO lag features yet)...\n",
      "   Features to normalize: ['PM2_5', 'PM10', 'NO2', 'SO2', 'temperature_2m', 'relative_humidity_2m', 'wind_speed_10m', 'wind_direction_10m', 'precipitation']\n",
      "   [WARNING]  Computing min/max from TRAIN SET ONLY (preventing data leakage)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] PM2_5                         : [    0.00,   152.40] -> [0, 1]\n",
      "  [OK] PM10                          : [    0.00,   227.30] -> [0, 1]\n",
      "  [OK] NO2                           : [    0.00,   292.60] -> [0, 1]\n",
      "  [OK] SO2                           : [    0.00,    76.90] -> [0, 1]\n",
      "  [OK] temperature_2m                : [    1.90,    36.00] -> [0, 1]\n",
      "  [OK] relative_humidity_2m          : [   22.00,   100.00] -> [0, 1]\n",
      "  [OK] wind_speed_10m                : [    0.00,    82.30] -> [0, 1]\n",
      "  [OK] wind_direction_10m            : [    0.00,   360.00] -> [0, 1]\n",
      "  [OK] precipitation                 : [    0.00,    53.20] -> [0, 1]\n",
      "\n",
      "[SUCCESS] Scaler parameters computed from TRAIN SET only!\n",
      "\n",
      " Applying Min-Max scaling [0, 1] to all splits...\n",
      "[SUCCESS] Base feature normalization completed!\n",
      "   [DATA] All splits normalized using train statistics only\n",
      "   [WARNING]  Next: Create lag features FROM SCALED COLUMNS\n"
     ]
    }
   ],
   "source": [
    "# Bước 3: Normalize NUMERICAL GỐC (CHỈ gốc, KHÔNG có lag features)\n",
    "print(f\"\\n[PROCESSING] Step 3: Normalize NUMERICAL BASE FEATURES using TRAIN SET ONLY...\")\n",
    "\n",
    "# [WARNING] QUAN TRỌNG: CHỈ normalize các cột GỐC, KHÔNG bao gồm lag features\n",
    "# Lag features sẽ tạo SAU từ các cột đã scale\n",
    "numerical_base_cols = [\n",
    "    # Pollutants (current values only)\n",
    "    \"PM2_5\", \"PM10\", \"NO2\", \"SO2\",\n",
    "    # Weather features (current values only)\n",
    "    \"temperature_2m\", \"relative_humidity_2m\", \"wind_speed_10m\",\n",
    "    \"surface_pressure\", \"precipitation\"\n",
    "]\n",
    "\n",
    "print(f\"[DATA] Normalizing {len(numerical_base_cols)} BASE features (NO lag features yet)...\")\n",
    "print(f\"   Features to normalize: {numerical_base_cols}\")\n",
    "print(f\"   [WARNING]  Computing min/max from TRAIN SET ONLY (preventing data leakage)\")\n",
    "\n",
    "# Tính min/max CHỈ TỪ TRAIN SET\n",
    "scaler_params = {}\n",
    "\n",
    "for col_name in numerical_base_cols:\n",
    "    if col_name in df_train_raw.columns:\n",
    "        # CHỈ DÙNG TRAIN SET ĐỂ TÍNH MIN/MAX  \n",
    "        stats = df_train_raw.select(\n",
    "            F.min(col_name).alias(\"min\"),\n",
    "            F.max(col_name).alias(\"max\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        min_val = stats[\"min\"]\n",
    "        max_val = stats[\"max\"]\n",
    "        \n",
    "        # [WARNING] CRITICAL: Handle None values from null columns\n",
    "        if min_val is None or max_val is None:\n",
    "            print(f\"  [WARNING]  Skipping {col_name}: All values are null\")\n",
    "            continue\n",
    "        \n",
    "        # [WARNING] CRITICAL: Tránh chia 0 khi min = max\n",
    "        if max_val == min_val:\n",
    "            max_val = min_val + 1\n",
    "        \n",
    "        scaler_params[col_name] = {\"min\": min_val, \"max\": max_val}\n",
    "        print(f\"  [OK] {col_name:30s}: [{min_val:8.2f}, {max_val:8.2f}] -> [0, 1]\")\n",
    "\n",
    "print(f\"\\n[SUCCESS] Scaler parameters computed from TRAIN SET only!\")\n",
    "\n",
    "# Áp dụng normalization cho tất cả splits\n",
    "def apply_scaling(df, scaler_params):\n",
    "    \"\"\"Apply Min-Max scaling using precomputed parameters\"\"\"\n",
    "    df_scaled = df\n",
    "    for col_name, params in scaler_params.items():\n",
    "        if col_name in df.columns:\n",
    "            min_val = params[\"min\"]\n",
    "            max_val = params[\"max\"]\n",
    "            df_scaled = df_scaled.withColumn(\n",
    "                f\"{col_name}_scaled\",\n",
    "                (F.col(col_name) - min_val) / (max_val - min_val)\n",
    "            )\n",
    "    return df_scaled\n",
    "\n",
    "print(f\"\\n Applying Min-Max scaling [0, 1] to all splits...\")\n",
    "\n",
    "# Apply scaling and trigger computation\n",
    "df_train = apply_scaling(df_train_raw, scaler_params)\n",
    "df_val = apply_scaling(df_val_raw, scaler_params)\n",
    "df_test = apply_scaling(df_test_raw, scaler_params)\n",
    "\n",
    "# Trigger computation and cache\n",
    "_ = df_train.count()\n",
    "_ = df_val.count()\n",
    "_ = df_test.count()\n",
    "\n",
    "df_train = df_train.cache()\n",
    "df_val = df_val.cache()\n",
    "df_test = df_test.cache()\n",
    "\n",
    "# Unpersist raw versions to free memory\n",
    "df_train_raw.unpersist()\n",
    "df_val_raw.unpersist()\n",
    "df_test_raw.unpersist()\n",
    "\n",
    "print(f\"[SUCCESS] Base feature normalization completed!\")\n",
    "print(f\"   [DATA] All splits normalized using train statistics only\")\n",
    "print(f\"   [WARNING]  Next: Create lag features FROM SCALED COLUMNS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca9e0bd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T05:51:24.642702Z",
     "iopub.status.busy": "2025-11-09T05:51:24.641625Z",
     "iopub.status.idle": "2025-11-09T05:51:24.652760Z",
     "shell.execute_reply": "2025-11-09T05:51:24.651552Z"
    },
    "papermill": {
     "duration": 0.036962,
     "end_time": "2025-11-09T05:51:24.654212",
     "exception": false,
     "start_time": "2025-11-09T05:51:24.617250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SAVE] Step 4: Saving Scaler Parameters...\n",
      "[KAGGLE] Kaggle mode: Saving to /kaggle/working/processed\n",
      "[SUCCESS] Scaler parameters saved to: /kaggle/working/processed/scaler_params.json\n",
      "   - Computed from TRAIN SET only (no data leakage)\n",
      "   - Used for denormalizing predictions during inference\n",
      "   - Contains 9 base features\n",
      "\n",
      "[METADATA] Example scaler params (from train set):\n",
      "  PM2_5               : min=0.00, max=152.40\n",
      "  temperature_2m      : min=1.90, max=36.00\n",
      "  wind_speed_10m      : min=0.00, max=82.30\n"
     ]
    }
   ],
   "source": [
    "# Bước 4: Lưu Scaler Parameters\n",
    "print(f\"\\n[SAVE] Step 4: Saving Scaler Parameters...\")\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "scaler_json = {\n",
    "    col: {\"min\": float(params[\"min\"]), \"max\": float(params[\"max\"])} \n",
    "    for col, params in scaler_params.items()\n",
    "}\n",
    "\n",
    "# ========================================\n",
    "# ADAPTIVE PATH (Kaggle vs Colab vs Local)\n",
    "# ========================================\n",
    "if IN_KAGGLE:\n",
    "    # [KAGGLE] Kaggle: Write to /kaggle/working (auto-saved on commit)\n",
    "    processed_dir = Path(\"/kaggle/working/processed\")\n",
    "    print(f\"[KAGGLE] Kaggle mode: Saving to {processed_dir}\")\n",
    "    \n",
    "elif IN_COLAB:\n",
    "    # [COLAB] Colab: Write to Google Drive\n",
    "    processed_dir = Path(\"/content/drive/MyDrive/pm25-data/processed\")\n",
    "    print(f\"[COLAB] Colab mode: Saving to Google Drive\")\n",
    "    \n",
    "else:\n",
    "    # [LOCAL] Local: Write to project folder\n",
    "    processed_dir = Path(\"../data/processed\")\n",
    "    print(f\"[LOCAL] Local mode: Saving to {processed_dir}\")\n",
    "\n",
    "# Tạo thư mục với parents=True (tạo cả parent directories nếu chưa có)\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Lưu ra file JSON\n",
    "scaler_path = processed_dir / \"scaler_params.json\"\n",
    "with open(scaler_path, 'w') as f:\n",
    "    json.dump(scaler_json, f, indent=2)\n",
    "\n",
    "print(f\"[SUCCESS] Scaler parameters saved to: {scaler_path}\")\n",
    "print(f\"   - Computed from TRAIN SET only (no data leakage)\")\n",
    "print(f\"   - Used for denormalizing predictions during inference\")\n",
    "print(f\"   - Contains {len(scaler_params)} base features\")\n",
    "\n",
    "# Hiển thị ví dụ\n",
    "print(f\"\\n[METADATA] Example scaler params (from train set):\")\n",
    "example_cols = [\"PM2_5\", \"temperature_2m\", \"wind_speed_10m\"]\n",
    "for col in example_cols:\n",
    "    if col in scaler_params:\n",
    "        params = scaler_params[col]\n",
    "        print(f\"  {col:20s}: min={params['min']:.2f}, max={params['max']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79359497",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T05:51:24.702217Z",
     "iopub.status.busy": "2025-11-09T05:51:24.701885Z",
     "iopub.status.idle": "2025-11-09T05:52:17.996054Z",
     "shell.execute_reply": "2025-11-09T05:52:17.994424Z"
    },
    "papermill": {
     "duration": 53.321113,
     "end_time": "2025-11-09T05:52:17.998418",
     "exception": false,
     "start_time": "2025-11-09T05:51:24.677305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PROCESSING] Step 5: Creating Lag Features FROM SCALED COLUMNS (XGBoost only)...\n",
      "\n",
      "[METADATA] Creating lag features:\n",
      "   Deep Learning models: No lags needed (learn from sequences)\n",
      "   XGBoost: 6 lags × 8 variables = 48 features\n",
      "   [SUCCESS] Using SCALED columns as source (proper scale relationship)\n",
      "\n",
      "[PROCESSING] Creating lag features for all splits...\n",
      "  [OK] Train: 48 lag features created\n",
      "  [OK] Val:   48 lag features created\n",
      "  [OK] Test:  48 lag features created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUCCESS] Lag features created successfully!\n",
      "   [SUCCESS] All lags created FROM SCALED columns\n",
      "   [SUCCESS] Lag and base features have SAME scale parameters\n",
      "   [SUCCESS] Proper temporal relationship preserved\n",
      "\n",
      "[PROCESSING] Handling null values in lag features...\n",
      "\n",
      "[DATA] Null counts BEFORE handling:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PM2_5_lag1_scaled                  :       14 nulls (0.01%)\n",
      "  PM2_5_lag2_scaled                  :       28 nulls (0.01%)\n",
      "  PM2_5_lag3_scaled                  :       42 nulls (0.02%)\n",
      "\n",
      "[WARNING]  Reason: First 24 hours of each location have no previous data\n",
      "   Strategy: DROP records with ANY null lag feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[?]  Dropping records with null lag features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DATA] Records dropped (null lag features):\n",
      "  [?] Train: 187,587 -> 187,251 (dropped 336, 0.18%)\n",
      "  [?] Val:   49,956 -> 49,620 (dropped 336, 0.67%)\n",
      "  [?] Test:  51,614 -> 51,278 (dropped 336, 0.65%)\n",
      "\n",
      "[SUCCESS] Verification - checking for remaining nulls...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PM2_5_lag1_scaled                  :        0 nulls [SUCCESS]\n",
      "  PM2_5_lag2_scaled                  :        0 nulls [SUCCESS]\n",
      "  PM2_5_lag3_scaled                  :        0 nulls [SUCCESS]\n",
      "\n",
      "[SUCCESS] All lag features are clean!\n",
      "\n",
      "[SUCCESS] Lag features + Null handling completed!\n",
      "   - Created 48 lag features FROM SCALED columns\n",
      "   - Lost only first 24 hours per location\n",
      "   - All lag features now have valid values\n",
      "   - Data quality ensured for XGBoost training\n"
     ]
    }
   ],
   "source": [
    "# Bước 5: Tạo Lag Features TỪ CÁC CỘT ĐÃ SCALE (CHỈ CHO XGBOOST)\n",
    "print(f\"\\n[PROCESSING] Step 5: Creating Lag Features FROM SCALED COLUMNS (XGBoost only)...\")\n",
    "\n",
    "# [WARNING] QUAN TRỌNG: Lag features được tạo TỪ CÁC CỘT ĐÃ SCALE\n",
    "# -> Đảm bảo lag và gốc có CÙNG SCALE PARAMETERS\n",
    "# -> Giữ đúng mối quan hệ giữa giá trị hiện tại và quá khứ\n",
    "\n",
    "LAG_STEPS = [1, 2, 3, 6, 12, 24]  # 1h, 2h, 3h, 6h, 12h, 24h trước\n",
    "\n",
    "# Columns cần tạo lag (sử dụng bản SCALED)\n",
    "lag_base_columns = [\"PM2_5\", \"PM10\", \"NO2\", \"SO2\", \n",
    "                    \"temperature_2m\", \"relative_humidity_2m\", \n",
    "                    \"wind_speed_10m\", \"surface_pressure\", \"precipitation\"]\n",
    "\n",
    "print(f\"\\n[METADATA] Creating lag features:\")\n",
    "print(f\"   Deep Learning models: No lags needed (learn from sequences)\")\n",
    "print(f\"   XGBoost: {len(LAG_STEPS)} lags × {len(lag_base_columns)} variables = {len(LAG_STEPS) * len(lag_base_columns)} features\")\n",
    "print(f\"   [SUCCESS] Using SCALED columns as source (proper scale relationship)\")\n",
    "\n",
    "# Window cho từng location (sắp xếp theo thời gian)\n",
    "w_lag = Window.partitionBy(\"location_id\").orderBy(\"datetime\")\n",
    "\n",
    "# Tạo lag features cho từng split (train, val, test)\n",
    "def create_lag_features(df, lag_base_columns, lag_steps):\n",
    "    \"\"\"Create lag features from SCALED columns\"\"\"\n",
    "    df_with_lags = df\n",
    "    \n",
    "    for col_name in lag_base_columns:\n",
    "        col_scaled = f\"{col_name}_scaled\"\n",
    "        \n",
    "        if col_scaled in df.columns:\n",
    "            for lag in lag_steps:\n",
    "                lag_col_name = f\"{col_name}_lag{lag}_scaled\"\n",
    "                \n",
    "                # [SUCCESS] Tạo lag TỪ CỘT ĐÃ SCALE\n",
    "                df_with_lags = df_with_lags.withColumn(\n",
    "                    lag_col_name,\n",
    "                    F.lag(col_scaled, lag).over(w_lag)\n",
    "                )\n",
    "    \n",
    "    return df_with_lags\n",
    "\n",
    "# Apply to all splits\n",
    "print(f\"\\n[PROCESSING] Creating lag features for all splits...\")\n",
    "df_train = create_lag_features(df_train, lag_base_columns, LAG_STEPS)\n",
    "df_val = create_lag_features(df_val, lag_base_columns, LAG_STEPS)\n",
    "df_test = create_lag_features(df_test, lag_base_columns, LAG_STEPS)\n",
    "\n",
    "print(f\"  [OK] Train: {len(LAG_STEPS) * len(lag_base_columns)} lag features created\")\n",
    "print(f\"  [OK] Val:   {len(LAG_STEPS) * len(lag_base_columns)} lag features created\")\n",
    "print(f\"  [OK] Test:  {len(LAG_STEPS) * len(lag_base_columns)} lag features created\")\n",
    "\n",
    "# Trigger computation and cache\n",
    "_ = df_train.count()\n",
    "_ = df_val.count()\n",
    "_ = df_test.count()\n",
    "\n",
    "df_train = df_train.cache()\n",
    "df_val = df_val.cache()\n",
    "df_test = df_test.cache()\n",
    "\n",
    "print(f\"\\n[SUCCESS] Lag features created successfully!\")\n",
    "print(f\"   [SUCCESS] All lags created FROM SCALED columns\")\n",
    "print(f\"   [SUCCESS] Lag and base features have SAME scale parameters\")\n",
    "print(f\"   [SUCCESS] Proper temporal relationship preserved\")\n",
    "\n",
    "# ========================================\n",
    "# XỬ LÝ NULL VALUES TRONG LAG FEATURES\n",
    "# ========================================\n",
    "print(f\"\\n[PROCESSING] Handling null values in lag features...\")\n",
    "\n",
    "# Tạo list tất cả lag feature names\n",
    "lag_feature_names = [f\"{col}_lag{lag}_scaled\" for col in lag_base_columns for lag in LAG_STEPS]\n",
    "\n",
    "# Đếm nulls TRƯỚC khi xử lý\n",
    "print(f\"\\n[DATA] Null counts BEFORE handling:\")\n",
    "sample_lag_features = lag_feature_names[:3]\n",
    "for lag_col in sample_lag_features:\n",
    "    if lag_col in df_train.columns:\n",
    "        null_count = df_train.filter(F.col(lag_col).isNull()).count()\n",
    "        total_count = df_train.count()\n",
    "        print(f\"  {lag_col:35s}: {null_count:8,} nulls ({null_count/total_count*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n[WARNING]  Reason: First {max(LAG_STEPS)} hours of each location have no previous data\")\n",
    "print(f\"   Strategy: DROP records with ANY null lag feature\")\n",
    "\n",
    "# Track counts before drop\n",
    "train_before = df_train.count()\n",
    "val_before = df_val.count()\n",
    "test_before = df_test.count()\n",
    "\n",
    "# Function to drop nulls\n",
    "def drop_lag_nulls(df, lag_features):\n",
    "    \"\"\"Drop records with any null lag feature\"\"\"\n",
    "    df_clean = df\n",
    "    for col in lag_features:\n",
    "        if col in df.columns:\n",
    "            df_clean = df_clean.filter(F.col(col).isNotNull())\n",
    "    return df_clean\n",
    "\n",
    "# Apply to all splits\n",
    "print(f\"\\n[?]  Dropping records with null lag features...\")\n",
    "df_train_clean = drop_lag_nulls(df_train, lag_feature_names)\n",
    "df_val_clean = drop_lag_nulls(df_val, lag_feature_names)\n",
    "df_test_clean = drop_lag_nulls(df_test, lag_feature_names)\n",
    "\n",
    "# Count after\n",
    "train_after = df_train_clean.count()\n",
    "val_after = df_val_clean.count()\n",
    "test_after = df_test_clean.count()\n",
    "\n",
    "# Cache cleaned datasets\n",
    "df_train_clean = df_train_clean.cache()\n",
    "df_val_clean = df_val_clean.cache()\n",
    "df_test_clean = df_test_clean.cache()\n",
    "\n",
    "# Unpersist old ones\n",
    "df_train.unpersist()\n",
    "df_val.unpersist()\n",
    "df_test.unpersist()\n",
    "\n",
    "# Reassign\n",
    "df_train = df_train_clean\n",
    "df_val = df_val_clean\n",
    "df_test = df_test_clean\n",
    "\n",
    "print(f\"\\n[DATA] Records dropped (null lag features):\")\n",
    "print(f\"  [?] Train: {train_before:,} -> {train_after:,} (dropped {train_before - train_after:,}, {(train_before - train_after)/train_before*100:.2f}%)\")\n",
    "print(f\"  [?] Val:   {val_before:,} -> {val_after:,} (dropped {val_before - val_after:,}, {(val_before - val_after)/val_before*100:.2f}%)\")\n",
    "print(f\"  [?] Test:  {test_before:,} -> {test_after:,} (dropped {test_before - test_after:,}, {(test_before - test_after)/test_before*100:.2f}%)\")\n",
    "\n",
    "# Verify no nulls\n",
    "print(f\"\\n[SUCCESS] Verification - checking for remaining nulls...\")\n",
    "sample_check = lag_feature_names[:3]\n",
    "total_nulls_after = 0\n",
    "for lag_col in sample_check:\n",
    "    if lag_col in df_train.columns:\n",
    "        null_count = df_train.filter(F.col(lag_col).isNull()).count()\n",
    "        total_nulls_after += null_count\n",
    "        status = \"[SUCCESS]\" if null_count == 0 else \"[ERROR]\"\n",
    "        print(f\"  {lag_col:35s}: {null_count:8,} nulls {status}\")\n",
    "\n",
    "if total_nulls_after == 0:\n",
    "    print(f\"\\n[SUCCESS] All lag features are clean!\")\n",
    "else:\n",
    "    print(f\"\\n[WARNING]  Still {total_nulls_after} nulls found!\")\n",
    "\n",
    "print(f\"\\n[SUCCESS] Lag features + Null handling completed!\")\n",
    "print(f\"   - Created {len(lag_feature_names)} lag features FROM SCALED columns\")\n",
    "print(f\"   - Lost only first {max(LAG_STEPS)} hours per location\")\n",
    "print(f\"   - All lag features now have valid values\")\n",
    "print(f\"   - Data quality ensured for XGBoost training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b991028",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T05:52:18.058348Z",
     "iopub.status.busy": "2025-11-09T05:52:18.058071Z",
     "iopub.status.idle": "2025-11-09T05:52:18.071606Z",
     "shell.execute_reply": "2025-11-09T05:52:18.070302Z"
    },
    "papermill": {
     "duration": 0.041692,
     "end_time": "2025-11-09T05:52:18.073296",
     "exception": false,
     "start_time": "2025-11-09T05:52:18.031604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PROCESSING] Step 6: Preparing Model-Specific Features...\n",
      "[MODEL] DEEP LEARNING Features: 15 features\n",
      "   - Current pollutants (scaled): 3\n",
      "   - Weather (scaled): 5\n",
      "   - Time (cyclic): 6\n",
      "   - Time (binary): 1\n",
      "   - NO LAG FEATURES (models learn from sequences)\n",
      "\n",
      "[DATA] XGBOOST Features: 63 features\n",
      "   - Deep Learning base features: 15\n",
      "   - Lag features (from scaled columns): 48\n",
      "   - Total: 63 features\n",
      "\n",
      "[SUCCESS] Model-specific features prepared:\n",
      "  [MODEL] CNN1D-BLSTM-Attention: 15 features\n",
      "  [MODEL] LSTM: 15 features\n",
      "  [DATA] XGBoost: 63 features\n",
      "  [TARGET] Target: PM2_5_scaled\n",
      "\n",
      "[SUCCESS] All feature columns exist in datasets!\n"
     ]
    }
   ],
   "source": [
    "# Bước 6: Chuẩn bị Features cho từng Model\n",
    "print(\"\\n[PROCESSING] Step 6: Preparing Model-Specific Features...\")\n",
    "\n",
    "# ========================================\n",
    "# FEATURES CHO DEEP LEARNING MODELS (CNN1D-BLSTM, LSTM)\n",
    "# ========================================\n",
    "# Không cần lag features vì models tự học temporal patterns từ sequences\n",
    "\n",
    "dl_input_features = []\n",
    "\n",
    "# 1. Pollutants scaled (trừ PM2_5 - đây là target)\n",
    "dl_input_features.extend([\"PM10_scaled\", \"NO2_scaled\", \"SO2_scaled\"])\n",
    "\n",
    "# 2. Weather features scaled (core features)\n",
    "dl_input_features.extend([\n",
    "    \"temperature_2m_scaled\", \"relative_humidity_2m_scaled\",\n",
    "    \"wind_speed_10m_scaled\", \"surface_pressure_scaled\", \"precipitation_scaled\"  # ✅ Added surface_pressure\n",
    "])\n",
    "\n",
    "# 3. Time features (cyclic encoding - đã ở dạng sin/cos trong [-1, 1])\n",
    "dl_input_features.extend([\n",
    "    \"hour_sin\", \"hour_cos\", \n",
    "    \"month_sin\", \"month_cos\",\n",
    "    \"day_of_week_sin\", \"day_of_week_cos\",\n",
    "    \"wind_direction_sin\", \"wind_direction_cos\"\n",
    "])\n",
    "\n",
    "# 4. Time features (binary)\n",
    "dl_input_features.extend([\"is_weekend\"])\n",
    "\n",
    "print(f\"[MODEL] DEEP LEARNING Features: {len(dl_input_features)} features\")\n",
    "print(f\"   - Current pollutants (scaled): 3\")\n",
    "print(f\"   - Weather (scaled): 5\") \n",
    "print(f\"   - Time (cyclic): 6\")\n",
    "print(f\"   - Time (binary): 1\")\n",
    "print(f\"   - NO LAG FEATURES (models learn from sequences)\")\n",
    "\n",
    "# ========================================  \n",
    "# FEATURES CHO XGBOOST\n",
    "# ========================================\n",
    "# Cần lag features vì không có khả năng xử lý sequences\n",
    "\n",
    "xgb_input_features = dl_input_features.copy()  # Start with DL features\n",
    "\n",
    "# Thêm lag features CHỈ CHO XGBOOST (đã được tạo từ scaled columns)\n",
    "for col_name in lag_base_columns:\n",
    "    for lag in LAG_STEPS:\n",
    "        lag_col_name = f\"{col_name}_lag{lag}_scaled\"\n",
    "        xgb_input_features.append(lag_col_name)\n",
    "\n",
    "print(f\"\\n[DATA] XGBOOST Features: {len(xgb_input_features)} features\")\n",
    "print(f\"   - Deep Learning base features: {len(dl_input_features)}\")\n",
    "print(f\"   - Lag features (from scaled columns): {len(lag_base_columns) * len(LAG_STEPS)}\")\n",
    "print(f\"   - Total: {len(xgb_input_features)} features\")\n",
    "\n",
    "# Target variable (đã scaled)\n",
    "target_feature = \"PM2_5_scaled\"\n",
    "\n",
    "print(f\"\\n[SUCCESS] Model-specific features prepared:\")\n",
    "print(f\"  [MODEL] CNN1D-BLSTM-Attention: {len(dl_input_features)} features\")\n",
    "print(f\"  [MODEL] LSTM: {len(dl_input_features)} features\")  \n",
    "print(f\"  [DATA] XGBoost: {len(xgb_input_features)} features\")\n",
    "print(f\"  [TARGET] Target: {target_feature}\")\n",
    "\n",
    "# [WARNING] CRITICAL: Verify ALL columns exist\n",
    "missing_dl = [col for col in dl_input_features if col not in df_train.columns]\n",
    "missing_xgb = [col for col in xgb_input_features if col not in df_train.columns]\n",
    "missing_target = target_feature not in df_train.columns\n",
    "\n",
    "if missing_dl or missing_xgb or missing_target:\n",
    "    print(f\"\\n[ERROR] MISSING COLUMNS DETECTED:\")\n",
    "    if missing_dl: \n",
    "        print(f\"  DL models: {missing_dl}\")\n",
    "    if missing_xgb: \n",
    "        print(f\"  XGBoost: {missing_xgb[:5]}...\")  # Show first 5\n",
    "    if missing_target:\n",
    "        print(f\"  Target: {target_feature}\")\n",
    "    \n",
    "    print(f\"\\n[WARNING]  Available scaled columns:\")\n",
    "    scaled_cols = [c for c in df_train.columns if c.endswith('_scaled')]\n",
    "    print(f\"  {scaled_cols[:10]}...\")\n",
    "    \n",
    "    raise ValueError(\"Missing required feature columns! Check normalization step.\")\n",
    "else:\n",
    "    print(f\"\\n[SUCCESS] All feature columns exist in datasets!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2d24bdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T05:52:18.126606Z",
     "iopub.status.busy": "2025-11-09T05:52:18.126190Z",
     "iopub.status.idle": "2025-11-09T05:52:31.471363Z",
     "shell.execute_reply": "2025-11-09T05:52:31.470380Z"
    },
    "papermill": {
     "duration": 13.375163,
     "end_time": "2025-11-09T05:52:31.473932",
     "exception": false,
     "start_time": "2025-11-09T05:52:18.098769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PROCESSING] Step 7: Preparing Final Model-Specific Datasets...\n",
      "\n",
      "[MODEL] Deep Learning datasets (no lag features):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] Train: 187,251 records, 15 features\n",
      "  [OK] Val:   49,620 records, 15 features\n",
      "  [OK] Test:  51,278 records, 15 features\n",
      "\n",
      "[DATA] XGBoost datasets (with lag features):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7811:=================================================>      (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] Train: 187,251 records, 63 features\n",
      "  [OK] Val:   49,620 records, 63 features\n",
      "  [OK] Test:  51,278 records, 63 features\n",
      "\n",
      "[SUCCESS] Final datasets prepared!\n",
      "   [MODEL] Deep Learning: 15 features (no lags)\n",
      "   [DATA] XGBoost: 63 features (with 48 lags)\n",
      "   [TARGET] Target: PM2_5_scaled\n",
      "   [SUCCESS] All datasets cleaned and ready for training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Bước 7: Prepare Final Model Datasets\n",
    "print(\"\\n[PROCESSING] Step 7: Preparing Final Model-Specific Datasets...\")\n",
    "\n",
    "# ========================================\n",
    "# DEEP LEARNING DATASETS (CNN1D-BLSTM & LSTM)\n",
    "# ========================================\n",
    "# Không cần lag features, chỉ cần base features + time features\n",
    "\n",
    "print(f\"\\n[MODEL] Deep Learning datasets (no lag features):\")\n",
    "\n",
    "# Select only DL features + target\n",
    "dl_train = df_train.select(\"location_id\", \"datetime\", target_feature, *dl_input_features)\n",
    "dl_val = df_val.select(\"location_id\", \"datetime\", target_feature, *dl_input_features)\n",
    "dl_test = df_test.select(\"location_id\", \"datetime\", target_feature, *dl_input_features)\n",
    "\n",
    "# Cache\n",
    "dl_train = dl_train.cache()\n",
    "dl_val = dl_val.cache()\n",
    "dl_test = dl_test.cache()\n",
    "\n",
    "dl_train_count = dl_train.count()\n",
    "dl_val_count = dl_val.count()\n",
    "dl_test_count = dl_test.count()\n",
    "\n",
    "print(f\"  [OK] Train: {dl_train_count:,} records, {len(dl_input_features)} features\")\n",
    "print(f\"  [OK] Val:   {dl_val_count:,} records, {len(dl_input_features)} features\")\n",
    "print(f\"  [OK] Test:  {dl_test_count:,} records, {len(dl_input_features)} features\")\n",
    "\n",
    "# ========================================\n",
    "# XGBOOST DATASETS\n",
    "# ========================================\n",
    "# Cần cả base features + lag features\n",
    "\n",
    "print(f\"\\n[DATA] XGBoost datasets (with lag features):\")\n",
    "\n",
    "# Select XGB features + target\n",
    "xgb_train = df_train.select(\"location_id\", \"datetime\", target_feature, *xgb_input_features)\n",
    "xgb_val = df_val.select(\"location_id\", \"datetime\", target_feature, *xgb_input_features)\n",
    "xgb_test = df_test.select(\"location_id\", \"datetime\", target_feature, *xgb_input_features)\n",
    "\n",
    "# Cache\n",
    "xgb_train = xgb_train.cache()\n",
    "xgb_val = xgb_val.cache()\n",
    "xgb_test = xgb_test.cache()\n",
    "\n",
    "xgb_train_count = xgb_train.count()\n",
    "xgb_val_count = xgb_val.count()\n",
    "xgb_test_count = xgb_test.count()\n",
    "\n",
    "print(f\"  [OK] Train: {xgb_train_count:,} records, {len(xgb_input_features)} features\")\n",
    "print(f\"  [OK] Val:   {xgb_val_count:,} records, {len(xgb_input_features)} features\")\n",
    "print(f\"  [OK] Test:  {xgb_test_count:,} records, {len(xgb_input_features)} features\")\n",
    "\n",
    "print(f\"\\n[SUCCESS] Final datasets prepared!\")\n",
    "print(f\"   [MODEL] Deep Learning: {len(dl_input_features)} features (no lags)\")\n",
    "print(f\"   [DATA] XGBoost: {len(xgb_input_features)} features (with {len(lag_base_columns) * len(LAG_STEPS)} lags)\")\n",
    "print(f\"   [TARGET] Target: {target_feature}\")\n",
    "print(f\"   [SUCCESS] All datasets cleaned and ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "362e688e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T05:52:31.531259Z",
     "iopub.status.busy": "2025-11-09T05:52:31.530921Z",
     "iopub.status.idle": "2025-11-09T05:52:32.808104Z",
     "shell.execute_reply": "2025-11-09T05:52:32.806960Z"
    },
    "papermill": {
     "duration": 1.308702,
     "end_time": "2025-11-09T05:52:32.811460",
     "exception": false,
     "start_time": "2025-11-09T05:52:31.502758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[DATA] FEATURE ENGINEERING PIPELINE SUMMARY\n",
      "================================================================================\n",
      "\n",
      "[SUCCESS] PIPELINE EXECUTION ORDER (Correct - No Data Leakage):\n",
      "   [1] Time Features -> Added cyclic (sin/cos) + is_weekend\n",
      "   [2] Temporal Split -> 70% train / 15% val / 15% test\n",
      "   [3] Normalization -> Min-Max [0,1] using TRAIN statistics ONLY\n",
      "   [4] Lag Features + Null Handling -> Created FROM SCALED columns, dropped nulls\n",
      "   [5] Scaler Params -> Saved for inference\n",
      "   [6] Model Features -> Prepared for Deep Learning & XGBoost\n",
      "   [7] Final Datasets -> Ready for training\n",
      "\n",
      "[DATA] DATASET STATISTICS:\n",
      "   Total records: 288,149\n",
      "   Total locations: 14\n",
      "   Time range: 2022-11-01 -> 2025-09-30\n",
      "\n",
      "[METADATA] FEATURE BREAKDOWN:\n",
      "   [MODEL] Deep Learning (CNN1D-BLSTM & LSTM): 15 features\n",
      "      ├─ Pollutants (scaled): 3 (PM10, NO2, SO2)\n",
      "      ├─ Weather (scaled): 5 (temp, humidity, wind, precipitation)\n",
      "      ├─ Time (cyclic): 6 (hour, month, day_of_week -> sin/cos)\n",
      "      └─ Time (binary): 1 (is_weekend)\n",
      "   \n",
      "   [DATA] XGBoost: 63 features\n",
      "      ├─ Deep Learning features: 15\n",
      "      └─ Lag features: 48 (8 vars × 6 lags)\n",
      "\n",
      "[TARGET] TARGET VARIABLE:\n",
      "   PM2_5_scaled (normalized PM2.5 in [0, 1])\n",
      "\n",
      "[SUCCESS] DATA QUALITY CHECKS:\n",
      "   [OK] No missing values in target\n",
      "   [OK] No missing values in features\n",
      "   [OK] No outliers (removed by WHO/EPA standards)\n",
      "   [OK] Proper temporal ordering\n",
      "   [OK] No data leakage (train/val/test temporally separated)\n",
      "   [OK] Correct scale relationship (lag from scaled columns)\n",
      "   [OK] No nulls in lag features (first 24h dropped)\n",
      "\n",
      "[SAVE] SAVED ARTIFACTS:\n",
      "   [FILES] scaler_params.json -> Min-Max parameters (train set only)\n",
      "   [FILES] feature_metadata.json -> Feature lists & configuration\n",
      "\n",
      "[RUN] READY FOR NEXT PHASE:\n",
      "   Variables in memory:\n",
      "   - Deep Learning: dl_train, dl_val, dl_test\n",
      "   - XGBoost: xgb_train, xgb_val, xgb_test\n",
      "   Next step: Sequence creation for Deep Learning models\n",
      "================================================================================\n",
      "\n",
      "[KAGGLE] Kaggle mode: Saving metadata to /kaggle/working/processed\n",
      "\n",
      "[SAVE] Feature metadata saved to: /kaggle/working/processed/feature_metadata.json\n",
      "   [SUCCESS] Pipeline version: 2.0 (refactored - no data leakage)\n",
      "   [SUCCESS] Contains: feature lists, lag config, split info, dataset counts\n"
     ]
    }
   ],
   "source": [
    "# Bước 8: Feature Engineering Summary + Metadata Saving\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[DATA] FEATURE ENGINEERING PIPELINE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[SUCCESS] PIPELINE EXECUTION ORDER (Correct - No Data Leakage):\")\n",
    "print(f\"   [1] Time Features -> Added cyclic (sin/cos) + is_weekend\")\n",
    "print(f\"   [2] Temporal Split -> 70% train / 15% val / 15% test\")\n",
    "print(f\"   [3] Normalization -> Min-Max [0,1] using TRAIN statistics ONLY\")\n",
    "print(f\"   [4] Lag Features + Null Handling -> Created FROM SCALED columns, dropped nulls\")\n",
    "print(f\"   [5] Scaler Params -> Saved for inference\")\n",
    "print(f\"   [6] Model Features -> Prepared for Deep Learning & XGBoost\")\n",
    "print(f\"   [7] Final Datasets -> Ready for training\")\n",
    "\n",
    "print(f\"\\n[DATA] DATASET STATISTICS:\")\n",
    "print(f\"   Total records: {dl_train_count + dl_val_count + dl_test_count:,}\")\n",
    "print(f\"   Total locations: {df_train.select('location_id').distinct().count()}\")\n",
    "print(f\"   Time range: {min_time.strftime('%Y-%m-%d')} -> {max_time.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(f\"\\n[METADATA] FEATURE BREAKDOWN:\")\n",
    "print(f\"   [MODEL] Deep Learning (CNN1D-BLSTM & LSTM): {len(dl_input_features)} features\")\n",
    "print(f\"      ├─ Pollutants (scaled): 3 (PM10, NO2, SO2)\")\n",
    "print(f\"      ├─ Weather (scaled): 5 (temp, humidity, wind, precipitation)\")\n",
    "print(f\"      ├─ Time (cyclic): 6 (hour, month, day_of_week -> sin/cos)\")\n",
    "print(f\"      └─ Time (binary): 1 (is_weekend)\")\n",
    "print(f\"   \")\n",
    "print(f\"   [DATA] XGBoost: {len(xgb_input_features)} features\")\n",
    "print(f\"      ├─ Deep Learning features: {len(dl_input_features)}\")\n",
    "print(f\"      └─ Lag features: {len(lag_base_columns) * len(LAG_STEPS)} ({len(lag_base_columns)} vars × {len(LAG_STEPS)} lags)\")\n",
    "\n",
    "print(f\"\\n[TARGET] TARGET VARIABLE:\")\n",
    "print(f\"   {target_feature} (normalized PM2.5 in [0, 1])\")\n",
    "\n",
    "print(f\"\\n[SUCCESS] DATA QUALITY CHECKS:\")\n",
    "print(f\"   [OK] No missing values in target\")\n",
    "print(f\"   [OK] No missing values in features\")\n",
    "print(f\"   [OK] No outliers (removed by WHO/EPA standards)\")\n",
    "print(f\"   [OK] Proper temporal ordering\")\n",
    "print(f\"   [OK] No data leakage (train/val/test temporally separated)\")\n",
    "print(f\"   [OK] Correct scale relationship (lag from scaled columns)\")\n",
    "print(f\"   [OK] No nulls in lag features (first {max(LAG_STEPS)}h dropped)\")\n",
    "\n",
    "print(f\"\\n[SAVE] SAVED ARTIFACTS:\")\n",
    "print(f\"   [FILES] scaler_params.json -> Min-Max parameters (train set only)\")\n",
    "print(f\"   [FILES] feature_metadata.json -> Feature lists & configuration\")\n",
    "\n",
    "print(f\"\\n[RUN] READY FOR NEXT PHASE:\")\n",
    "print(f\"   Variables in memory:\")\n",
    "print(f\"   - Deep Learning: dl_train, dl_val, dl_test\")\n",
    "print(f\"   - XGBoost: xgb_train, xgb_val, xgb_test\")\n",
    "print(f\"   Next step: Sequence creation for Deep Learning models\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========================================\n",
    "# SAVE FEATURE METADATA\n",
    "# ========================================\n",
    "# Lưu metadata về feature engineering để tham khảo trong tương lai\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Metadata cho feature engineering\n",
    "dataset_metadata = {\n",
    "    \"project\": \"PM2.5 Prediction\",\n",
    "    \"preprocessing_version\": \"2.0_refactored\",\n",
    "    \"pipeline_order\": [\n",
    "        \"Time Features (cyclic encoding)\",\n",
    "        \"Temporal Split (70/15/15)\",\n",
    "        \"Normalization (train stats only)\",\n",
    "        \"Lag Features (from scaled columns)\",\n",
    "        \"Null Handling (drop first 24h per location)\"\n",
    "    ],\n",
    "    \"deep_learning_features\": dl_input_features,\n",
    "    \"xgboost_features\": xgb_input_features,\n",
    "    \"target_feature\": target_feature,\n",
    "    \"lag_config\": {\n",
    "        \"lag_steps\": LAG_STEPS,\n",
    "        \"lag_base_columns\": lag_base_columns,\n",
    "        \"total_lag_features\": len(lag_base_columns) * len(LAG_STEPS)\n",
    "    },\n",
    "    \"temporal_split\": {\n",
    "        \"train_end\": train_end.isoformat(),\n",
    "        \"val_end\": val_end.isoformat(),\n",
    "        \"min_time\": min_time.isoformat(),\n",
    "        \"max_time\": max_time.isoformat()\n",
    "    },\n",
    "    \"dataset_counts\": {\n",
    "        \"dl_train\": dl_train_count,\n",
    "        \"dl_val\": dl_val_count,\n",
    "        \"dl_test\": dl_test_count,\n",
    "        \"xgb_train\": xgb_train_count,\n",
    "        \"xgb_val\": xgb_val_count,\n",
    "        \"xgb_test\": xgb_test_count\n",
    "    },\n",
    "    \"total_features\": {\n",
    "        \"deep_learning\": len(dl_input_features),\n",
    "        \"xgboost\": len(xgb_input_features)\n",
    "    }\n",
    "}\n",
    "\n",
    "# ========================================\n",
    "# ADAPTIVE PATH (Kaggle vs Colab vs Local)\n",
    "# ========================================\n",
    "if IN_KAGGLE:\n",
    "    # [KAGGLE] Kaggle: Write to /kaggle/working (auto-saved on commit)\n",
    "    processed_dir = Path(\"/kaggle/working/processed\")\n",
    "    print(f\"\\n[KAGGLE] Kaggle mode: Saving metadata to {processed_dir}\")\n",
    "    \n",
    "elif IN_COLAB:\n",
    "    # [COLAB] Colab: Write to Google Drive\n",
    "    processed_dir = Path(\"/content/drive/MyDrive/pm25-data/processed\")\n",
    "    print(f\"\\n[COLAB] Colab mode: Saving metadata to Google Drive\")\n",
    "    \n",
    "else:\n",
    "    # [LOCAL] Local: Write to project folder\n",
    "    processed_dir = Path(\"../data/processed\")\n",
    "    print(f\"\\n[LOCAL] Local mode: Saving metadata to {processed_dir}\")\n",
    "\n",
    "# Tạo thư mục với parents=True (tạo cả parent directories nếu chưa có)\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Lưu metadata\n",
    "metadata_path = processed_dir / \"feature_metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(dataset_metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\n[SAVE] Feature metadata saved to: {metadata_path}\")\n",
    "print(f\"   [SUCCESS] Pipeline version: 2.0 (refactored - no data leakage)\")\n",
    "print(f\"   [SUCCESS] Contains: feature lists, lag config, split info, dataset counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10c908be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T05:52:32.926262Z",
     "iopub.status.busy": "2025-11-09T05:52:32.924575Z",
     "iopub.status.idle": "2025-11-09T05:57:56.222517Z",
     "shell.execute_reply": "2025-11-09T05:57:56.221332Z"
    },
    "papermill": {
     "duration": 323.357972,
     "end_time": "2025-11-09T05:57:56.224163",
     "exception": false,
     "start_time": "2025-11-09T05:52:32.866191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PROCESSING] Step 9: Creating Sequence Data for Deep Learning Models...\n",
      "[GEAR]  Sequence Configuration:\n",
      "   - CNN1D-BLSTM-Attention: 48 timesteps\n",
      "   - LSTM: 24 timesteps\n",
      "\n",
      "[DATA] Creating sequences for each model...\n",
      "\n",
      "[MODEL] CNN1D-BLSTM-Attention (48 timesteps):\n",
      "    Creating 48-step sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [?]  Layer 1: Dropped first 48 records/location\n",
      "         Records: 186,579\n",
      "        [INSTALL] Processing 4 batches (15 features)...\n",
      "           Batch 1/4: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 2/4: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 3/4: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 4/4: 3 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [?] Filtering null sequences...\n",
      "      [SUCCESS] Layer 2: No data gaps detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/09 05:53:52 WARN DAGScheduler: Broadcasting large task binary with size 1398.3 KiB\n",
      "25/11/09 05:53:59 WARN DAGScheduler: Broadcasting large task binary with size 1403.6 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [SUCCESS] Final: 186,579 records (100.0% retained)\n",
      "    Creating 48-step sequences...\n",
      "      [?]  Layer 1: Dropped first 48 records/location\n",
      "         Records: 48,948\n",
      "        [INSTALL] Processing 4 batches (15 features)...\n",
      "           Batch 1/4: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 2/4: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 3/4: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 4/4: 3 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [?] Filtering null sequences...\n",
      "      [SUCCESS] Layer 2: No data gaps detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [SUCCESS] Final: 48,948 records (100.0% retained)\n",
      "    Creating 48-step sequences...\n",
      "      [?]  Layer 1: Dropped first 48 records/location\n",
      "         Records: 50,606\n",
      "        [INSTALL] Processing 4 batches (15 features)...\n",
      "           Batch 1/4: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 2/4: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 3/4: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 4/4: 3 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [?] Filtering null sequences...\n",
      "      [SUCCESS] Layer 2: No data gaps detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [SUCCESS] Final: 50,606 records (100.0% retained)\n",
      "    [SUCCESS] CNN sequences created successfully\n",
      "\n",
      "[PROCESSING] LSTM (24 timesteps):\n",
      "    Creating 24-step sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [?]  Layer 1: Dropped first 24 records/location\n",
      "         Records: 186,915\n",
      "        [INSTALL] Processing 4 batches (15 features)...\n",
      "           Batch 1/4: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 2/4: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 3/4: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 4/4: 3 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [?] Filtering null sequences...\n",
      "      [SUCCESS] Layer 2: No data gaps detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/09 05:56:36 WARN DAGScheduler: Broadcasting large task binary with size 1095.5 KiB\n",
      "25/11/09 05:56:41 WARN DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [SUCCESS] Final: 186,915 records (100.0% retained)\n",
      "    Creating 24-step sequences...\n",
      "      [?]  Layer 1: Dropped first 24 records/location\n",
      "         Records: 49,284\n",
      "        [INSTALL] Processing 4 batches (15 features)...\n",
      "           Batch 1/4: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 2/4: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 3/4: 4 features\n",
      "           Batch 4/4: 3 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [?] Filtering null sequences...\n",
      "      [SUCCESS] Layer 2: No data gaps detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [SUCCESS] Final: 49,284 records (100.0% retained)\n",
      "    Creating 24-step sequences...\n",
      "      [?]  Layer 1: Dropped first 24 records/location\n",
      "         Records: 50,942\n",
      "        [INSTALL] Processing 4 batches (15 features)...\n",
      "           Batch 1/4: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 2/4: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 3/4: 4 features\n",
      "           Batch 4/4: 3 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [?] Filtering null sequences...\n",
      "      [SUCCESS] Layer 2: No data gaps detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12526:=========================================>             (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [SUCCESS] Final: 50,942 records (100.0% retained)\n",
      "    [SUCCESS] LSTM sequences created successfully\n",
      "\n",
      "[SUCCESS] Sequence data preparation completed!\n",
      "\n",
      "[METADATA] Data Quality Guarantee:\n",
      "   [OK] Layer 1: No incomplete history (first 48/24 records dropped)\n",
      "   [OK] Layer 2: No data gaps in middle (nulls filtered out)\n",
      "   [OK] Result: 100% clean sequences with ZERO nulls\n",
      "   [OK] Ready for high-quality model training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Bước 9: Create Sequence Data for Deep Learning Models\n",
    "print(\"\\n[PROCESSING] Step 9: Creating Sequence Data for Deep Learning Models...\")\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "# Sequence configuration (optimized for Colab)\n",
    "CNN_SEQUENCE_LENGTH = 48  # Optimal for long-term patterns\n",
    "LSTM_SEQUENCE_LENGTH = 24  # Optimal for medium-term patterns\n",
    "\n",
    "print(f\"[GEAR]  Sequence Configuration:\")\n",
    "print(f\"   - CNN1D-BLSTM-Attention: {CNN_SEQUENCE_LENGTH} timesteps\")\n",
    "print(f\"   - LSTM: {LSTM_SEQUENCE_LENGTH} timesteps\")\n",
    "\n",
    "def create_sequences_optimized(df, feature_cols, target_col, sequence_length):\n",
    "    \"\"\"\n",
    "    Optimized sequence creation with checkpointing to avoid StackOverflow\n",
    "    \n",
    "    [TARGET] Key Strategy:\n",
    "    - Batch processing to avoid deep logical plans\n",
    "    - Checkpoint after each batch to reset plan depth\n",
    "    - Use broadcast joins for efficiency\n",
    "    - Single final filter for null handling\n",
    "    \n",
    "    [?] Null Handling (2-Layer Protection):\n",
    "    Layer 1: Drop first N records/location (incomplete history)\n",
    "    Layer 2: Filter ANY null in sequences (data gaps)\n",
    "    Result: 100% clean sequences with ZERO nulls\n",
    "    \"\"\"\n",
    "    print(f\"    Creating {sequence_length}-step sequences...\")\n",
    "    \n",
    "    window_spec = Window.partitionBy(\"location_id\").orderBy(\"datetime\")\n",
    "    \n",
    "    # ========================================\n",
    "    # LAYER 1: Drop first N records (incomplete history)\n",
    "    # ========================================\n",
    "    df_base = df.select(\"location_id\", \"datetime\", target_col, *feature_cols) \\\n",
    "                .repartition(4, \"location_id\") \\\n",
    "                .withColumn(\"row_num\", F.row_number().over(window_spec)) \\\n",
    "                .filter(F.col(\"row_num\") > sequence_length) \\\n",
    "                .drop(\"row_num\") \\\n",
    "                .cache()\n",
    "    \n",
    "    records_after_layer1 = df_base.count()  # Materialize\n",
    "    print(f\"      [?]  Layer 1: Dropped first {sequence_length} records/location\")\n",
    "    print(f\"         Records: {records_after_layer1:,}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # BATCH PROCESSING (避免 StackOverflow)\n",
    "    # ========================================\n",
    "    # Chia features thành batches nhỏ để tránh logical plan quá sâu\n",
    "    BATCH_SIZE = 4  # Mỗi batch xử lý 4 features (4 × 48 lags = 192 ops - safe)\n",
    "    feature_batches = [feature_cols[i:i+BATCH_SIZE] for i in range(0, len(feature_cols), BATCH_SIZE)]\n",
    "    \n",
    "    print(f\"        [INSTALL] Processing {len(feature_batches)} batches ({len(feature_cols)} features)...\")\n",
    "    \n",
    "    base_cols = [\"location_id\", \"datetime\"]\n",
    "    result_df = df_base.select(*base_cols)\n",
    "    \n",
    "    for batch_idx, batch_features in enumerate(feature_batches, 1):\n",
    "        print(f\"           Batch {batch_idx}/{len(feature_batches)}: {len(batch_features)} features\")\n",
    "        \n",
    "        # Tạo batch DataFrame\n",
    "        batch_df = df_base.select(*base_cols, *batch_features)\n",
    "        \n",
    "        # Tạo sequences cho batch này\n",
    "        for col_name in batch_features:\n",
    "            # Tạo array of lags [t-1, t-2, ..., t-N]\n",
    "            lag_exprs = [F.lag(col_name, step).over(window_spec) for step in range(1, sequence_length + 1)]\n",
    "            batch_df = batch_df.withColumn(f\"{col_name}_sequence\", F.array(*lag_exprs))\n",
    "\n",
    "            lag_exprs = [F.lag(col_name, step).over(window_spec)\n",
    "             for step in range(sequence_length, 0, -1)]  # ✅ Đảo ngược: N -> 1\n",
    "            batch_df = batch_df.withColumn(f\"{col_name}_sequence\", F.array(*lag_exprs))\n",
    "        \n",
    "        # Select chỉ sequence columns\n",
    "        sequence_cols = [f\"{col}_sequence\" for col in batch_features]\n",
    "        batch_df = batch_df.select(*base_cols, *sequence_cols).cache()\n",
    "        batch_df.count()  # Materialize để reset logical plan\n",
    "        \n",
    "        # Join vào result\n",
    "        result_df = result_df.join(batch_df, base_cols, \"inner\")\n",
    "        \n",
    "        # Unpersist batch (giải phóng memory)\n",
    "        batch_df.unpersist()\n",
    "    \n",
    "    # ========================================\n",
    "    # LAYER 2: Filter nulls in sequences\n",
    "    # ========================================\n",
    "    print(f\"        [?] Filtering null sequences...\")\n",
    "    \n",
    "    all_sequence_cols = [f\"{col}_sequence\" for col in feature_cols]\n",
    "    \n",
    "    # Build null filter: ALL sequences must be NOT NULL\n",
    "    from functools import reduce\n",
    "    null_filter = reduce(\n",
    "        lambda acc, col: acc & F.col(col).isNotNull(),\n",
    "        all_sequence_cols,\n",
    "        F.lit(True)\n",
    "    )\n",
    "    \n",
    "    # Also check: NO null VALUES inside arrays (extra safety)\n",
    "    # Trick: size(array) should equal sequence_length (nulls make size smaller)\n",
    "    for seq_col in all_sequence_cols:\n",
    "        null_filter = null_filter & (F.size(seq_col) == sequence_length)\n",
    "    \n",
    "    result_df = result_df.filter(null_filter)\n",
    "    records_after_layer2 = result_df.count()\n",
    "    dropped = records_after_layer1 - records_after_layer2\n",
    "    \n",
    "    if dropped > 0:\n",
    "        print(f\"      [?]  Layer 2: Dropped {dropped:,} records with nulls\")\n",
    "        print(f\"         Records: {records_after_layer1:,} -> {records_after_layer2:,}\")\n",
    "    else:\n",
    "        print(f\"      [SUCCESS] Layer 2: No data gaps detected\")\n",
    "    \n",
    "    # ========================================\n",
    "    # FINAL: Add target and clean up\n",
    "    # ========================================\n",
    "    result_df = result_df.join(\n",
    "        df_base.select(\"location_id\", \"datetime\", target_col),\n",
    "        [\"location_id\", \"datetime\"],\n",
    "        \"inner\"\n",
    "    ).filter(F.col(target_col).isNotNull()) \\\n",
    "     .withColumnRenamed(target_col, \"target_value\") \\\n",
    "     .cache()\n",
    "    \n",
    "    final_count = result_df.count()\n",
    "    retention_rate = (final_count / records_after_layer1) * 100\n",
    "    \n",
    "    print(f\"      [SUCCESS] Final: {final_count:,} records ({retention_rate:.1f}% retained)\")\n",
    "    \n",
    "    # Cleanup\n",
    "    df_base.unpersist()\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "print(\"\\n[DATA] Creating sequences for each model...\")\n",
    "\n",
    "# Create CNN1D-BLSTM sequences\n",
    "print(f\"\\n[MODEL] CNN1D-BLSTM-Attention ({CNN_SEQUENCE_LENGTH} timesteps):\")\n",
    "try:\n",
    "    cnn_train_clean = create_sequences_optimized(dl_train, dl_input_features, target_feature, CNN_SEQUENCE_LENGTH)\n",
    "    cnn_val_clean = create_sequences_optimized(dl_val, dl_input_features, target_feature, CNN_SEQUENCE_LENGTH)\n",
    "    cnn_test_clean = create_sequences_optimized(dl_test, dl_input_features, target_feature, CNN_SEQUENCE_LENGTH)\n",
    "    print(f\"    [SUCCESS] CNN sequences created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"    [ERROR] CNN sequence creation failed: {str(e)[:100]}...\")\n",
    "    cnn_train_clean = cnn_val_clean = cnn_test_clean = None\n",
    "\n",
    "# Create LSTM sequences  \n",
    "print(f\"\\n[PROCESSING] LSTM ({LSTM_SEQUENCE_LENGTH} timesteps):\")\n",
    "try:\n",
    "    lstm_train_clean = create_sequences_optimized(dl_train, dl_input_features, target_feature, LSTM_SEQUENCE_LENGTH)\n",
    "    lstm_val_clean = create_sequences_optimized(dl_val, dl_input_features, target_feature, LSTM_SEQUENCE_LENGTH)\n",
    "    lstm_test_clean = create_sequences_optimized(dl_test, dl_input_features, target_feature, LSTM_SEQUENCE_LENGTH)\n",
    "    print(f\"    [SUCCESS] LSTM sequences created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"    [ERROR] LSTM sequence creation failed: {str(e)[:100]}...\")\n",
    "    lstm_train_clean = lstm_val_clean = lstm_test_clean = None\n",
    "\n",
    "print(f\"\\n[SUCCESS] Sequence data preparation completed!\")\n",
    "print(f\"\\n[METADATA] Data Quality Guarantee:\")\n",
    "print(f\"   [OK] Layer 1: No incomplete history (first {CNN_SEQUENCE_LENGTH}/{LSTM_SEQUENCE_LENGTH} records dropped)\")\n",
    "print(f\"   [OK] Layer 2: No data gaps in middle (nulls filtered out)\")\n",
    "print(f\"   [OK] Result: 100% clean sequences with ZERO nulls\")\n",
    "print(f\"   [OK] Ready for high-quality model training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6eba575d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T05:57:56.299025Z",
     "iopub.status.busy": "2025-11-09T05:57:56.298599Z",
     "iopub.status.idle": "2025-11-09T05:59:03.552603Z",
     "shell.execute_reply": "2025-11-09T05:59:03.551492Z"
    },
    "papermill": {
     "duration": 67.294335,
     "end_time": "2025-11-09T05:59:03.554387",
     "exception": false,
     "start_time": "2025-11-09T05:57:56.260052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INSTALL] Step 10: Exporting Final Datasets to Disk...\n",
      "[KAGGLE] Kaggle mode: Saving to /kaggle/working/processed\n",
      "   [WARNING]  Files will be auto-saved when you commit notebook\n",
      "\n",
      "[DATA] Dataset Status:\n",
      "  CNN1D-BLSTM: [SUCCESS] Ready\n",
      "  LSTM: [SUCCESS] Ready\n",
      "  XGBoost: [SUCCESS] Ready\n",
      "\n",
      "[SAVE] Exporting datasets to Parquet format...\n",
      "\n",
      "  [MODEL] Exporting CNN1D-BLSTM datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/09 05:57:59 WARN DAGScheduler: Broadcasting large task binary with size 1605.5 KiB\n",
      "25/11/09 05:58:22 WARN DAGScheduler: Broadcasting large task binary with size 1403.6 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     [SUCCESS] Saved to: /kaggle/working/processed/cnn_sequences/\n",
      "        - train: 186,579 records\n",
      "        - val:   48,948 records\n",
      "        - test:  50,606 records\n",
      "\n",
      "  [PROCESSING] Exporting LSTM datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/09 05:58:32 WARN DAGScheduler: Broadcasting large task binary with size 1302.8 KiB\n",
      "25/11/09 05:58:48 WARN DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     [SUCCESS] Saved to: /kaggle/working/processed/lstm_sequences/\n",
      "        - train: 186,915 records\n",
      "        - val:   49,284 records\n",
      "        - test:  50,942 records\n",
      "\n",
      "  [DATA] Exporting XGBoost datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     [SUCCESS] Saved to: /kaggle/working/processed/xgboost/\n",
      "        - train: 187,251 records\n",
      "        - val:   49,620 records\n",
      "        - test:  51,278 records\n",
      "\n",
      "[SAVE] Saving metadata...\n",
      "   [SUCCESS] Metadata saved to: /kaggle/working/processed/datasets_ready.json\n",
      "   [SUCCESS] Scaler params saved to: /kaggle/working/processed/scaler_params.json\n",
      "   [SUCCESS] Feature metadata saved to: /kaggle/working/processed/feature_metadata.json\n",
      "\n",
      "================================================================================\n",
      "[SUCCESS] DATA PREPROCESSING & EXPORT COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "[KAGGLE] KAGGLE OUTPUT:\n",
      "   [?] Location: /kaggle/working/processed/\n",
      "   [?] To save permanently:\n",
      "      1. Click 'Save Version' (top right)\n",
      "      2. Choose 'Save & Run All' (recommended)\n",
      "      3. Wait for completion (~20-30 min)\n",
      "      4. Output will appear in 'Output' tab\n",
      "      5. Use as dataset: '+ Add Data' -> Your Output\n",
      "\n",
      "[?] Exported Directory Structure:\n",
      "   /kaggle/working/processed/\n",
      "   ├── cnn_sequences/\n",
      "   │   ├── train/  (186,579 records)\n",
      "   │   ├── val/    (48,948 records)\n",
      "   │   └── test/   (50,606 records)\n",
      "   ├── lstm_sequences/\n",
      "   │   ├── train/  (186,915 records)\n",
      "   │   ├── val/    (49,284 records)\n",
      "   │   └── test/   (50,942 records)\n",
      "   ├── xgboost/\n",
      "   │   ├── train/  (187,251 records)\n",
      "   │   ├── val/    (49,620 records)\n",
      "   │   └── test/   (51,278 records)\n",
      "   ├── scaler_params.json\n",
      "   ├── feature_metadata.json\n",
      "   └── datasets_ready.json\n",
      "\n",
      "[DATA] Total Dataset Sizes:\n",
      "   - CNN1D-BLSTM: 286,133 records (48 timesteps, 15 features)\n",
      "   - LSTM:        287,141 records (24 timesteps, 15 features)\n",
      "   - XGBoost:     288,149 records (63 features)\n",
      "\n",
      "[RUN] Ready for Model Training Phase!\n",
      "   [?] Next: Create new notebook, add this output as dataset\n",
      "   [?] Load: spark.read.parquet('/kaggle/input/<output-name>/processed/...')\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Bước 10: Export Final Datasets to Disk\n",
    "print(\"\\n[INSTALL] Step 10: Exporting Final Datasets to Disk...\")\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ========================================\n",
    "# ADAPTIVE OUTPUT PATH (Kaggle vs Colab vs Local)\n",
    "# ========================================\n",
    "if IN_KAGGLE:\n",
    "    # [KAGGLE] Kaggle: Write to /kaggle/working (auto-saved on commit)\n",
    "    processed_dir = Path(\"/kaggle/working/processed\")\n",
    "    print(f\"[KAGGLE] Kaggle mode: Saving to {processed_dir}\")\n",
    "    print(f\"   [WARNING]  Files will be auto-saved when you commit notebook\")\n",
    "    \n",
    "elif IN_COLAB:\n",
    "    # [COLAB] Colab: Write to Google Drive\n",
    "    processed_dir = Path(\"/content/drive/MyDrive/pm25-data/processed\")\n",
    "    print(f\"[COLAB] Colab mode: Saving to Google Drive\")\n",
    "    \n",
    "else:\n",
    "    # [LOCAL] Local: Write to project folder\n",
    "    processed_dir = Path(\"../data/processed\")\n",
    "    print(f\"[LOCAL] Local mode: Saving to {processed_dir}\")\n",
    "\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check dataset availability\n",
    "datasets_ready = {\n",
    "    \"cnn\": cnn_train_clean is not None and cnn_val_clean is not None and cnn_test_clean is not None,\n",
    "    \"lstm\": lstm_train_clean is not None and lstm_val_clean is not None and lstm_test_clean is not None,\n",
    "    \"xgb\": xgb_train is not None and xgb_val is not None and xgb_test is not None\n",
    "}\n",
    "\n",
    "print(f\"\\n[DATA] Dataset Status:\")\n",
    "for model, ready in datasets_ready.items():\n",
    "    model_name = {\"cnn\": \"CNN1D-BLSTM\", \"lstm\": \"LSTM\", \"xgb\": \"XGBoost\"}[model]\n",
    "    status = \"[SUCCESS] Ready\" if ready else \"[ERROR] Not Ready\"\n",
    "    print(f\"  {model_name}: {status}\")\n",
    "\n",
    "# ========================================\n",
    "# EXPORT DATASETS TO PARQUET\n",
    "# ========================================\n",
    "print(f\"\\n[SAVE] Exporting datasets to Parquet format...\")\n",
    "\n",
    "export_summary = {\n",
    "    \"cnn\": {\"train\": 0, \"val\": 0, \"test\": 0},\n",
    "    \"lstm\": {\"train\": 0, \"val\": 0, \"test\": 0},\n",
    "    \"xgb\": {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "}\n",
    "\n",
    "# Export CNN1D-BLSTM datasets\n",
    "if datasets_ready[\"cnn\"]:\n",
    "    print(f\"\\n  [MODEL] Exporting CNN1D-BLSTM datasets...\")\n",
    "    cnn_dir = processed_dir / \"cnn_sequences\"\n",
    "    cnn_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    cnn_train_clean.write.mode(\"overwrite\").parquet(str(cnn_dir / \"train\"))\n",
    "    cnn_val_clean.write.mode(\"overwrite\").parquet(str(cnn_dir / \"val\"))\n",
    "    cnn_test_clean.write.mode(\"overwrite\").parquet(str(cnn_dir / \"test\"))\n",
    "    \n",
    "    export_summary[\"cnn\"][\"train\"] = cnn_train_clean.count()\n",
    "    export_summary[\"cnn\"][\"val\"] = cnn_val_clean.count()\n",
    "    export_summary[\"cnn\"][\"test\"] = cnn_test_clean.count()\n",
    "    \n",
    "    print(f\"     [SUCCESS] Saved to: {cnn_dir}/\")\n",
    "    print(f\"        - train: {export_summary['cnn']['train']:,} records\")\n",
    "    print(f\"        - val:   {export_summary['cnn']['val']:,} records\")\n",
    "    print(f\"        - test:  {export_summary['cnn']['test']:,} records\")\n",
    "\n",
    "# Export LSTM datasets\n",
    "if datasets_ready[\"lstm\"]:\n",
    "    print(f\"\\n  [PROCESSING] Exporting LSTM datasets...\")\n",
    "    lstm_dir = processed_dir / \"lstm_sequences\"\n",
    "    lstm_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    lstm_train_clean.write.mode(\"overwrite\").parquet(str(lstm_dir / \"train\"))\n",
    "    lstm_val_clean.write.mode(\"overwrite\").parquet(str(lstm_dir / \"val\"))\n",
    "    lstm_test_clean.write.mode(\"overwrite\").parquet(str(lstm_dir / \"test\"))\n",
    "    \n",
    "    export_summary[\"lstm\"][\"train\"] = lstm_train_clean.count()\n",
    "    export_summary[\"lstm\"][\"val\"] = lstm_val_clean.count()\n",
    "    export_summary[\"lstm\"][\"test\"] = lstm_test_clean.count()\n",
    "    \n",
    "    print(f\"     [SUCCESS] Saved to: {lstm_dir}/\")\n",
    "    print(f\"        - train: {export_summary['lstm']['train']:,} records\")\n",
    "    print(f\"        - val:   {export_summary['lstm']['val']:,} records\")\n",
    "    print(f\"        - test:  {export_summary['lstm']['test']:,} records\")\n",
    "\n",
    "# Export XGBoost datasets\n",
    "if datasets_ready[\"xgb\"]:\n",
    "    print(f\"\\n  [DATA] Exporting XGBoost datasets...\")\n",
    "    xgb_dir = processed_dir / \"xgboost\"\n",
    "    xgb_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    xgb_train.write.mode(\"overwrite\").parquet(str(xgb_dir / \"train\"))\n",
    "    xgb_val.write.mode(\"overwrite\").parquet(str(xgb_dir / \"val\"))\n",
    "    xgb_test.write.mode(\"overwrite\").parquet(str(xgb_dir / \"test\"))\n",
    "    \n",
    "    export_summary[\"xgb\"][\"train\"] = xgb_train.count()\n",
    "    export_summary[\"xgb\"][\"val\"] = xgb_val.count()\n",
    "    export_summary[\"xgb\"][\"test\"] = xgb_test.count()\n",
    "    \n",
    "    print(f\"     [SUCCESS] Saved to: {xgb_dir}/\")\n",
    "    print(f\"        - train: {export_summary['xgb']['train']:,} records\")\n",
    "    print(f\"        - val:   {export_summary['xgb']['val']:,} records\")\n",
    "    print(f\"        - test:  {export_summary['xgb']['test']:,} records\")\n",
    "\n",
    "# ========================================\n",
    "# SAVE METADATA\n",
    "# ========================================\n",
    "print(f\"\\n[SAVE] Saving metadata...\")\n",
    "\n",
    "# Create comprehensive metadata\n",
    "final_metadata = {\n",
    "    \"project\": \"PM2.5 Prediction\",\n",
    "    \"preprocessing_completed\": True,\n",
    "    \"export_timestamp\": str(pd.Timestamp.now()),\n",
    "    \"environment\": \"kaggle\" if IN_KAGGLE else (\"colab\" if IN_COLAB else \"local\"),\n",
    "    \"models\": {\n",
    "        \"cnn1d_blstm\": {\n",
    "            \"sequence_length\": CNN_SEQUENCE_LENGTH,\n",
    "            \"features\": len(dl_input_features),\n",
    "            \"ready\": datasets_ready[\"cnn\"],\n",
    "            \"export_path\": str(processed_dir / \"cnn_sequences\"),\n",
    "            \"record_counts\": export_summary[\"cnn\"]\n",
    "        },\n",
    "        \"lstm\": {\n",
    "            \"sequence_length\": LSTM_SEQUENCE_LENGTH, \n",
    "            \"features\": len(dl_input_features),\n",
    "            \"ready\": datasets_ready[\"lstm\"],\n",
    "            \"export_path\": str(processed_dir / \"lstm_sequences\"),\n",
    "            \"record_counts\": export_summary[\"lstm\"]\n",
    "        },\n",
    "        \"xgboost\": {\n",
    "            \"features\": len(xgb_input_features),\n",
    "            \"lag_steps\": LAG_STEPS,\n",
    "            \"ready\": datasets_ready[\"xgb\"],\n",
    "            \"export_path\": str(processed_dir / \"xgboost\"),\n",
    "            \"record_counts\": export_summary[\"xgb\"]\n",
    "        }\n",
    "    },\n",
    "    \"feature_details\": {\n",
    "        \"deep_learning_features\": dl_input_features,\n",
    "        \"xgboost_features\": xgb_input_features,\n",
    "        \"target\": target_feature\n",
    "    },\n",
    "    \"data_format\": \"parquet\",\n",
    "    \"null_handling\": {\n",
    "        \"strategy\": \"2-layer protection\",\n",
    "        \"layer1\": f\"Dropped first {CNN_SEQUENCE_LENGTH}/{LSTM_SEQUENCE_LENGTH} records per location\",\n",
    "        \"layer2\": \"Filtered records with nulls in sequence history\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = processed_dir / \"datasets_ready.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(final_metadata, f, indent=2)\n",
    "\n",
    "print(f\"   [SUCCESS] Metadata saved to: {metadata_path}\")\n",
    "\n",
    "# Save scaler params\n",
    "scaler_path = processed_dir / \"scaler_params.json\"\n",
    "scaler_json = {\n",
    "    col: {\"min\": float(params[\"min\"]), \"max\": float(params[\"max\"])} \n",
    "    for col, params in scaler_params.items()\n",
    "}\n",
    "with open(scaler_path, 'w') as f:\n",
    "    json.dump(scaler_json, f, indent=2)\n",
    "print(f\"   [SUCCESS] Scaler params saved to: {scaler_path}\")\n",
    "\n",
    "# Save feature metadata\n",
    "feature_metadata_path = processed_dir / \"feature_metadata.json\"\n",
    "feature_metadata = {\n",
    "    \"deep_learning_features\": dl_input_features,\n",
    "    \"xgboost_features\": xgb_input_features,\n",
    "    \"target\": target_feature,\n",
    "    \"lag_steps\": LAG_STEPS,\n",
    "    \"lag_base_columns\": lag_base_columns\n",
    "}\n",
    "with open(feature_metadata_path, 'w') as f:\n",
    "    json.dump(feature_metadata, f, indent=2)\n",
    "print(f\"   [SUCCESS] Feature metadata saved to: {feature_metadata_path}\")\n",
    "\n",
    "# ========================================\n",
    "# FINAL SUMMARY\n",
    "# ========================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"[SUCCESS] DATA PREPROCESSING & EXPORT COMPLETE!\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    print(f\"\\n[KAGGLE] KAGGLE OUTPUT:\")\n",
    "    print(f\"   [?] Location: /kaggle/working/processed/\")\n",
    "    print(f\"   [?] To save permanently:\")\n",
    "    print(f\"      1. Click 'Save Version' (top right)\")\n",
    "    print(f\"      2. Choose 'Save & Run All' (recommended)\")\n",
    "    print(f\"      3. Wait for completion (~20-30 min)\")\n",
    "    print(f\"      4. Output will appear in 'Output' tab\")\n",
    "    print(f\"      5. Use as dataset: '+ Add Data' -> Your Output\")\n",
    "    \n",
    "elif IN_COLAB:\n",
    "    print(f\"\\n[COLAB] COLAB OUTPUT:\")\n",
    "    print(f\"   [?] Saved to Google Drive: {processed_dir}\")\n",
    "    print(f\"   [SUCCESS] Files persist across sessions\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n[LOCAL] LOCAL OUTPUT:\")\n",
    "    print(f\"   [?] Location: {processed_dir.absolute()}\")\n",
    "\n",
    "print(f\"\\n[?] Exported Directory Structure:\")\n",
    "print(f\"   {processed_dir}/\")\n",
    "print(f\"   ├── cnn_sequences/\")\n",
    "print(f\"   │   ├── train/  ({export_summary['cnn']['train']:,} records)\")\n",
    "print(f\"   │   ├── val/    ({export_summary['cnn']['val']:,} records)\")\n",
    "print(f\"   │   └── test/   ({export_summary['cnn']['test']:,} records)\")\n",
    "print(f\"   ├── lstm_sequences/\")\n",
    "print(f\"   │   ├── train/  ({export_summary['lstm']['train']:,} records)\")\n",
    "print(f\"   │   ├── val/    ({export_summary['lstm']['val']:,} records)\")\n",
    "print(f\"   │   └── test/   ({export_summary['lstm']['test']:,} records)\")\n",
    "print(f\"   ├── xgboost/\")\n",
    "print(f\"   │   ├── train/  ({export_summary['xgb']['train']:,} records)\")\n",
    "print(f\"   │   ├── val/    ({export_summary['xgb']['val']:,} records)\")\n",
    "print(f\"   │   └── test/   ({export_summary['xgb']['test']:,} records)\")\n",
    "print(f\"   ├── scaler_params.json\")\n",
    "print(f\"   ├── feature_metadata.json\")\n",
    "print(f\"   └── datasets_ready.json\")\n",
    "\n",
    "print(f\"\\n[DATA] Total Dataset Sizes:\")\n",
    "total_cnn = sum(export_summary['cnn'].values())\n",
    "total_lstm = sum(export_summary['lstm'].values())\n",
    "total_xgb = sum(export_summary['xgb'].values())\n",
    "print(f\"   - CNN1D-BLSTM: {total_cnn:,} records ({CNN_SEQUENCE_LENGTH} timesteps, {len(dl_input_features)} features)\")\n",
    "print(f\"   - LSTM:        {total_lstm:,} records ({LSTM_SEQUENCE_LENGTH} timesteps, {len(dl_input_features)} features)\")\n",
    "print(f\"   - XGBoost:     {total_xgb:,} records ({len(xgb_input_features)} features)\")\n",
    "\n",
    "print(f\"\\n[RUN] Ready for Model Training Phase!\")\n",
    "if IN_KAGGLE:\n",
    "    print(f\"   [?] Next: Create new notebook, add this output as dataset\")\n",
    "    print(f\"   [?] Load: spark.read.parquet('/kaggle/input/<output-name>/processed/...')\")\n",
    "elif IN_COLAB:\n",
    "    print(f\"   [?] Load from Drive in next session\")\n",
    "else:\n",
    "    print(f\"   [?] Load: spark.read.parquet('{processed_dir}/...')\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee5eafd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T05:59:03.630385Z",
     "iopub.status.busy": "2025-11-09T05:59:03.630074Z",
     "iopub.status.idle": "2025-11-09T05:59:03.647986Z",
     "shell.execute_reply": "2025-11-09T05:59:03.646656Z"
    },
    "papermill": {
     "duration": 0.058405,
     "end_time": "2025-11-09T05:59:03.649901",
     "exception": false,
     "start_time": "2025-11-09T05:59:03.591496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Parquet File Size Analysis...\n",
      "================================================================================\n",
      "[KAGGLE] Kaggle mode: Analyzing /kaggle/working/processed\n",
      "\n",
      "[INSTALL] Dataset Sizes:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[MODEL] CNN Sequences:\n",
      "   - train:     67.91 MB\n",
      "   - val  :      4.93 MB\n",
      "   - test :      5.21 MB\n",
      "   Total:      78.04 MB\n",
      "\n",
      "[MODEL] LSTM Sequences:\n",
      "   - train:     40.56 MB\n",
      "   - val  :      4.62 MB\n",
      "   - test :      4.79 MB\n",
      "   Total:      49.96 MB\n",
      "\n",
      "[MODEL] XGBoost Data:\n",
      "   - train:     13.34 MB\n",
      "   - val  :      4.22 MB\n",
      "   - test :      4.04 MB\n",
      "   Total:      21.59 MB\n",
      "\n",
      " Metadata Files:\n",
      "   - scaler_params.json       :     507.00 B\n",
      "   - feature_metadata.json    :      2.38 KB\n",
      "   - datasets_ready.json      :      3.54 KB\n",
      "\n",
      "================================================================================\n",
      "[DATA] TOTAL SIZE SUMMARY:\n",
      "================================================================================\n",
      "   CNN Sequences       :     78.04 MB ( 52.2%)\n",
      "   LSTM Sequences      :     49.96 MB ( 33.4%)\n",
      "   XGBoost Data        :     21.59 MB ( 14.4%)\n",
      "   Metadata            :      6.41 KB (  0.0%)\n",
      "\n",
      "   GRAND TOTAL         :    149.60 MB\n"
     ]
    }
   ],
   "source": [
    "#  Kiểm tra kích thước files Parquet đã export\n",
    "print(\"\\n Parquet File Size Analysis...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import os\n",
    "\n",
    "# ========================================\n",
    "# ADAPTIVE PATH\n",
    "# ========================================\n",
    "if IN_KAGGLE:\n",
    "    processed_dir = Path(\"/kaggle/working/processed\")\n",
    "    print(f\"[KAGGLE] Kaggle mode: Analyzing {processed_dir}\")\n",
    "elif IN_COLAB:\n",
    "    processed_dir = Path(\"/content/drive/MyDrive/pm25-data/processed\")\n",
    "    print(f\"[COLAB] Colab mode: Analyzing Google Drive\")\n",
    "else:\n",
    "    processed_dir = Path(\"../data/processed\")\n",
    "    print(f\"[LOCAL] Local mode: Analyzing {processed_dir}\")\n",
    "\n",
    "# ========================================\n",
    "# Tính kích thước thư mục\n",
    "# ========================================\n",
    "def get_dir_size(path):\n",
    "    \"\"\"Tính tổng kích thước của thư mục (bao gồm tất cả subdirectories)\"\"\"\n",
    "    total_size = 0\n",
    "    try:\n",
    "        for dirpath, dirnames, filenames in os.walk(path):\n",
    "            for filename in filenames:\n",
    "                filepath = os.path.join(dirpath, filename)\n",
    "                if os.path.exists(filepath):\n",
    "                    total_size += os.path.getsize(filepath)\n",
    "    except Exception as e:\n",
    "        print(f\"   [WARNING]  Error accessing {path}: {e}\")\n",
    "        return 0\n",
    "    return total_size\n",
    "\n",
    "def format_size(bytes_size):\n",
    "    \"\"\"Format bytes thành human-readable\"\"\"\n",
    "    for unit in ['B', 'KB', 'MB', 'GB']:\n",
    "        if bytes_size < 1024.0:\n",
    "            return f\"{bytes_size:.2f} {unit}\"\n",
    "        bytes_size /= 1024.0\n",
    "    return f\"{bytes_size:.2f} TB\"\n",
    "\n",
    "# ========================================\n",
    "# Phân tích từng dataset\n",
    "# ========================================\n",
    "print(\"\\n[INSTALL] Dataset Sizes:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "datasets = {\n",
    "    'CNN Sequences': 'cnn_sequences',\n",
    "    'LSTM Sequences': 'lstm_sequences',\n",
    "    'XGBoost Data': 'xgboost'\n",
    "}\n",
    "\n",
    "total_size = 0\n",
    "size_breakdown = {}\n",
    "\n",
    "for name, folder in datasets.items():\n",
    "    dataset_path = processed_dir / folder\n",
    "    if dataset_path.exists():\n",
    "        # Tính size cho từng split\n",
    "        splits = ['train', 'val', 'test']\n",
    "        dataset_total = 0\n",
    "        print(f\"\\n[MODEL] {name}:\")\n",
    "        \n",
    "        for split in splits:\n",
    "            split_path = dataset_path / split\n",
    "            if split_path.exists():\n",
    "                size = get_dir_size(split_path)\n",
    "                dataset_total += size\n",
    "                print(f\"   - {split:5s}: {format_size(size):>12s}\")\n",
    "        \n",
    "        print(f\"   {'Total:':7s} {format_size(dataset_total):>12s}\")\n",
    "        size_breakdown[name] = dataset_total\n",
    "        total_size += dataset_total\n",
    "    else:\n",
    "        print(f\"\\n[WARNING]  {name}: Folder not found ({dataset_path})\")\n",
    "\n",
    "# Metadata files\n",
    "print(f\"\\n Metadata Files:\")\n",
    "metadata_files = ['scaler_params.json', 'feature_metadata.json', 'datasets_ready.json']\n",
    "metadata_total = 0\n",
    "for file in metadata_files:\n",
    "    file_path = processed_dir / file\n",
    "    if file_path.exists():\n",
    "        size = os.path.getsize(file_path)\n",
    "        metadata_total += size\n",
    "        print(f\"   - {file:25s}: {format_size(size):>12s}\")\n",
    "total_size += metadata_total\n",
    "\n",
    "# ========================================\n",
    "# Tổng kết\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[DATA] TOTAL SIZE SUMMARY:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, size in size_breakdown.items():\n",
    "    percentage = (size / total_size * 100) if total_size > 0 else 0\n",
    "    print(f\"   {name:20s}: {format_size(size):>12s} ({percentage:5.1f}%)\")\n",
    "\n",
    "print(f\"   {'Metadata':20s}: {format_size(metadata_total):>12s} ({(metadata_total/total_size*100):5.1f}%)\")\n",
    "print(f\"\\n   {'GRAND TOTAL':20s}: {format_size(total_size):>12s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b87ec8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T05:59:03.726379Z",
     "iopub.status.busy": "2025-11-09T05:59:03.725637Z",
     "iopub.status.idle": "2025-11-09T05:59:10.741612Z",
     "shell.execute_reply": "2025-11-09T05:59:10.740588Z"
    },
    "papermill": {
     "duration": 7.05649,
     "end_time": "2025-11-09T05:59:10.743206",
     "exception": false,
     "start_time": "2025-11-09T05:59:03.686716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[?] Loading Preprocessed Data with Pandas...\n",
      "================================================================================\n",
      "[KAGGLE] Kaggle mode: Loading from /kaggle/input/\n",
      "   Replace <your-dataset-name> with actual dataset name\n",
      "\n",
      "[INSTALL] Loading datasets...\n",
      "\n",
      "[MODEL] CNN1D-BLSTM-Attention:\n",
      "   [SUCCESS] Train: (186579, 18) | Val: (48948, 18) | Test: (50606, 18)\n"
     ]
    }
   ],
   "source": [
    "# [?] EXAMPLE: Load Preprocessed Data with Pandas\n",
    "print(\"\\n[?] Loading Preprocessed Data with Pandas...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========================================\n",
    "# ADAPTIVE PATH\n",
    "# ========================================\n",
    "if IN_KAGGLE:\n",
    "    data_dir = Path(\"/kaggle/working/processed\")\n",
    "    print(f\"[KAGGLE] Kaggle mode: Loading from /kaggle/input/\")\n",
    "    print(f\"   Replace <your-dataset-name> with actual dataset name\")\n",
    "elif IN_COLAB:\n",
    "    data_dir = Path(\"/content/drive/MyDrive/pm25-data/processed\")\n",
    "    print(f\"[COLAB] Colab mode: Loading from Google Drive\")\n",
    "else:\n",
    "    data_dir = Path(\"../data/processed\")\n",
    "    print(f\"[LOCAL] Local mode: Loading from {data_dir}\")\n",
    "\n",
    "# ========================================\n",
    "# LOAD PARQUET FILES\n",
    "# ========================================\n",
    "print(\"\\n[INSTALL] Loading datasets...\")\n",
    "\n",
    "try:\n",
    "    # CNN sequences (48 timesteps)\n",
    "    print(\"\\n[MODEL] CNN1D-BLSTM-Attention:\")\n",
    "    cnn_train = pd.read_parquet(data_dir / 'cnn_sequences' / 'train')\n",
    "    cnn_val = pd.read_parquet(data_dir / 'cnn_sequences' / 'val')\n",
    "    cnn_test = pd.read_parquet(data_dir / 'cnn_sequences' / 'test')\n",
    "    print(f\"   [SUCCESS] Train: {cnn_train.shape} | Val: {cnn_val.shape} | Test: {cnn_test.shape}\")\n",
    "    \n",
    "    # # LSTM sequences (24 timesteps)\n",
    "    # print(\"\\n[PROCESSING] LSTM:\")\n",
    "    # lstm_train = pd.read_parquet(data_dir / 'lstm_sequences' / 'train')\n",
    "    # lstm_val = pd.read_parquet(data_dir / 'lstm_sequences' / 'val')\n",
    "    # lstm_test = pd.read_parquet(data_dir / 'lstm_sequences' / 'test')\n",
    "    # print(f\"   [SUCCESS] Train: {lstm_train.shape} | Val: {lstm_val.shape} | Test: {lstm_test.shape}\")\n",
    "    \n",
    "    # # XGBoost data (flat features)\n",
    "    # print(\"\\n[DATA] XGBoost:\")\n",
    "    # xgb_train = pd.read_parquet(data_dir / 'xgboost' / 'train')\n",
    "    # xgb_val = pd.read_parquet(data_dir / 'xgboost' / 'val')\n",
    "    # xgb_test = pd.read_parquet(data_dir / 'xgboost' / 'test')\n",
    "    # print(f\"   [SUCCESS] Train: {xgb_train.shape} | Val: {xgb_val.shape} | Test: {xgb_test.shape}\")\n",
    "    \n",
    "    # print(f\"\\n[SUCCESS] All datasets loaded successfully!\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n[ERROR] Error: Dataset not found!\")\n",
    "    print(f\"   {e}\")\n",
    "    print(f\"\\n[INFO] Make sure to:\")\n",
    "    if IN_KAGGLE:\n",
    "        print(f\"   1. Add this notebook's output as dataset\")\n",
    "        print(f\"   2. Update <your-dataset-name> in path\")\n",
    "    else:\n",
    "        print(f\"   1. Run previous cells to generate data\")\n",
    "        print(f\"   2. Check path: {data_dir}\")\n",
    "\n",
    "# ========================================\n",
    "# LOAD METADATA\n",
    "# ========================================\n",
    "# print(\"\\n[METADATA] Loading metadata...\")\n",
    "\n",
    "# import json\n",
    "\n",
    "# try:\n",
    "#     # Scaler parameters (for denormalization)\n",
    "#     with open(data_dir / 'scaler_params.json', 'r') as f:\n",
    "#         scaler_params = json.load(f)\n",
    "#     print(f\"   [SUCCESS] Scaler params: {len(scaler_params)} features\")\n",
    "    \n",
    "#     # Feature metadata\n",
    "#     with open(data_dir / 'feature_metadata.json', 'r') as f:\n",
    "#         feature_metadata = json.load(f)\n",
    "#     print(f\"   [SUCCESS] Feature metadata: {feature_metadata['preprocessing_version']}\")\n",
    "    \n",
    "# except FileNotFoundError:\n",
    "#     print(f\"   [WARNING]  Metadata files not found (optional)\")\n",
    "#     scaler_params = None\n",
    "#     feature_metadata = None\n",
    "\n",
    "# # ========================================\n",
    "# # DATA PREPARATION FOR DEEP LEARNING\n",
    "# # ========================================\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"[TARGET] Prepare Data for Training:\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# # Example: CNN data preparation\n",
    "# print(\"\\n[INSTALL] CNN1D-BLSTM data preparation:\")\n",
    "\n",
    "# # Get sequence columns\n",
    "# sequence_cols = [col for col in cnn_train.columns if col.endswith('_sequence')]\n",
    "# print(f\"   - Sequence features: {len(sequence_cols)}\")\n",
    "# print(f\"   - Feature names: {sequence_cols[:3]}... (showing first 3)\")\n",
    "\n",
    "# # Convert to numpy arrays for deep learning\n",
    "# print(\"\\n   Converting to numpy arrays...\")\n",
    "\n",
    "# # Extract sequences (each row has arrays)\n",
    "# X_cnn_train = np.array([\n",
    "#     np.stack([cnn_train[col].iloc[i] for col in sequence_cols], axis=0)\n",
    "#     for i in range(len(cnn_train))\n",
    "# ])  # Shape: (samples, features, timesteps)\n",
    "\n",
    "# # Transpose to (samples, timesteps, features) for Keras/PyTorch\n",
    "# X_cnn_train = X_cnn_train.transpose(0, 2, 1)\n",
    "# y_cnn_train = cnn_train['target_value'].values\n",
    "\n",
    "# print(f\"   [SUCCESS] X_train shape: {X_cnn_train.shape} (samples, timesteps, features)\")\n",
    "# print(f\"   [SUCCESS] y_train shape: {y_cnn_train.shape}\")\n",
    "\n",
    "# # Same for validation and test\n",
    "# X_cnn_val = np.array([\n",
    "#     np.stack([cnn_val[col].iloc[i] for col in sequence_cols], axis=0)\n",
    "#     for i in range(len(cnn_val))\n",
    "# ]).transpose(0, 2, 1)\n",
    "# y_cnn_val = cnn_val['target_value'].values\n",
    "\n",
    "# X_cnn_test = np.array([\n",
    "#     np.stack([cnn_test[col].iloc[i] for col in sequence_cols], axis=0)\n",
    "#     for i in range(len(cnn_test))\n",
    "# ]).transpose(0, 2, 1)\n",
    "# y_cnn_test = cnn_test['target_value'].values\n",
    "\n",
    "# print(f\"   [SUCCESS] Val:  X={X_cnn_val.shape}, y={y_cnn_val.shape}\")\n",
    "# print(f\"   [SUCCESS] Test: X={X_cnn_test.shape}, y={y_cnn_test.shape}\")\n",
    "\n",
    "# # ========================================\n",
    "# # EXAMPLE: XGBoost data preparation\n",
    "# # ========================================\n",
    "# print(\"\\n[INSTALL] XGBoost data preparation:\")\n",
    "\n",
    "# # XGBoost data is already flat (no sequences)\n",
    "# X_xgb_train = xgb_train.drop(['location_id', 'datetime', 'target_value'], axis=1).values\n",
    "# y_xgb_train = xgb_train['target_value'].values\n",
    "\n",
    "# X_xgb_val = xgb_val.drop(['location_id', 'datetime', 'target_value'], axis=1).values\n",
    "# y_xgb_val = xgb_val['target_value'].values\n",
    "\n",
    "# X_xgb_test = xgb_test.drop(['location_id', 'datetime', 'target_value'], axis=1).values\n",
    "# y_xgb_test = xgb_test['target_value'].values\n",
    "\n",
    "# print(f\"   [SUCCESS] X_train shape: {X_xgb_train.shape} (samples, features)\")\n",
    "# print(f\"   [SUCCESS] y_train shape: {y_xgb_train.shape}\")\n",
    "# print(f\"   [SUCCESS] Val:  X={X_xgb_val.shape}, y={y_xgb_val.shape}\")\n",
    "# print(f\"   [SUCCESS] Test: X={X_xgb_test.shape}, y={y_xgb_test.shape}\")\n",
    "\n",
    "# # ========================================\n",
    "# # SUMMARY\n",
    "# # ========================================\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"[SUCCESS] DATA READY FOR TRAINING!\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# print(\"\\n[DATA] Available datasets:\")\n",
    "# print(\"   [MODEL] CNN1D-BLSTM-Attention:\")\n",
    "# print(f\"      X_cnn_train: {X_cnn_train.shape}\")\n",
    "# print(f\"      X_cnn_val:   {X_cnn_val.shape}\")\n",
    "# print(f\"      X_cnn_test:  {X_cnn_test.shape}\")\n",
    "\n",
    "# print(\"\\n   [PROCESSING] LSTM: (Similar structure, use lstm_train/val/test)\")\n",
    "\n",
    "# print(\"\\n   [DATA] XGBoost:\")\n",
    "# print(f\"      X_xgb_train: {X_xgb_train.shape}\")\n",
    "# print(f\"      X_xgb_val:   {X_xgb_val.shape}\")\n",
    "# print(f\"      X_xgb_test:  {X_xgb_test.shape}\")\n",
    "\n",
    "# print(\"\\n[RUN] Next steps:\")\n",
    "# print(\"   1. Build model: model = tf.keras.Sequential([...])\")\n",
    "# print(\"   2. Compile: model.compile(optimizer='adam', loss='mse')\")\n",
    "# print(\"   3. Train: model.fit(X_cnn_train, y_cnn_train, epochs=50)\")\n",
    "# print(\"   4. Evaluate: model.evaluate(X_cnn_test, y_cnn_test)\")\n",
    "\n",
    "# print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b96548f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T05:59:10.819529Z",
     "iopub.status.busy": "2025-11-09T05:59:10.819240Z",
     "iopub.status.idle": "2025-11-09T05:59:10.928264Z",
     "shell.execute_reply": "2025-11-09T05:59:10.927205Z"
    },
    "papermill": {
     "duration": 0.149464,
     "end_time": "2025-11-09T05:59:10.929756",
     "exception": false,
     "start_time": "2025-11-09T05:59:10.780292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>PM10_scaled_sequence</th>\n",
       "      <th>NO2_scaled_sequence</th>\n",
       "      <th>SO2_scaled_sequence</th>\n",
       "      <th>temperature_2m_scaled_sequence</th>\n",
       "      <th>relative_humidity_2m_scaled_sequence</th>\n",
       "      <th>wind_speed_10m_scaled_sequence</th>\n",
       "      <th>wind_direction_10m_scaled_sequence</th>\n",
       "      <th>precipitation_scaled_sequence</th>\n",
       "      <th>hour_sin_sequence</th>\n",
       "      <th>hour_cos_sequence</th>\n",
       "      <th>month_sin_sequence</th>\n",
       "      <th>month_cos_sequence</th>\n",
       "      <th>day_of_week_sin_sequence</th>\n",
       "      <th>day_of_week_cos_sequence</th>\n",
       "      <th>is_weekend_sequence</th>\n",
       "      <th>target_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-05 16:00:00</td>\n",
       "      <td>[0.01583809942806863, 0.04927408710954685, 0.1...</td>\n",
       "      <td>[0.11825017088174983, 0.13362952836637046, 0.1...</td>\n",
       "      <td>[0.02080624187256177, 0.02600780234070221, 0.0...</td>\n",
       "      <td>[0.592375366568915, 0.5982404692082112, 0.5923...</td>\n",
       "      <td>[0.6923076923076923, 0.6923076923076923, 0.679...</td>\n",
       "      <td>[0.16767922235722965, 0.19927095990279464, 0.1...</td>\n",
       "      <td>[0.15833333333333333, 0.1388888888888889, 0.11...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.001879699248120300...</td>\n",
       "      <td>[-0.7071067811865471, -0.2588190451025208, 1.2...</td>\n",
       "      <td>[-0.7071067811865479, -0.9659258262890683, -1....</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-2.4492935982947064e-16, -2.4492935982947064e...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0.118110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-05 21:00:00</td>\n",
       "      <td>[0.09634843818741749, 0.10118785745710514, 0.1...</td>\n",
       "      <td>[0.07894736842105263, 0.10389610389610389, 0.0...</td>\n",
       "      <td>[0.019505851755526656, 0.022106631989596878, 0...</td>\n",
       "      <td>[0.5249266862170089, 0.5425219941348973, 0.560...</td>\n",
       "      <td>[0.8205128205128205, 0.7564102564102564, 0.756...</td>\n",
       "      <td>[0.13122721749696234, 0.13973268529769137, 0.1...</td>\n",
       "      <td>[0.058333333333333334, 0.08888888888888889, 0....</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.8660254037844386, -0.9659258262890684, -1....</td>\n",
       "      <td>[0.5000000000000001, 0.2588190451025203, -1.83...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-2.4492935982947064e-16, -2.4492935982947064e...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0.074147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-06 20:00:00</td>\n",
       "      <td>[0.09326880774307082, 0.0941487021557413, 0.08...</td>\n",
       "      <td>[0.08065618591934381, 0.09330143540669857, 0.0...</td>\n",
       "      <td>[0.011703511053315994, 0.014304291287386216, 0...</td>\n",
       "      <td>[0.5043988269794722, 0.5102639296187683, 0.521...</td>\n",
       "      <td>[0.8846153846153846, 0.8974358974358975, 0.858...</td>\n",
       "      <td>[0.13973268529769137, 0.1057108140947752, 0.13...</td>\n",
       "      <td>[0.12777777777777777, 0.14444444444444443, 0.1...</td>\n",
       "      <td>[0.0, 0.0018796992481203006, 0.0, 0.0018796992...</td>\n",
       "      <td>[-0.9659258262890684, -1.0, -0.965925826289068...</td>\n",
       "      <td>[0.2588190451025203, -1.8369701987210297e-16, ...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[0.7818314824680298, 0.7818314824680298, 0.781...</td>\n",
       "      <td>[0.6234898018587336, 0.6234898018587336, 0.623...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0.096457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-07 07:00:00</td>\n",
       "      <td>[0.06467223933128025, 0.06467223933128025, 0.0...</td>\n",
       "      <td>[0.14490772385509226, 0.13704716336295283, 0.1...</td>\n",
       "      <td>[0.02600780234070221, 0.02860858257477243, 0.0...</td>\n",
       "      <td>[0.4780058651026393, 0.4809384164222874, 0.480...</td>\n",
       "      <td>[0.9487179487179487, 0.9487179487179487, 0.948...</td>\n",
       "      <td>[0.12393681652490887, 0.1275820170109356, 0.10...</td>\n",
       "      <td>[0.125, 0.10555555555555556, 0.091666666666666...</td>\n",
       "      <td>[0.0, 0.0018796992481203006, 0.0, 0.0, 0.0, 0....</td>\n",
       "      <td>[0.8660254037844386, 0.7071067811865475, 0.499...</td>\n",
       "      <td>[0.5000000000000001, 0.7071067811865476, 0.866...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[0.9749279121818236, 0.9749279121818236, 0.974...</td>\n",
       "      <td>[-0.22252093395631434, -0.22252093395631434, -...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0.079396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-07 14:00:00</td>\n",
       "      <td>[0.06775186977562692, 0.05147382314122305, 0.0...</td>\n",
       "      <td>[0.1544771018455229, 0.14866712235133286, 0.13...</td>\n",
       "      <td>[0.02080624187256177, 0.019505851755526656, 0....</td>\n",
       "      <td>[0.6070381231671554, 0.6011730205278593, 0.565...</td>\n",
       "      <td>[0.717948717948718, 0.7307692307692307, 0.8333...</td>\n",
       "      <td>[0.15674362089914945, 0.14580801944106928, 0.1...</td>\n",
       "      <td>[0.1, 0.10833333333333334, 0.08611111111111111...</td>\n",
       "      <td>[0.0, 0.0018796992481203006, 0.001879699248120...</td>\n",
       "      <td>[-0.2588190451025208, 1.2246467991473532e-16, ...</td>\n",
       "      <td>[-0.9659258262890683, -1.0, -0.965925826289068...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[0.9749279121818236, 0.9749279121818236, 0.974...</td>\n",
       "      <td>[-0.22252093395631434, -0.22252093395631434, -...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.099081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-07 15:00:00</td>\n",
       "      <td>[0.08930928288605368, 0.06775186977562692, 0.0...</td>\n",
       "      <td>[0.14251537935748462, 0.1544771018455229, 0.14...</td>\n",
       "      <td>[0.023407022106631988, 0.02080624187256177, 0....</td>\n",
       "      <td>[0.6158357771260997, 0.6070381231671554, 0.601...</td>\n",
       "      <td>[0.7051282051282052, 0.717948717948718, 0.7307...</td>\n",
       "      <td>[0.14580801944106928, 0.15674362089914945, 0.1...</td>\n",
       "      <td>[0.11388888888888889, 0.1, 0.10833333333333334...</td>\n",
       "      <td>[0.0, 0.0, 0.0018796992481203006, 0.0018796992...</td>\n",
       "      <td>[-0.4999999999999997, -0.2588190451025208, 1.2...</td>\n",
       "      <td>[-0.8660254037844388, -0.9659258262890683, -1....</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[0.9749279121818236, 0.9749279121818236, 0.974...</td>\n",
       "      <td>[-0.22252093395631434, -0.22252093395631434, -...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.090551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-07 16:00:00</td>\n",
       "      <td>[0.09810822701275847, 0.08930928288605368, 0.0...</td>\n",
       "      <td>[0.1404647983595352, 0.14251537935748462, 0.15...</td>\n",
       "      <td>[0.022106631989596878, 0.023407022106631988, 0...</td>\n",
       "      <td>[0.6129032258064516, 0.6158357771260997, 0.607...</td>\n",
       "      <td>[0.7051282051282052, 0.7051282051282052, 0.717...</td>\n",
       "      <td>[0.15188335358444716, 0.14580801944106928, 0.1...</td>\n",
       "      <td>[0.12222222222222222, 0.11388888888888889, 0.1...</td>\n",
       "      <td>[0.03195488721804511, 0.0, 0.0, 0.001879699248...</td>\n",
       "      <td>[-0.7071067811865471, -0.4999999999999997, -0....</td>\n",
       "      <td>[-0.7071067811865479, -0.8660254037844388, -0....</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[0.9749279121818236, 0.9749279121818236, 0.974...</td>\n",
       "      <td>[-0.22252093395631434, -0.22252093395631434, -...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.093176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-07 17:00:00</td>\n",
       "      <td>[0.10030796304443466, 0.09810822701275847, 0.0...</td>\n",
       "      <td>[0.11483253588516747, 0.1404647983595352, 0.14...</td>\n",
       "      <td>[0.022106631989596878, 0.022106631989596878, 0...</td>\n",
       "      <td>[0.6041055718475073, 0.6129032258064516, 0.615...</td>\n",
       "      <td>[0.717948717948718, 0.7051282051282052, 0.7051...</td>\n",
       "      <td>[0.15188335358444716, 0.15188335358444716, 0.1...</td>\n",
       "      <td>[0.10833333333333334, 0.12222222222222222, 0.1...</td>\n",
       "      <td>[0.03383458646616541, 0.03195488721804511, 0.0...</td>\n",
       "      <td>[-0.8660254037844384, -0.7071067811865471, -0....</td>\n",
       "      <td>[-0.5000000000000004, -0.7071067811865479, -0....</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[0.9749279121818236, 0.9749279121818236, 0.974...</td>\n",
       "      <td>[-0.22252093395631434, -0.22252093395631434, -...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.107612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-08 00:00:00</td>\n",
       "      <td>[0.05543334799824021, 0.07347118345798503, 0.0...</td>\n",
       "      <td>[0.11209842788790156, 0.08612440191387559, 0.0...</td>\n",
       "      <td>[0.02860858257477243, 0.03250975292587776, 0.0...</td>\n",
       "      <td>[0.5219941348973607, 0.5337243401759532, 0.536...</td>\n",
       "      <td>[0.9487179487179487, 0.9358974358974359, 0.923...</td>\n",
       "      <td>[0.1336573511543135, 0.061968408262454436, 0.0...</td>\n",
       "      <td>[0.14444444444444443, 0.14166666666666666, 0.1...</td>\n",
       "      <td>[0.07142857142857142, 0.06766917293233082, 0.0...</td>\n",
       "      <td>[-0.25881904510252157, -0.5000000000000004, -0...</td>\n",
       "      <td>[0.9659258262890681, 0.8660254037844384, 0.707...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[0.9749279121818236, 0.9749279121818236, 0.974...</td>\n",
       "      <td>[-0.22252093395631434, -0.22252093395631434, -...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.069554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-08 02:00:00</td>\n",
       "      <td>[0.061152661680598325, 0.05279366476022877, 0....</td>\n",
       "      <td>[0.13260423786739575, 0.13533834586466165, 0.1...</td>\n",
       "      <td>[0.04031209362808842, 0.04031209362808842, 0.0...</td>\n",
       "      <td>[0.5219941348973607, 0.5249266862170089, 0.521...</td>\n",
       "      <td>[0.9358974358974359, 0.9230769230769231, 0.948...</td>\n",
       "      <td>[0.1336573511543135, 0.13244228432563793, 0.13...</td>\n",
       "      <td>[0.1361111111111111, 0.175, 0.1444444444444444...</td>\n",
       "      <td>[0.0, 0.007518796992481203, 0.0714285714285714...</td>\n",
       "      <td>[0.25881904510252074, 0.0, -0.2588190451025215...</td>\n",
       "      <td>[0.9659258262890683, 1.0, 0.9659258262890681, ...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[0.43388373911755823, 0.43388373911755823, 0.9...</td>\n",
       "      <td>[-0.900968867902419, -0.900968867902419, -0.22...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.088583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  location_id            datetime  \\\n",
       "0      233335 2022-11-05 16:00:00   \n",
       "1      233335 2022-11-05 21:00:00   \n",
       "2      233335 2022-11-06 20:00:00   \n",
       "3      233335 2022-11-07 07:00:00   \n",
       "4      233335 2022-11-07 14:00:00   \n",
       "5      233335 2022-11-07 15:00:00   \n",
       "6      233335 2022-11-07 16:00:00   \n",
       "7      233335 2022-11-07 17:00:00   \n",
       "8      233335 2022-11-08 00:00:00   \n",
       "9      233335 2022-11-08 02:00:00   \n",
       "\n",
       "                                PM10_scaled_sequence  \\\n",
       "0  [0.01583809942806863, 0.04927408710954685, 0.1...   \n",
       "1  [0.09634843818741749, 0.10118785745710514, 0.1...   \n",
       "2  [0.09326880774307082, 0.0941487021557413, 0.08...   \n",
       "3  [0.06467223933128025, 0.06467223933128025, 0.0...   \n",
       "4  [0.06775186977562692, 0.05147382314122305, 0.0...   \n",
       "5  [0.08930928288605368, 0.06775186977562692, 0.0...   \n",
       "6  [0.09810822701275847, 0.08930928288605368, 0.0...   \n",
       "7  [0.10030796304443466, 0.09810822701275847, 0.0...   \n",
       "8  [0.05543334799824021, 0.07347118345798503, 0.0...   \n",
       "9  [0.061152661680598325, 0.05279366476022877, 0....   \n",
       "\n",
       "                                 NO2_scaled_sequence  \\\n",
       "0  [0.11825017088174983, 0.13362952836637046, 0.1...   \n",
       "1  [0.07894736842105263, 0.10389610389610389, 0.0...   \n",
       "2  [0.08065618591934381, 0.09330143540669857, 0.0...   \n",
       "3  [0.14490772385509226, 0.13704716336295283, 0.1...   \n",
       "4  [0.1544771018455229, 0.14866712235133286, 0.13...   \n",
       "5  [0.14251537935748462, 0.1544771018455229, 0.14...   \n",
       "6  [0.1404647983595352, 0.14251537935748462, 0.15...   \n",
       "7  [0.11483253588516747, 0.1404647983595352, 0.14...   \n",
       "8  [0.11209842788790156, 0.08612440191387559, 0.0...   \n",
       "9  [0.13260423786739575, 0.13533834586466165, 0.1...   \n",
       "\n",
       "                                 SO2_scaled_sequence  \\\n",
       "0  [0.02080624187256177, 0.02600780234070221, 0.0...   \n",
       "1  [0.019505851755526656, 0.022106631989596878, 0...   \n",
       "2  [0.011703511053315994, 0.014304291287386216, 0...   \n",
       "3  [0.02600780234070221, 0.02860858257477243, 0.0...   \n",
       "4  [0.02080624187256177, 0.019505851755526656, 0....   \n",
       "5  [0.023407022106631988, 0.02080624187256177, 0....   \n",
       "6  [0.022106631989596878, 0.023407022106631988, 0...   \n",
       "7  [0.022106631989596878, 0.022106631989596878, 0...   \n",
       "8  [0.02860858257477243, 0.03250975292587776, 0.0...   \n",
       "9  [0.04031209362808842, 0.04031209362808842, 0.0...   \n",
       "\n",
       "                      temperature_2m_scaled_sequence  \\\n",
       "0  [0.592375366568915, 0.5982404692082112, 0.5923...   \n",
       "1  [0.5249266862170089, 0.5425219941348973, 0.560...   \n",
       "2  [0.5043988269794722, 0.5102639296187683, 0.521...   \n",
       "3  [0.4780058651026393, 0.4809384164222874, 0.480...   \n",
       "4  [0.6070381231671554, 0.6011730205278593, 0.565...   \n",
       "5  [0.6158357771260997, 0.6070381231671554, 0.601...   \n",
       "6  [0.6129032258064516, 0.6158357771260997, 0.607...   \n",
       "7  [0.6041055718475073, 0.6129032258064516, 0.615...   \n",
       "8  [0.5219941348973607, 0.5337243401759532, 0.536...   \n",
       "9  [0.5219941348973607, 0.5249266862170089, 0.521...   \n",
       "\n",
       "                relative_humidity_2m_scaled_sequence  \\\n",
       "0  [0.6923076923076923, 0.6923076923076923, 0.679...   \n",
       "1  [0.8205128205128205, 0.7564102564102564, 0.756...   \n",
       "2  [0.8846153846153846, 0.8974358974358975, 0.858...   \n",
       "3  [0.9487179487179487, 0.9487179487179487, 0.948...   \n",
       "4  [0.717948717948718, 0.7307692307692307, 0.8333...   \n",
       "5  [0.7051282051282052, 0.717948717948718, 0.7307...   \n",
       "6  [0.7051282051282052, 0.7051282051282052, 0.717...   \n",
       "7  [0.717948717948718, 0.7051282051282052, 0.7051...   \n",
       "8  [0.9487179487179487, 0.9358974358974359, 0.923...   \n",
       "9  [0.9358974358974359, 0.9230769230769231, 0.948...   \n",
       "\n",
       "                      wind_speed_10m_scaled_sequence  \\\n",
       "0  [0.16767922235722965, 0.19927095990279464, 0.1...   \n",
       "1  [0.13122721749696234, 0.13973268529769137, 0.1...   \n",
       "2  [0.13973268529769137, 0.1057108140947752, 0.13...   \n",
       "3  [0.12393681652490887, 0.1275820170109356, 0.10...   \n",
       "4  [0.15674362089914945, 0.14580801944106928, 0.1...   \n",
       "5  [0.14580801944106928, 0.15674362089914945, 0.1...   \n",
       "6  [0.15188335358444716, 0.14580801944106928, 0.1...   \n",
       "7  [0.15188335358444716, 0.15188335358444716, 0.1...   \n",
       "8  [0.1336573511543135, 0.061968408262454436, 0.0...   \n",
       "9  [0.1336573511543135, 0.13244228432563793, 0.13...   \n",
       "\n",
       "                  wind_direction_10m_scaled_sequence  \\\n",
       "0  [0.15833333333333333, 0.1388888888888889, 0.11...   \n",
       "1  [0.058333333333333334, 0.08888888888888889, 0....   \n",
       "2  [0.12777777777777777, 0.14444444444444443, 0.1...   \n",
       "3  [0.125, 0.10555555555555556, 0.091666666666666...   \n",
       "4  [0.1, 0.10833333333333334, 0.08611111111111111...   \n",
       "5  [0.11388888888888889, 0.1, 0.10833333333333334...   \n",
       "6  [0.12222222222222222, 0.11388888888888889, 0.1...   \n",
       "7  [0.10833333333333334, 0.12222222222222222, 0.1...   \n",
       "8  [0.14444444444444443, 0.14166666666666666, 0.1...   \n",
       "9  [0.1361111111111111, 0.175, 0.1444444444444444...   \n",
       "\n",
       "                       precipitation_scaled_sequence  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.001879699248120300...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0018796992481203006, 0.0, 0.0018796992...   \n",
       "3  [0.0, 0.0018796992481203006, 0.0, 0.0, 0.0, 0....   \n",
       "4  [0.0, 0.0018796992481203006, 0.001879699248120...   \n",
       "5  [0.0, 0.0, 0.0018796992481203006, 0.0018796992...   \n",
       "6  [0.03195488721804511, 0.0, 0.0, 0.001879699248...   \n",
       "7  [0.03383458646616541, 0.03195488721804511, 0.0...   \n",
       "8  [0.07142857142857142, 0.06766917293233082, 0.0...   \n",
       "9  [0.0, 0.007518796992481203, 0.0714285714285714...   \n",
       "\n",
       "                                   hour_sin_sequence  \\\n",
       "0  [-0.7071067811865471, -0.2588190451025208, 1.2...   \n",
       "1  [-0.8660254037844386, -0.9659258262890684, -1....   \n",
       "2  [-0.9659258262890684, -1.0, -0.965925826289068...   \n",
       "3  [0.8660254037844386, 0.7071067811865475, 0.499...   \n",
       "4  [-0.2588190451025208, 1.2246467991473532e-16, ...   \n",
       "5  [-0.4999999999999997, -0.2588190451025208, 1.2...   \n",
       "6  [-0.7071067811865471, -0.4999999999999997, -0....   \n",
       "7  [-0.8660254037844384, -0.7071067811865471, -0....   \n",
       "8  [-0.25881904510252157, -0.5000000000000004, -0...   \n",
       "9  [0.25881904510252074, 0.0, -0.2588190451025215...   \n",
       "\n",
       "                                   hour_cos_sequence  \\\n",
       "0  [-0.7071067811865479, -0.9659258262890683, -1....   \n",
       "1  [0.5000000000000001, 0.2588190451025203, -1.83...   \n",
       "2  [0.2588190451025203, -1.8369701987210297e-16, ...   \n",
       "3  [0.5000000000000001, 0.7071067811865476, 0.866...   \n",
       "4  [-0.9659258262890683, -1.0, -0.965925826289068...   \n",
       "5  [-0.8660254037844388, -0.9659258262890683, -1....   \n",
       "6  [-0.7071067811865479, -0.8660254037844388, -0....   \n",
       "7  [-0.5000000000000004, -0.7071067811865479, -0....   \n",
       "8  [0.9659258262890681, 0.8660254037844384, 0.707...   \n",
       "9  [0.9659258262890683, 1.0, 0.9659258262890681, ...   \n",
       "\n",
       "                                  month_sin_sequence  \\\n",
       "0  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "1  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "2  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "3  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "4  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "5  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "6  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "7  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "8  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "9  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "\n",
       "                                  month_cos_sequence  \\\n",
       "0  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "1  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "2  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "3  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "4  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "5  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "6  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "7  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "8  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "9  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "\n",
       "                            day_of_week_sin_sequence  \\\n",
       "0  [-2.4492935982947064e-16, -2.4492935982947064e...   \n",
       "1  [-2.4492935982947064e-16, -2.4492935982947064e...   \n",
       "2  [0.7818314824680298, 0.7818314824680298, 0.781...   \n",
       "3  [0.9749279121818236, 0.9749279121818236, 0.974...   \n",
       "4  [0.9749279121818236, 0.9749279121818236, 0.974...   \n",
       "5  [0.9749279121818236, 0.9749279121818236, 0.974...   \n",
       "6  [0.9749279121818236, 0.9749279121818236, 0.974...   \n",
       "7  [0.9749279121818236, 0.9749279121818236, 0.974...   \n",
       "8  [0.9749279121818236, 0.9749279121818236, 0.974...   \n",
       "9  [0.43388373911755823, 0.43388373911755823, 0.9...   \n",
       "\n",
       "                            day_of_week_cos_sequence  \\\n",
       "0  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n",
       "1  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n",
       "2  [0.6234898018587336, 0.6234898018587336, 0.623...   \n",
       "3  [-0.22252093395631434, -0.22252093395631434, -...   \n",
       "4  [-0.22252093395631434, -0.22252093395631434, -...   \n",
       "5  [-0.22252093395631434, -0.22252093395631434, -...   \n",
       "6  [-0.22252093395631434, -0.22252093395631434, -...   \n",
       "7  [-0.22252093395631434, -0.22252093395631434, -...   \n",
       "8  [-0.22252093395631434, -0.22252093395631434, -...   \n",
       "9  [-0.900968867902419, -0.900968867902419, -0.22...   \n",
       "\n",
       "                                 is_weekend_sequence  target_value  \n",
       "0  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...      0.118110  \n",
       "1  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...      0.074147  \n",
       "2  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...      0.096457  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, ...      0.079396  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...      0.099081  \n",
       "5  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...      0.090551  \n",
       "6  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...      0.093176  \n",
       "7  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...      0.107612  \n",
       "8  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...      0.069554  \n",
       "9  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...      0.088583  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_train.head(10)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8684233,
     "sourceId": 13659172,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1263.377709,
   "end_time": "2025-11-09T05:59:13.889329",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-09T05:38:10.511620",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

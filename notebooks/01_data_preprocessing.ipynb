{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b24733ca",
   "metadata": {},
   "source": [
    "# PM2.5 Prediction - Data Preprocessing\n",
    "\n",
    "Notebook n√†y th·ª±c hi·ªán ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu v·ªõi PySpark:\n",
    "1. K·∫øt n·ªëi Spark cluster\n",
    "2. ƒê·ªçc v√† kh√°m ph√° d·ªØ li·ªáu\n",
    "3. T·ªïng quan v·ªÅ dataset\n",
    "4. **L√†m s·∫°ch d·ªØ li·ªáu** (Outlier Removal ‚Üí Missing Value Imputation)\n",
    "5. Feature engineering\n",
    "6. Data summary & statistics\n",
    "7. L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6420c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Using Java: C:\\Program Files\\Java\\jdk-21\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set Java 21 for PySpark (kh·ªõp v·ªõi Java ƒë√£ c√†i)\n",
    "os.environ['JAVA_HOME'] = r'C:\\Program Files\\Java\\jdk-21'\n",
    "os.environ['PATH'] = os.environ['JAVA_HOME'] + r'\\bin;' + os.environ.get('PATH', '')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(f\"‚úì Using Java: {os.environ['JAVA_HOME']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18df14d2",
   "metadata": {},
   "source": [
    "## 1. K·∫øt n·ªëi Spark Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "057cbfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Spark version: 4.0.1\n",
      "‚úì Spark mode: local[*]\n",
      "‚úì Application ID: local-1762608273757\n",
      "‚úì Cores: 12\n",
      "‚úì Python worker timeout: 600s (10 minutes)\n"
     ]
    }
   ],
   "source": [
    "# T·∫°o Spark Session (Local Mode - kh√¥ng c·∫ßn Docker cluster)\n",
    "# V·ªõi c·∫•u h√¨nh t·ªëi ∆∞u cho Pandas UDF v√† convert operations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PM25-Preprocessing\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.python.worker.timeout\", \"600\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .config(\"spark.network.timeout\", \"600s\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"‚úì Spark version: {spark.version}\")\n",
    "print(f\"‚úì Spark mode: {spark.sparkContext.master}\")\n",
    "print(f\"‚úì Application ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"‚úì Cores: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"‚úì Python worker timeout: 600s (10 minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cdf19f",
   "metadata": {},
   "source": [
    "## 2. ƒê·ªãnh nghƒ©a Schema v√† Scan Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f6e4012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Schemas defined\n"
     ]
    }
   ],
   "source": [
    "# Schema cho d·ªØ li·ªáu OpenAQ\n",
    "openaq_schema = StructType([\n",
    "    StructField(\"location_id\", StringType(), True),\n",
    "    StructField(\"sensors_id\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"datetime\", TimestampType(), True),\n",
    "    StructField(\"lat\", DoubleType(), True),\n",
    "    StructField(\"lon\", DoubleType(), True),\n",
    "    StructField(\"parameter\", StringType(), True),\n",
    "    StructField(\"units\", StringType(), True),\n",
    "    StructField(\"value\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Schema cho d·ªØ li·ªáu Weather\n",
    "weather_schema = StructType([\n",
    "    StructField(\"time\", TimestampType(), True),\n",
    "    StructField(\"temperature_2m\", DoubleType(), True),\n",
    "    StructField(\"relative_humidity_2m\", DoubleType(), True),\n",
    "    StructField(\"wind_speed_10m\", DoubleType(), True),\n",
    "    StructField(\"wind_direction_10m\", DoubleType(), True),\n",
    "    StructField(\"surface_pressure\", DoubleType(), True),\n",
    "    StructField(\"precipitation\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "print(\"‚úì Schemas defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb7210a",
   "metadata": {},
   "source": [
    "### 2.1 Scan v√† Map Files theo Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df1b984b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Found 14 pollutant files:\n",
      "  ‚úì Location 233335: pollutant_location_233335.csv + weather_location_233335.csv\n",
      "  ‚úì Location 233336: pollutant_location_233336.csv + weather_location_233336.csv\n",
      "  ‚úì Location 7727: pollutant_location_7727.csv + weather_location_7727.csv\n",
      "  ‚úì Location 7728: pollutant_location_7728.csv + weather_location_7728.csv\n",
      "  ‚úì Location 7730: pollutant_location_7730.csv + weather_location_7730.csv\n",
      "  ‚úì Location 7732: pollutant_location_7732.csv + weather_location_7732.csv\n",
      "  ‚úì Location 7733: pollutant_location_7733.csv + weather_location_7733.csv\n",
      "  ‚úì Location 7734: pollutant_location_7734.csv + weather_location_7734.csv\n",
      "  ‚úì Location 7735: pollutant_location_7735.csv + weather_location_7735.csv\n",
      "  ‚úì Location 7736: pollutant_location_7736.csv + weather_location_7736.csv\n",
      "  ‚úì Location 7737: pollutant_location_7737.csv + weather_location_7737.csv\n",
      "  ‚úì Location 7739: pollutant_location_7739.csv + weather_location_7739.csv\n",
      "  ‚úì Location 7740: pollutant_location_7740.csv + weather_location_7740.csv\n",
      "  ‚úì Location 7742: pollutant_location_7742.csv + weather_location_7742.csv\n",
      "\n",
      "‚úÖ Total locations to process: 14\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# T√¨m t·∫•t c·∫£ c√°c file pollutant\n",
    "raw_data_path = Path(\"../data/raw\")\n",
    "pollutant_files = list(raw_data_path.glob(\"pollutant_location_*.csv\"))\n",
    "\n",
    "print(f\"üìÅ Found {len(pollutant_files)} pollutant files:\")\n",
    "\n",
    "# T·∫°o mapping gi·ªØa pollutant v√† weather files\n",
    "location_mapping = {}\n",
    "\n",
    "for pollutant_file in pollutant_files:\n",
    "    # Extract location_id t·ª´ t√™n file: pollutant_location_7727.csv ‚Üí 7727\n",
    "    match = re.search(r'pollutant_location_(\\d+)\\.csv', pollutant_file.name)\n",
    "    \n",
    "    if match:\n",
    "        location_id = match.group(1)\n",
    "        weather_file = raw_data_path / f\"weather_location_{location_id}.csv\"\n",
    "        \n",
    "        # Ki·ªÉm tra file weather t∆∞∆°ng ·ª©ng c√≥ t·ªìn t·∫°i kh√¥ng\n",
    "        if weather_file.exists():\n",
    "            location_mapping[location_id] = {\n",
    "                'pollutant': str(pollutant_file),\n",
    "                'weather': str(weather_file)\n",
    "            }\n",
    "            print(f\"  ‚úì Location {location_id}: {pollutant_file.name} + {weather_file.name}\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è  Location {location_id}: Missing weather file!\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total locations to process: {len(location_mapping)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bd57cd",
   "metadata": {},
   "source": [
    "### 2.2 X·ª≠ l√Ω t·ª´ng Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62c0476b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Processing Location 233335...\n",
      "  üìä Air quality (PM2.5, PM10, SO2, NO2): 83,970 records\n",
      "  üìä Air quality (PM2.5, PM10, SO2, NO2): 83,970 records\n",
      "  üå§Ô∏è  Weather: 25,560 records\n",
      "  üå§Ô∏è  Weather: 25,560 records\n",
      "  ‚úì After join: 21,686 records\n",
      "\n",
      "üîÑ Processing Location 233336...\n",
      "  ‚úì After join: 21,686 records\n",
      "\n",
      "üîÑ Processing Location 233336...\n",
      "  üìä Air quality (PM2.5, PM10, SO2, NO2): 83,818 records\n",
      "  üå§Ô∏è  Weather: 25,560 records\n",
      "  üìä Air quality (PM2.5, PM10, SO2, NO2): 83,818 records\n",
      "  üå§Ô∏è  Weather: 25,560 records\n",
      "  ‚úì After join: 21,262 records\n",
      "\n",
      "üîÑ Processing Location 7727...\n",
      "  ‚úì After join: 21,262 records\n",
      "\n",
      "üîÑ Processing Location 7727...\n",
      "  üìä Air quality (PM2.5, PM10, SO2, NO2): 86,684 records\n",
      "  üå§Ô∏è  Weather: 25,560 records\n",
      "  üìä Air quality (PM2.5, PM10, SO2, NO2): 86,684 records\n",
      "  üå§Ô∏è  Weather: 25,560 records\n",
      "  ‚úì After join: 22,367 records\n",
      "\n",
      "üîÑ Processing Location 7728...\n",
      "  üìä Air quality (PM2.5, PM10, SO2, NO2): 84,638 records\n",
      "  ‚úì After join: 22,367 records\n",
      "\n",
      "üîÑ Processing Location 7728...\n",
      "  üìä Air quality (PM2.5, PM10, SO2, NO2): 84,638 records\n",
      "  üå§Ô∏è  Weather: 25,560 records\n",
      "  üå§Ô∏è  Weather: 25,560 records\n",
      "  ‚úì After join: 21,683 records\n",
      "\n",
      "üîÑ Processing Location 7730...\n",
      "  üìä Air quality (PM2.5, PM10, SO2, NO2): 82,776 records\n",
      "  ‚úì After join: 21,683 records\n",
      "\n",
      "üîÑ Processing Location 7730...\n",
      "  üìä Air quality (PM2.5, PM10, SO2, NO2): 82,776 records\n",
      "  üå§Ô∏è  Weather: 25,560 records\n",
      "  üå§Ô∏è  Weather: 25,560 records\n",
      "  ‚úì After join: 21,274 records\n",
      "\n",
      "üîÑ Processing Location 7732...\n",
      "  üìä Air quality (PM2.5, PM10, SO2, NO2): 83,386 records\n",
      "  ‚úì After join: 21,274 records\n",
      "\n",
      "üîÑ Processing Location 7732...\n",
      "  üìä Air quality (PM2.5, PM10, SO2, NO2): 83,386 records\n",
      "  üå§Ô∏è  Weather: 25,560 records\n",
      "  üå§Ô∏è  Weather: 25,560 records\n",
      "  ‚úì After join: 21,306 records\n",
      "\n",
      "üîÑ Processing Location 7733...\n",
      "  üìä Air quality (PM2.5, PM10, SO2, NO2): 83,047 records\n",
      "  ‚úì After join: 21,306 records\n",
      "\n",
      "üîÑ Processing Location 7733...\n",
      "  üìä Air quality (PM2.5, PM10, SO2, NO2): 83,047 records\n",
      "  üå§Ô∏è  Weather: 25,560 records\n",
      "  üå§Ô∏è  Weather: 25,560 records\n",
      "  ‚úì After join: 21,235 records\n",
      "\n",
      "üîÑ Processing Location 7734...\n",
      "  üìä Air quality (PM2.5, PM10, SO2, NO2): 82,570 records\n",
      "  ‚úì After join: 21,235 records\n",
      "\n",
      "üîÑ Processing Location 7734...\n",
      "  üìä Air quality (PM2.5, PM10, SO2, NO2): 82,570 records\n",
      "  üå§Ô∏è  Weather: 25,560 records\n",
      "  üå§Ô∏è  Weather: 25,560 records\n",
      "  ‚úì After join: 21,254 records\n",
      "\n",
      "üîÑ Processing Location 7735...\n",
      "  üìä Air quality (PM2.5, PM10, SO2, NO2): 83,373 records\n",
      "  ‚úì After join: 21,254 records\n",
      "\n",
      "üîÑ Processing Location 7735...\n",
      "  üìä Air quality (PM2.5, PM10, SO2, NO2): 83,373 records\n",
      "  üå§Ô∏è  Weather: 25,560 records\n",
      "  üå§Ô∏è  Weather: 25,560 records\n",
      "  ‚úì After join: 21,277 records\n",
      "\n",
      "üîÑ Processing Location 7736...\n",
      "  üìä Air quality (PM2.5, PM10, SO2, NO2): 83,295 records\n",
      "  ‚úì After join: 21,277 records\n",
      "\n",
      "üîÑ Processing Location 7736...\n",
      "  üìä Air quality (PM2.5, PM10, SO2, NO2): 83,295 records\n",
      "  üå§Ô∏è  Weather: 25,560 records\n",
      "  üå§Ô∏è  Weather: 25,560 records\n",
      "  ‚úì After join: 21,281 records\n",
      "\n",
      "üîÑ Processing Location 7737...\n",
      "  üìä Air quality (PM2.5, PM10, SO2, NO2): 83,079 records\n",
      "  ‚úì After join: 21,281 records\n",
      "\n",
      "üîÑ Processing Location 7737...\n",
      "  üìä Air quality (PM2.5, PM10, SO2, NO2): 83,079 records\n",
      "  üå§Ô∏è  Weather: 25,560 records\n",
      "  üå§Ô∏è  Weather: 25,560 records\n",
      "  ‚úì After join: 21,275 records\n",
      "\n",
      "üîÑ Processing Location 7739...\n",
      "  üìä Air quality (PM2.5, PM10, SO2, NO2): 84,686 records\n",
      "  ‚úì After join: 21,275 records\n",
      "\n",
      "üîÑ Processing Location 7739...\n",
      "  üìä Air quality (PM2.5, PM10, SO2, NO2): 84,686 records\n",
      "  üå§Ô∏è  Weather: 25,560 records\n",
      "  üå§Ô∏è  Weather: 25,560 records\n",
      "  ‚úì After join: 21,728 records\n",
      "\n",
      "üîÑ Processing Location 7740...\n",
      "  üìä Air quality (PM2.5, PM10, SO2, NO2): 84,504 records\n",
      "  ‚úì After join: 21,728 records\n",
      "\n",
      "üîÑ Processing Location 7740...\n",
      "  üìä Air quality (PM2.5, PM10, SO2, NO2): 84,504 records\n",
      "  üå§Ô∏è  Weather: 25,560 records\n",
      "  üå§Ô∏è  Weather: 25,560 records\n",
      "  ‚úì After join: 21,694 records\n",
      "\n",
      "üîÑ Processing Location 7742...\n",
      "  üìä Air quality (PM2.5, PM10, SO2, NO2): 82,234 records\n",
      "  ‚úì After join: 21,694 records\n",
      "\n",
      "üîÑ Processing Location 7742...\n",
      "  üìä Air quality (PM2.5, PM10, SO2, NO2): 82,234 records\n",
      "  üå§Ô∏è  Weather: 25,560 records\n",
      "  üå§Ô∏è  Weather: 25,560 records\n",
      "  ‚úì After join: 21,094 records\n",
      "\n",
      "‚úÖ Processed 14 locations successfully!\n",
      "  ‚úì After join: 21,094 records\n",
      "\n",
      "‚úÖ Processed 14 locations successfully!\n"
     ]
    }
   ],
   "source": [
    "# List ƒë·ªÉ ch·ª©a dataframes c·ªßa t·ª´ng location\n",
    "all_locations_data = []\n",
    "\n",
    "for location_id, files in location_mapping.items():\n",
    "    print(f\"\\nüîÑ Processing Location {location_id}...\")\n",
    "    \n",
    "    # ƒê·ªçc pollutant data\n",
    "    df_air = spark.read.csv(\n",
    "        files['pollutant'],\n",
    "        header=True,\n",
    "        schema=openaq_schema\n",
    "    )\n",
    "    \n",
    "    # üîç L·ªåC CH·ªà L·∫§Y C√ÅC CH·ªà S·ªê QUAN T√ÇM: PM2.5, PM10, SO2, NO2\n",
    "    df_air = df_air.filter(\n",
    "        F.col(\"parameter\").isin([\"pm25\", \"pm10\", \"so2\", \"no2\"])\n",
    "    )\n",
    "    \n",
    "    # ƒê·ªçc weather data\n",
    "    df_weather = spark.read.csv(\n",
    "        files['weather'],\n",
    "        header=True,\n",
    "        schema=weather_schema\n",
    "    )\n",
    "    \n",
    "    print(f\"  üìä Air quality (PM2.5, PM10, SO2, NO2): {df_air.count():,} records\")\n",
    "    print(f\"  üå§Ô∏è  Weather: {df_weather.count():,} records\")\n",
    "    \n",
    "    # Weather data - drop missing (√≠t missing)\n",
    "    df_weather_clean = df_weather.na.drop()\n",
    "    \n",
    "    # Pivot pollutant data\n",
    "    df_air_pivot = df_air.groupBy(\n",
    "        \"location_id\", \"location\", \"datetime\", \"lat\", \"lon\"\n",
    "    ).pivot(\"parameter\").agg(F.first(\"value\"))\n",
    "    \n",
    "    # Rename columns\n",
    "    column_mapping = {\n",
    "        \"pm25\": \"PM2_5\",\n",
    "        \"pm10\": \"PM10\",\n",
    "        \"no2\": \"NO2\",\n",
    "        \"so2\": \"SO2\"\n",
    "    }\n",
    "    \n",
    "    for old_name, new_name in column_mapping.items():\n",
    "        if old_name in df_air_pivot.columns:\n",
    "            df_air_pivot = df_air_pivot.withColumnRenamed(old_name, new_name)\n",
    "    \n",
    "    # Join v·ªõi weather data (theo datetime)\n",
    "    df_location = df_air_pivot.join(\n",
    "        df_weather_clean,\n",
    "        df_air_pivot.datetime == df_weather_clean.time,\n",
    "        \"inner\"\n",
    "    ).drop(\"time\")\n",
    "    \n",
    "    print(f\"  ‚úì After join: {df_location.count():,} records\")\n",
    "    \n",
    "    # Th√™m v√†o list\n",
    "    all_locations_data.append(df_location)\n",
    "\n",
    "print(f\"\\n‚úÖ Processed {len(all_locations_data)} locations successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b156b356",
   "metadata": {},
   "source": [
    "### 2.3 G·ªôp t·∫•t c·∫£ Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "409a2995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Combining 14 locations...\n",
      "‚úÖ Combined dataset: 300,416 total records\n",
      "‚úÖ Combined dataset: 300,416 total records\n",
      "‚úÖ Number of locations: 14\n",
      "‚úÖ Number of locations: 14\n",
      "+-----------+------------+-------------------+--------+---------+----+----+-----+---+--------------+--------------------+--------------+------------------+----------------+-------------+\n",
      "|location_id|    location|           datetime|     lat|      lon| NO2|PM10|PM2_5|SO2|temperature_2m|relative_humidity_2m|wind_speed_10m|wind_direction_10m|surface_pressure|precipitation|\n",
      "+-----------+------------+-------------------+--------+---------+----+----+-----+---+--------------+--------------------+--------------+------------------+----------------+-------------+\n",
      "|     233335|North-245631|2022-11-01 00:00:00|22.49671|114.12824|12.9|53.2| 15.9|4.4|          23.9|                45.0|          24.3|              21.0|          1008.1|          0.0|\n",
      "|     233335|North-245631|2022-11-01 01:00:00|22.49671|114.12824|11.7|55.1| 16.4|4.2|          23.4|                47.0|          24.1|              20.0|          1007.7|          0.0|\n",
      "|     233335|North-245631|2022-11-01 02:00:00|22.49671|114.12824|12.7|56.3| 21.1|3.9|          23.1|                47.0|          23.9|              21.0|          1006.9|          0.0|\n",
      "|     233335|North-245631|2022-11-01 03:00:00|22.49671|114.12824|11.2|54.3| 17.4|4.0|          22.8|                48.0|          24.0|              26.0|          1006.3|          0.0|\n",
      "|     233335|North-245631|2022-11-01 04:00:00|22.49671|114.12824|10.9|50.9| 15.8|4.3|          22.6|                48.0|          25.6|              24.0|          1006.2|          0.0|\n",
      "|     233335|North-245631|2022-11-01 05:00:00|22.49671|114.12824|12.7|43.0| 19.0|3.9|          22.4|                48.0|          25.0|              23.0|          1006.0|          0.0|\n",
      "|     233335|North-245631|2022-11-01 06:00:00|22.49671|114.12824|23.2|34.1| 14.8|4.3|          22.2|                49.0|          24.7|              21.0|          1006.5|          0.0|\n",
      "|     233335|North-245631|2022-11-01 07:00:00|22.49671|114.12824|35.4|30.8| 13.4|4.3|          22.1|                50.0|          24.7|              23.0|          1007.1|          0.0|\n",
      "|     233335|North-245631|2022-11-01 08:00:00|22.49671|114.12824|35.7|32.7| 16.6|4.6|          22.8|                49.0|          20.0|              23.0|          1008.7|          0.0|\n",
      "|     233335|North-245631|2022-11-01 09:00:00|22.49671|114.12824|22.8|33.0| 13.3|4.6|          22.6|                49.0|          24.1|              17.0|          1009.2|          0.0|\n",
      "+-----------+------------+-------------------+--------+---------+----+----+-----+---+--------------+--------------------+--------------+------------------+----------------+-------------+\n",
      "only showing top 10 rows\n",
      "+-----------+------------+-------------------+--------+---------+----+----+-----+---+--------------+--------------------+--------------+------------------+----------------+-------------+\n",
      "|location_id|    location|           datetime|     lat|      lon| NO2|PM10|PM2_5|SO2|temperature_2m|relative_humidity_2m|wind_speed_10m|wind_direction_10m|surface_pressure|precipitation|\n",
      "+-----------+------------+-------------------+--------+---------+----+----+-----+---+--------------+--------------------+--------------+------------------+----------------+-------------+\n",
      "|     233335|North-245631|2022-11-01 00:00:00|22.49671|114.12824|12.9|53.2| 15.9|4.4|          23.9|                45.0|          24.3|              21.0|          1008.1|          0.0|\n",
      "|     233335|North-245631|2022-11-01 01:00:00|22.49671|114.12824|11.7|55.1| 16.4|4.2|          23.4|                47.0|          24.1|              20.0|          1007.7|          0.0|\n",
      "|     233335|North-245631|2022-11-01 02:00:00|22.49671|114.12824|12.7|56.3| 21.1|3.9|          23.1|                47.0|          23.9|              21.0|          1006.9|          0.0|\n",
      "|     233335|North-245631|2022-11-01 03:00:00|22.49671|114.12824|11.2|54.3| 17.4|4.0|          22.8|                48.0|          24.0|              26.0|          1006.3|          0.0|\n",
      "|     233335|North-245631|2022-11-01 04:00:00|22.49671|114.12824|10.9|50.9| 15.8|4.3|          22.6|                48.0|          25.6|              24.0|          1006.2|          0.0|\n",
      "|     233335|North-245631|2022-11-01 05:00:00|22.49671|114.12824|12.7|43.0| 19.0|3.9|          22.4|                48.0|          25.0|              23.0|          1006.0|          0.0|\n",
      "|     233335|North-245631|2022-11-01 06:00:00|22.49671|114.12824|23.2|34.1| 14.8|4.3|          22.2|                49.0|          24.7|              21.0|          1006.5|          0.0|\n",
      "|     233335|North-245631|2022-11-01 07:00:00|22.49671|114.12824|35.4|30.8| 13.4|4.3|          22.1|                50.0|          24.7|              23.0|          1007.1|          0.0|\n",
      "|     233335|North-245631|2022-11-01 08:00:00|22.49671|114.12824|35.7|32.7| 16.6|4.6|          22.8|                49.0|          20.0|              23.0|          1008.7|          0.0|\n",
      "|     233335|North-245631|2022-11-01 09:00:00|22.49671|114.12824|22.8|33.0| 13.3|4.6|          22.6|                49.0|          24.1|              17.0|          1009.2|          0.0|\n",
      "+-----------+------------+-------------------+--------+---------+----+----+-----+---+--------------+--------------------+--------------+------------------+----------------+-------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# G·ªôp t·∫•t c·∫£ locations l·∫°i\n",
    "print(f\"üîÑ Combining {len(all_locations_data)} locations...\")\n",
    "\n",
    "df_combined = all_locations_data[0]\n",
    "for df in all_locations_data[1:]:\n",
    "    df_combined = df_combined.union(df)\n",
    "\n",
    "print(f\"‚úÖ Combined dataset: {df_combined.count():,} total records\")\n",
    "print(f\"‚úÖ Number of locations: {df_combined.select('location_id').distinct().count()}\")\n",
    "\n",
    "# S·∫Øp x·∫øp theo location_id v√† datetime\n",
    "df_combined = df_combined.orderBy(\"location_id\", \"datetime\")\n",
    "\n",
    "df_combined.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3751add",
   "metadata": {},
   "source": [
    "## 3. T·ªïng quan Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ae5670b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Dataset Overview by Location:\n",
      "+-----------+--------------------+-----+\n",
      "|location_id|location            |count|\n",
      "+-----------+--------------------+-----+\n",
      "|233335     |North-245631        |21686|\n",
      "|233336     |Southern-245632     |21262|\n",
      "|7727       |Tung Chung-7727     |22367|\n",
      "|7728       |Mong Kok-7728       |21683|\n",
      "|7730       |Central/Western-7730|21274|\n",
      "|7732       |Causeway Bay-7732   |21306|\n",
      "|7733       |Sha Tin-7733        |21235|\n",
      "|7734       |Sham Shui Po-7734   |21254|\n",
      "|7735       |Kwun Tong-7735      |21277|\n",
      "|7736       |Kwai Chung-7736     |21281|\n",
      "|7737       |Tai Po-7737         |21275|\n",
      "|7739       |Yuen Long-7739      |21728|\n",
      "|7740       |Tsuen Wan-7740      |21694|\n",
      "|7742       |Tuen Mun-7742       |2375 |\n",
      "|7742       |Tuen Mun-932161     |18719|\n",
      "+-----------+--------------------+-----+\n",
      "\n",
      "\n",
      "üìÖ Time Range by Location:\n",
      "+-----------+--------------------+-----+\n",
      "|location_id|location            |count|\n",
      "+-----------+--------------------+-----+\n",
      "|233335     |North-245631        |21686|\n",
      "|233336     |Southern-245632     |21262|\n",
      "|7727       |Tung Chung-7727     |22367|\n",
      "|7728       |Mong Kok-7728       |21683|\n",
      "|7730       |Central/Western-7730|21274|\n",
      "|7732       |Causeway Bay-7732   |21306|\n",
      "|7733       |Sha Tin-7733        |21235|\n",
      "|7734       |Sham Shui Po-7734   |21254|\n",
      "|7735       |Kwun Tong-7735      |21277|\n",
      "|7736       |Kwai Chung-7736     |21281|\n",
      "|7737       |Tai Po-7737         |21275|\n",
      "|7739       |Yuen Long-7739      |21728|\n",
      "|7740       |Tsuen Wan-7740      |21694|\n",
      "|7742       |Tuen Mun-7742       |2375 |\n",
      "|7742       |Tuen Mun-932161     |18719|\n",
      "+-----------+--------------------+-----+\n",
      "\n",
      "\n",
      "üìÖ Time Range by Location:\n",
      "+-----------+-------------------+-------------------+-------+\n",
      "|location_id|start_date         |end_date           |records|\n",
      "+-----------+-------------------+-------------------+-------+\n",
      "|233335     |2022-11-01 00:00:00|2025-09-30 23:00:00|21686  |\n",
      "|233336     |2022-11-01 00:00:00|2025-09-30 23:00:00|21262  |\n",
      "|7727       |2022-11-01 00:00:00|2025-09-30 23:00:00|22367  |\n",
      "|7728       |2022-11-01 00:00:00|2025-09-30 23:00:00|21683  |\n",
      "|7730       |2022-11-01 00:00:00|2025-09-30 23:00:00|21274  |\n",
      "|7732       |2022-11-01 00:00:00|2025-09-30 23:00:00|21306  |\n",
      "|7733       |2022-11-01 00:00:00|2025-09-30 23:00:00|21235  |\n",
      "|7734       |2022-11-01 00:00:00|2025-09-30 23:00:00|21254  |\n",
      "|7735       |2022-11-01 00:00:00|2025-09-30 23:00:00|21277  |\n",
      "|7736       |2022-11-01 00:00:00|2025-09-30 23:00:00|21281  |\n",
      "|7737       |2022-11-01 00:00:00|2025-09-30 23:00:00|21275  |\n",
      "|7739       |2022-11-01 00:00:00|2025-09-30 23:00:00|21728  |\n",
      "|7740       |2022-11-01 00:00:00|2025-09-30 23:00:00|21694  |\n",
      "|7742       |2022-11-01 00:00:00|2025-09-30 23:00:00|21094  |\n",
      "+-----------+-------------------+-------------------+-------+\n",
      "\n",
      "+-----------+-------------------+-------------------+-------+\n",
      "|location_id|start_date         |end_date           |records|\n",
      "+-----------+-------------------+-------------------+-------+\n",
      "|233335     |2022-11-01 00:00:00|2025-09-30 23:00:00|21686  |\n",
      "|233336     |2022-11-01 00:00:00|2025-09-30 23:00:00|21262  |\n",
      "|7727       |2022-11-01 00:00:00|2025-09-30 23:00:00|22367  |\n",
      "|7728       |2022-11-01 00:00:00|2025-09-30 23:00:00|21683  |\n",
      "|7730       |2022-11-01 00:00:00|2025-09-30 23:00:00|21274  |\n",
      "|7732       |2022-11-01 00:00:00|2025-09-30 23:00:00|21306  |\n",
      "|7733       |2022-11-01 00:00:00|2025-09-30 23:00:00|21235  |\n",
      "|7734       |2022-11-01 00:00:00|2025-09-30 23:00:00|21254  |\n",
      "|7735       |2022-11-01 00:00:00|2025-09-30 23:00:00|21277  |\n",
      "|7736       |2022-11-01 00:00:00|2025-09-30 23:00:00|21281  |\n",
      "|7737       |2022-11-01 00:00:00|2025-09-30 23:00:00|21275  |\n",
      "|7739       |2022-11-01 00:00:00|2025-09-30 23:00:00|21728  |\n",
      "|7740       |2022-11-01 00:00:00|2025-09-30 23:00:00|21694  |\n",
      "|7742       |2022-11-01 00:00:00|2025-09-30 23:00:00|21094  |\n",
      "+-----------+-------------------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Th·ªëng k√™ theo location\n",
    "print(\"üìä Dataset Overview by Location:\")\n",
    "df_combined.groupBy(\"location_id\", \"location\").count().orderBy(\"location_id\").show(truncate=False)\n",
    "\n",
    "# Time range c·ªßa t·ª´ng location\n",
    "print(\"\\nüìÖ Time Range by Location:\")\n",
    "df_combined.groupBy(\"location_id\").agg(\n",
    "    F.min(\"datetime\").alias(\"start_date\"),\n",
    "    F.max(\"datetime\").alias(\"end_date\"),\n",
    "    F.count(\"*\").alias(\"records\")\n",
    ").orderBy(\"location_id\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "368fb50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Missing Values Summary:\n",
      "  NO2                      :    7,620 (  2.54%)\n",
      "  NO2                      :    7,620 (  2.54%)\n",
      "  PM10                     :    3,391 (  1.13%)\n",
      "  PM10                     :    3,391 (  1.13%)\n",
      "  PM2_5                    :   11,161 (  3.72%)\n",
      "  PM2_5                    :   11,161 (  3.72%)\n",
      "  SO2                      :    7,432 (  2.47%)\n",
      "  SO2                      :    7,432 (  2.47%)\n"
     ]
    }
   ],
   "source": [
    "# Ki·ªÉm tra missing values\n",
    "print(\"‚ö†Ô∏è  Missing Values Summary:\")\n",
    "for col_name in df_combined.columns:\n",
    "    null_count = df_combined.filter(F.col(col_name).isNull()).count()\n",
    "    total = df_combined.count()\n",
    "    pct = (null_count / total) * 100\n",
    "    if null_count > 0:  # Ch·ªâ hi·ªÉn th·ªã c·ªôt c√≥ missing\n",
    "        print(f\"  {col_name:25s}: {null_count:8,} ({pct:6.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f767ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Overall Statistics:\n",
      "+-------+------------------+------------------+------------------+-----------------+------------------+--------------------+------------------+------------------+\n",
      "|summary|             PM2_5|              PM10|               NO2|              SO2|    temperature_2m|relative_humidity_2m|    wind_speed_10m|     precipitation|\n",
      "+-------+------------------+------------------+------------------+-----------------+------------------+--------------------+------------------+------------------+\n",
      "|  count|            289255|            297025|            292796|           292984|            300416|              300416|            300416|            300416|\n",
      "|   mean| 15.68455722459414|25.423372443396968|39.196086012103976|3.720443778499876|23.179505752023783|   79.53433239241585|12.479650884107345|0.2698651203664358|\n",
      "| stddev|10.895508720844544|19.754274618263253| 26.13457654994178| 2.43761328690624| 5.546174423257824|  15.460363021119077| 6.232915473285466| 1.148102146212537|\n",
      "|    min|               0.0|               0.0|               0.0|              0.0|               1.9|                16.0|               0.0|               0.0|\n",
      "|    max|             182.5|             401.7|             292.6|             76.9|              36.5|               100.0|              86.6|              53.2|\n",
      "+-------+------------------+------------------+------------------+-----------------+------------------+--------------------+------------------+------------------+\n",
      "\n",
      "+-------+------------------+------------------+------------------+-----------------+------------------+--------------------+------------------+------------------+\n",
      "|summary|             PM2_5|              PM10|               NO2|              SO2|    temperature_2m|relative_humidity_2m|    wind_speed_10m|     precipitation|\n",
      "+-------+------------------+------------------+------------------+-----------------+------------------+--------------------+------------------+------------------+\n",
      "|  count|            289255|            297025|            292796|           292984|            300416|              300416|            300416|            300416|\n",
      "|   mean| 15.68455722459414|25.423372443396968|39.196086012103976|3.720443778499876|23.179505752023783|   79.53433239241585|12.479650884107345|0.2698651203664358|\n",
      "| stddev|10.895508720844544|19.754274618263253| 26.13457654994178| 2.43761328690624| 5.546174423257824|  15.460363021119077| 6.232915473285466| 1.148102146212537|\n",
      "|    min|               0.0|               0.0|               0.0|              0.0|               1.9|                16.0|               0.0|               0.0|\n",
      "|    max|             182.5|             401.7|             292.6|             76.9|              36.5|               100.0|              86.6|              53.2|\n",
      "+-------+------------------+------------------+------------------+-----------------+------------------+--------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Statistics t·ªïng quan\n",
    "print(\"üìà Overall Statistics:\")\n",
    "df_combined.select(\n",
    "    \"PM2_5\", \"PM10\", \"NO2\", \"SO2\",\n",
    "    \"temperature_2m\", \"relative_humidity_2m\", \"wind_speed_10m\", \"precipitation\"\n",
    ").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02439644",
   "metadata": {},
   "source": [
    "## 4. L√†m s·∫°ch D·ªØ li·ªáu\n",
    "\n",
    "**Quy tr√¨nh l√†m s·∫°ch:**\n",
    "1. **Lo·∫°i b·ªè Outliers tr∆∞·ªõc** - ƒê·ªÉ tr√°nh gi√° tr·ªã c·ª±c ƒëoan ·∫£nh h∆∞·ªüng ƒë·∫øn t√≠nh to√°n statistics\n",
    "2. **Fill Missing Values sau** - Imputation d·ª±a tr√™n d·ªØ li·ªáu ƒë√£ lo·∫°i b·ªè outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e126b6",
   "metadata": {},
   "source": [
    "### 4.1. Lo·∫°i b·ªè Outliers\n",
    "\n",
    "Lo·∫°i b·ªè c√°c gi√° tr·ªã c·ª±c ƒëoan tr∆∞·ªõc khi imputation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a8a917f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Outlier Removal:\n",
      "  Before: 300,416 records\n",
      "  After:  289,255 records\n",
      "  Removed: 11,161 records (3.72%)\n",
      "\n",
      "  ‚ö†Ô∏è  Note: Records with PM2.5 = null are REMOVED (target variable must have real values)\n",
      "\n",
      "‚ö†Ô∏è  Missing values after outlier removal:\n",
      "  PM2_5     :        0 (  0.00%) ‚úÖ (Target - must be 0%)\n",
      "  PM2_5     :        0 (  0.00%) ‚úÖ (Target - must be 0%)\n",
      "  PM10      :      296 (  0.10%)\n",
      "  PM10      :      296 (  0.10%)\n",
      "  NO2       :    7,369 (  2.55%)\n",
      "  SO2       :    7,186 (  2.48%)\n"
     ]
    }
   ],
   "source": [
    "# Lo·∫°i b·ªè outliers theo WHO/EPA International Standards (cho d·ªØ li·ªáu Hong Kong)\n",
    "# ‚ö†Ô∏è  QUAN TR·ªåNG: PM2.5 l√† TARGET variable - PH·∫¢I c√≥ gi√° tr·ªã th·∫≠t!\n",
    "#     ‚Üí Records c√≥ PM2.5 = null s·∫Ω B·ªä LO·∫†I B·ªé\n",
    "#     ‚Üí Ch·ªâ c√°c features kh√°c (PM10, NO2, SO2) m·ªõi ƒë∆∞·ª£c ph√©p null v√† impute sau\n",
    "\n",
    "df_no_outliers = df_combined.filter(\n",
    "    # üéØ TARGET: PM2.5 theo WHO Emergency threshold (kh√¥ng cho ph√©p null)\n",
    "    (F.col(\"PM2_5\").isNotNull()) & \n",
    "    (F.col(\"PM2_5\") >= 0) & (F.col(\"PM2_5\") < 250) &  # WHO Emergency: 250 Œºg/m¬≥\n",
    "    \n",
    "    # üìä FEATURES: WHO/EPA International Standards - Cho ph√©p null, ch·ªâ lo·∫°i outliers\n",
    "    ((F.col(\"PM10\").isNull()) | ((F.col(\"PM10\") >= 0) & (F.col(\"PM10\") < 430))) &  # WHO Emergency: 430 Œºg/m¬≥\n",
    "    ((F.col(\"NO2\").isNull()) | ((F.col(\"NO2\") >= 0) & (F.col(\"NO2\") < 400))) &     # WHO/EU: 400 Œºg/m¬≥ (1-hour)\n",
    "    ((F.col(\"SO2\").isNull()) | ((F.col(\"SO2\") >= 0) & (F.col(\"SO2\") < 500))) &     # WHO/EU: 500 Œºg/m¬≥ (10-min)\n",
    "    \n",
    "    # üå§Ô∏è WEATHER: WMO standards cho Hong Kong\n",
    "    (F.col(\"precipitation\") >= 0) & (F.col(\"precipitation\") < 100)  # WMO: 100mm/h extreme rain\n",
    ")\n",
    "\n",
    "records_before = df_combined.count()\n",
    "records_after = df_no_outliers.count()\n",
    "removed = records_before - records_after\n",
    "\n",
    "print(f\"üìä Outlier Removal:\")\n",
    "print(f\"  Before: {records_before:,} records\")\n",
    "print(f\"  After:  {records_after:,} records\")\n",
    "print(f\"  Removed: {removed:,} records ({removed/records_before*100:.2f}%)\")\n",
    "print(f\"\\n  ‚ö†Ô∏è  Note: Records with PM2.5 = null are REMOVED (target variable must have real values)\")\n",
    "\n",
    "# Ki·ªÉm tra missing values sau khi lo·∫°i outliers\n",
    "print(\"\\n‚ö†Ô∏è  Missing values after outlier removal:\")\n",
    "for col_name in [\"PM2_5\", \"PM10\", \"NO2\", \"SO2\"]:\n",
    "    if col_name in df_no_outliers.columns:\n",
    "        null_count = df_no_outliers.filter(F.col(col_name).isNull()).count()\n",
    "        total = df_no_outliers.count()\n",
    "        pct = (null_count / total) * 100\n",
    "        if null_count > 0:\n",
    "            print(f\"  {col_name:10s}: {null_count:8,} ({pct:6.2f}%)\")\n",
    "        elif col_name == \"PM2_5\":\n",
    "            print(f\"  {col_name:10s}: {null_count:8,} ({pct:6.2f}%) ‚úÖ (Target - must be 0%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0994f2",
   "metadata": {},
   "source": [
    "### 4.2. X·ª≠ l√Ω Missing Values (Interpolation)\n",
    "\n",
    "**Chi·∫øn l∆∞·ª£c Imputation cho Time Series:**\n",
    "- **PM2.5**: ƒê√£ lo·∫°i b·ªè t·∫•t c·∫£ records c√≥ null (target variable)\n",
    "- **PM10, NO2, SO2**: S·ª≠ d·ª•ng **Linear Interpolation** (t·ªët nh·∫•t cho time series)\n",
    "  - B∆∞·ªõc 1: **Linear Interpolation** - N·ªôi suy tuy·∫øn t√≠nh d·ª±a tr√™n gi√° tr·ªã tr∆∞·ªõc & sau\n",
    "  - B∆∞·ªõc 2: **Forward Fill** - X·ª≠ l√Ω missing ·ªü cu·ªëi chu·ªói (kh√¥ng c√≥ gi√° tr·ªã sau)\n",
    "  - B∆∞·ªõc 3: **Backward Fill** - X·ª≠ l√Ω missing ·ªü ƒë·∫ßu chu·ªói (kh√¥ng c√≥ gi√° tr·ªã tr∆∞·ªõc)\n",
    "  - B∆∞·ªõc 4: **Mean** - Backup cu·ªëi c√πng (n·∫øu c√≤n missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de82f80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Time Series Imputation Strategy (PySpark Native):\n",
      "   1. True Linear Interpolation - y = y‚ÇÅ + (y‚ÇÇ-y‚ÇÅ) √ó (t-t‚ÇÅ)/(t‚ÇÇ-t‚ÇÅ)\n",
      "   2. Forward Fill - If only prev value available\n",
      "   3. Backward Fill - If only next value available\n",
      "   4. Null - If no surrounding values (rare)\n",
      "\n",
      "   Columns to impute: ['PM10', 'NO2', 'SO2']\n",
      "   PM2.5 NOT imputed (target variable - already removed nulls)\n",
      "   üîí Safe: Window partitioned by location_id (no cross-location interpolation)\n",
      "\n",
      "‚ö†Ô∏è  Missing values BEFORE interpolation:\n",
      "  PM10      :      296 (  0.10%)\n",
      "  NO2       :    7,369 (  2.55%)\n",
      "  SO2       :    7,186 (  2.48%)\n"
     ]
    }
   ],
   "source": [
    "# Chi·∫øn l∆∞·ª£c Imputation cho Time Series Data\n",
    "# S·ª≠ d·ª•ng PySpark Window Functions - N·ªôi suy tuy·∫øn t√≠nh d·ª±a tr√™n kho·∫£ng c√°ch th·ªùi gian\n",
    "\n",
    "# List c√°c c·ªôt FEATURES c·∫ßn impute (KH√îNG bao g·ªìm PM2.5 - target variable)\n",
    "pollutant_cols = [\"PM10\", \"NO2\", \"SO2\"]  # ‚ö†Ô∏è Kh√¥ng c√≥ PM2.5!\n",
    "\n",
    "print(f\"üîÑ Time Series Imputation Strategy (PySpark Native):\")\n",
    "print(f\"   1. True Linear Interpolation - y = y‚ÇÅ + (y‚ÇÇ-y‚ÇÅ) √ó (t-t‚ÇÅ)/(t‚ÇÇ-t‚ÇÅ)\")\n",
    "print(f\"   2. Forward Fill - If only prev value available\")\n",
    "print(f\"   3. Backward Fill - If only next value available\")\n",
    "print(f\"   4. Null - If no surrounding values (rare)\")\n",
    "print(f\"\\n   Columns to impute: {pollutant_cols}\")\n",
    "print(f\"   PM2.5 NOT imputed (target variable - already removed nulls)\")\n",
    "print(f\"   üîí Safe: Window partitioned by location_id (no cross-location interpolation)\\n\")\n",
    "\n",
    "# Cache ƒë·ªÉ tƒÉng performance\n",
    "df_filled = df_no_outliers.cache()\n",
    "\n",
    "# Ki·ªÉm tra missing TR∆Ø·ªöC khi interpolate\n",
    "print(\"‚ö†Ô∏è  Missing values BEFORE interpolation:\")\n",
    "for col_name in pollutant_cols:\n",
    "    if col_name in df_filled.columns:\n",
    "        null_count = df_filled.filter(F.col(col_name).isNull()).count()\n",
    "        total = df_filled.count()\n",
    "        pct = (null_count / total) * 100\n",
    "        if null_count > 0:\n",
    "            print(f\"  {col_name:10s}: {null_count:8,} ({pct:6.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a622c600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Applying true linear interpolation per location (PySpark native)...\n",
      "  ‚ñ∂ Interpolating PM10... ‚úì\n",
      "  ‚ñ∂ Interpolating NO2... ‚úì\n",
      "  ‚ñ∂ Interpolating SO2... ‚úì\n",
      "\n",
      "‚úÖ Linear interpolation completed! Total records: 289,255\n",
      "   ‚öôÔ∏è  Method: True linear interpolation based on time distance (epoch)\n",
      "   üîí Safe: No cross-location interpolation (partitioned by location_id)\n",
      "   üöÄ Optimized: Native PySpark (no Pandas conversion)\n"
     ]
    }
   ],
   "source": [
    "# √Åp d·ª•ng True Linear Interpolation v·ªõi PySpark (kh√¥ng d√πng Pandas)\n",
    "# N·ªôi suy tuy·∫øn t√≠nh d·ª±a tr√™n kho·∫£ng c√°ch th·ªùi gian TH·ª∞C (epoch)\n",
    "# Window function ƒë·∫£m b·∫£o KH√îNG n·ªôi suy ch√©o gi·ªØa c√°c locations\n",
    "\n",
    "print(\"üîÑ Applying true linear interpolation per location (PySpark native)...\")\n",
    "\n",
    "# T·∫°o c·ªôt epoch (timestamp d·∫°ng s·ªë) ƒë·ªÉ t√≠nh to√°n kho·∫£ng c√°ch th·ªùi gian\n",
    "df_filled = df_filled.withColumn(\"epoch\", F.col(\"datetime\").cast(\"long\"))\n",
    "\n",
    "# ƒê·ªãnh nghƒ©a Window cho t·ª´ng location\n",
    "w_forward = (\n",
    "    Window.partitionBy(\"location_id\")\n",
    "    .orderBy(\"epoch\")\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    ")\n",
    "\n",
    "w_backward = (\n",
    "    Window.partitionBy(\"location_id\")\n",
    "    .orderBy(\"epoch\")\n",
    "    .rowsBetween(Window.currentRow, Window.unboundedFollowing)\n",
    ")\n",
    "\n",
    "# X·ª≠ l√Ω t·ª´ng pollutant column\n",
    "for col_name in pollutant_cols:\n",
    "    if col_name not in df_filled.columns:\n",
    "        continue\n",
    "    \n",
    "    print(f\"  ‚ñ∂ Interpolating {col_name}...\", end=\" \", flush=True)\n",
    "    \n",
    "    # B∆∞·ªõc 1: T√¨m gi√° tr·ªã & timestamp TR∆Ø·ªöC v√† SAU g·∫ßn nh·∫•t (c√≥ gi√° tr·ªã non-null)\n",
    "    df_filled = (\n",
    "        df_filled\n",
    "        .withColumn(f\"{col_name}_prev_value\", F.last(col_name, True).over(w_forward))\n",
    "        .withColumn(f\"{col_name}_next_value\", F.first(col_name, True).over(w_backward))\n",
    "        .withColumn(f\"{col_name}_prev_time\", F.last(F.when(F.col(col_name).isNotNull(), F.col(\"epoch\")), True).over(w_forward))\n",
    "        .withColumn(f\"{col_name}_next_time\", F.first(F.when(F.col(col_name).isNotNull(), F.col(\"epoch\")), True).over(w_backward))\n",
    "    )\n",
    "    \n",
    "    # B∆∞·ªõc 2: T√≠nh to√°n Linear Interpolation theo c√¥ng th·ª©c:\n",
    "    # y = y‚ÇÅ + (y‚ÇÇ - y‚ÇÅ) * (t - t‚ÇÅ) / (t‚ÇÇ - t‚ÇÅ)\n",
    "    interpolated_value = (\n",
    "        F.col(f\"{col_name}_prev_value\") +\n",
    "        (F.col(f\"{col_name}_next_value\") - F.col(f\"{col_name}_prev_value\")) *\n",
    "        ((F.col(\"epoch\") - F.col(f\"{col_name}_prev_time\")) /\n",
    "         (F.col(f\"{col_name}_next_time\") - F.col(f\"{col_name}_prev_time\")))\n",
    "    )\n",
    "    \n",
    "    # B∆∞·ªõc 3: Logic ch·ªçn gi√° tr·ªã cu·ªëi c√πng v·ªõi fallback\n",
    "    df_filled = df_filled.withColumn(\n",
    "        col_name,\n",
    "        F.when(F.col(col_name).isNotNull(), F.col(col_name))  # Gi·ªØ nguy√™n n·∫øu c√≥ gi√° tr·ªã\n",
    "         .when(\n",
    "             # Linear interpolation n·∫øu c√≥ c·∫£ prev & next v√† kh√¥ng chia 0\n",
    "             (F.col(f\"{col_name}_prev_value\").isNotNull()) &\n",
    "             (F.col(f\"{col_name}_next_value\").isNotNull()) &\n",
    "             ((F.col(f\"{col_name}_next_time\") - F.col(f\"{col_name}_prev_time\")) != 0),\n",
    "             interpolated_value\n",
    "         )\n",
    "         .when(F.col(f\"{col_name}_prev_value\").isNotNull(), F.col(f\"{col_name}_prev_value\"))  # Forward fill\n",
    "         .when(F.col(f\"{col_name}_next_value\").isNotNull(), F.col(f\"{col_name}_next_value\"))  # Backward fill\n",
    "         .otherwise(None)  # V·∫´n null n·∫øu kh√¥ng c√≥ data n√†o\n",
    "    )\n",
    "    \n",
    "    # B∆∞·ªõc 4: X√≥a c√°c c·ªôt ph·ª• ƒë·ªÉ gi·∫£m memory\n",
    "    df_filled = df_filled.drop(\n",
    "        f\"{col_name}_prev_value\", f\"{col_name}_next_value\",\n",
    "        f\"{col_name}_prev_time\", f\"{col_name}_next_time\"\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì\")\n",
    "\n",
    "# Cache k·∫øt qu·∫£ sau khi interpolation\n",
    "df_filled = df_filled.cache()\n",
    "\n",
    "# Trigger computation v√† ƒë·∫øm records\n",
    "count = df_filled.count()\n",
    "print(f\"\\n‚úÖ Linear interpolation completed! Total records: {count:,}\")\n",
    "print(f\"   ‚öôÔ∏è  Method: True linear interpolation based on time distance (epoch)\")\n",
    "print(f\"   üîí Safe: No cross-location interpolation (partitioned by location_id)\")\n",
    "print(f\"   üöÄ Optimized: Native PySpark (no Pandas conversion)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9001c1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Final Missing Values Check (After Interpolation):\n",
      "  PM2_5 (Target): 0 nulls ‚úÖ (Must be 0)\n",
      "  PM10      : 0 nulls ‚úÖ\n",
      "  NO2       : 0 nulls ‚úÖ\n",
      "  SO2       : 0 nulls ‚úÖ\n",
      "\n",
      "  ‚úÖ No missing values in any feature columns!\n",
      "\n",
      "üìä Final Verification:\n",
      "  PM2_5     : 0 nulls ‚úÖ\n",
      "  PM10      : 0 nulls ‚úÖ\n",
      "  NO2       : 0 nulls ‚úÖ\n",
      "  SO2       : 0 nulls ‚úÖ\n",
      "\n",
      "‚úÖ Data cleaning completed with True Linear Interpolation!\n",
      "   Final dataset: 289,255 records\n",
      "   ‚ö†Ô∏è  All records have REAL PM2.5 values (target variable)\n",
      "   ‚úÖ Features interpolated smoothly (time-based linear interpolation)\n",
      "   ‚úÖ Edge cases (no surrounding data) removed\n",
      "   üöÄ Performance: Native PySpark (no Pandas conversion)\n"
     ]
    }
   ],
   "source": [
    "# Verify: PM2.5 kh√¥ng c√≥ null, c√°c features kh√°c kh√¥ng c√≥ null\n",
    "print(\"\\nüìã Final Missing Values Check (After Interpolation):\")\n",
    "\n",
    "# Ki·ªÉm tra PM2.5 (target)\n",
    "pm25_nulls = df_filled.filter(F.col(\"PM2_5\").isNull()).count()\n",
    "print(f\"  PM2_5 (Target): {pm25_nulls:,} nulls ‚úÖ (Must be 0)\")\n",
    "\n",
    "# Ki·ªÉm tra features\n",
    "total_nulls = 0\n",
    "for col_name in pollutant_cols:\n",
    "    if col_name in df_filled.columns:\n",
    "        null_count = df_filled.filter(F.col(col_name).isNull()).count()\n",
    "        total_nulls += null_count\n",
    "        if null_count > 0:\n",
    "            print(f\"  {col_name:10s}: {null_count:,} nulls ‚ö†Ô∏è\")\n",
    "        else:\n",
    "            print(f\"  {col_name:10s}: {null_count:,} nulls ‚úÖ\")\n",
    "\n",
    "# X·ª≠ l√Ω edge case: Drop records c√≤n null (kh√¥ng c√≥ gi√° tr·ªã xung quanh ƒë·ªÉ interpolate)\n",
    "if total_nulls > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  Found {total_nulls} remaining nulls (edge cases with no surrounding data)\")\n",
    "    print(f\"   ‚Üí Dropping these records to ensure data quality...\")\n",
    "    \n",
    "    records_before_drop = df_filled.count()\n",
    "    \n",
    "    # Drop records c√≥ b·∫•t k·ª≥ feature n√†o c√≤n null\n",
    "    for col_name in pollutant_cols:\n",
    "        df_filled = df_filled.filter(F.col(col_name).isNotNull())\n",
    "    \n",
    "    records_after_drop = df_filled.count()\n",
    "    dropped = records_before_drop - records_after_drop\n",
    "    \n",
    "    print(f\"   Before drop: {records_before_drop:,} records\")\n",
    "    print(f\"   After drop:  {records_after_drop:,} records\")\n",
    "    print(f\"   Dropped:     {dropped:,} records ({dropped/records_before_drop*100:.2f}%)\")\n",
    "    print(f\"\\n   ‚úÖ All feature columns now have 0 nulls!\")\n",
    "else:\n",
    "    print(\"\\n  ‚úÖ No missing values in any feature columns!\")\n",
    "\n",
    "# X√≥a c·ªôt epoch (ƒë√£ d√πng xong)\n",
    "df_filled = df_filled.drop(\"epoch\")\n",
    "\n",
    "# Verify l·∫ßn cu·ªëi\n",
    "print(f\"\\nüìä Final Verification:\")\n",
    "for col_name in [\"PM2_5\"] + pollutant_cols:\n",
    "    if col_name in df_filled.columns:\n",
    "        null_count = df_filled.filter(F.col(col_name).isNull()).count()\n",
    "        print(f\"  {col_name:10s}: {null_count:,} nulls ‚úÖ\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data cleaning completed with True Linear Interpolation!\")\n",
    "print(f\"   Final dataset: {df_filled.count():,} records\")\n",
    "print(f\"   ‚ö†Ô∏è  All records have REAL PM2.5 values (target variable)\")\n",
    "print(f\"   ‚úÖ Features interpolated smoothly (time-based linear interpolation)\")\n",
    "print(f\"   ‚úÖ Edge cases (no surrounding data) removed\")\n",
    "print(f\"   üöÄ Performance: Native PySpark (no Pandas conversion)\")\n",
    "\n",
    "# C·∫≠p nh·∫≠t df_combined v·ªõi d·ªØ li·ªáu ƒë√£ clean v√† s·∫Øp x·∫øp\n",
    "df_combined = df_filled.orderBy(\"location_id\", \"datetime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7365dd0b",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering & Normalization\n",
    "\n",
    "**Quy tr√¨nh ƒê√öNG ƒë·ªÉ tr√°nh Data Leakage:**\n",
    "1. **Time Features** - Th√™m cyclic encoding (sin/cos) v√† is_weekend (kh√¥ng c·∫ßn normalize)\n",
    "2. **Temporal Split** - Chia train/validation/test theo th·ªùi gian (70/15/15)\n",
    "3. **Normalization** - Chu·∫©n h√≥a **CH·ªà numerical G·ªêC** b·∫±ng Min-Max t·ª´ train set\n",
    "4. **Lag Features** - T·∫°o lag T·ª™ C√ÅC C·ªòT ƒê√É SCALE (gi·ªØ ƒë√∫ng scale relationship)\n",
    "5. **Model-Specific Datasets** - Chu·∫©n b·ªã ri√™ng cho Deep Learning v√† XGBoost\n",
    "6. **Null Handling** - X·ª≠ l√Ω nulls trong lag features cu·ªëi c√πng\n",
    "\n",
    "**‚ö†Ô∏è QUAN TR·ªåNG:** Lag features ph·∫£i t·∫°o SAU khi normalize ƒë·ªÉ gi·ªØ ƒë√∫ng m·ªëi quan h·ªá scale!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03014c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Step 1: Adding Time Features...\n",
      "‚úì Time features added successfully!\n",
      "‚úì Time features added successfully!\n",
      "‚úì Total records: 289,050\n",
      "‚úì Total columns: 29\n",
      "\n",
      "üìã Available Features:\n",
      "  Pollutants: PM2_5, PM10, NO2, SO2\n",
      "  Weather: temperature_2m, relative_humidity_2m, wind_speed_10m, wind_direction_10m, surface_pressure, precipitation\n",
      "  Time (Linear): year, month, day, hour, day_of_week, day_of_year, week_of_year, is_weekend\n",
      "  Time (Cyclic): hour_sin, hour_cos, month_sin, month_cos, day_of_week_sin, day_of_week_cos\n",
      "‚úì Total records: 289,050\n",
      "‚úì Total columns: 29\n",
      "\n",
      "üìã Available Features:\n",
      "  Pollutants: PM2_5, PM10, NO2, SO2\n",
      "  Weather: temperature_2m, relative_humidity_2m, wind_speed_10m, wind_direction_10m, surface_pressure, precipitation\n",
      "  Time (Linear): year, month, day, hour, day_of_week, day_of_year, week_of_year, is_weekend\n",
      "  Time (Cyclic): hour_sin, hour_cos, month_sin, month_cos, day_of_week_sin, day_of_week_cos\n"
     ]
    }
   ],
   "source": [
    "# B∆∞·ªõc 1: Th√™m Time Features t·ª´ d·ªØ li·ªáu ƒë√£ clean\n",
    "print(\"üîÑ Step 1: Adding Time Features (No normalization needed)...\")\n",
    "\n",
    "import math\n",
    "\n",
    "df_features = df_combined \\\n",
    "    .withColumn(\"hour\", F.hour(\"datetime\")) \\\n",
    "    .withColumn(\"month\", F.month(\"datetime\")) \\\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(\"datetime\"))\n",
    "\n",
    "# Cyclic encoding cho hour (24h cycle)\n",
    "df_features = df_features \\\n",
    "    .withColumn(\"hour_sin\", F.sin(2 * math.pi * F.col(\"hour\") / 24)) \\\n",
    "    .withColumn(\"hour_cos\", F.cos(2 * math.pi * F.col(\"hour\") / 24))\n",
    "\n",
    "# Cyclic encoding cho month (12 month cycle)\n",
    "df_features = df_features \\\n",
    "    .withColumn(\"month_sin\", F.sin(2 * math.pi * F.col(\"month\") / 12)) \\\n",
    "    .withColumn(\"month_cos\", F.cos(2 * math.pi * F.col(\"month\") / 12))\n",
    "\n",
    "# Cyclic encoding cho day_of_week (7 day cycle)\n",
    "df_features = df_features \\\n",
    "    .withColumn(\"day_of_week_sin\", F.sin(2 * math.pi * F.col(\"day_of_week\") / 7)) \\\n",
    "    .withColumn(\"day_of_week_cos\", F.cos(2 * math.pi * F.col(\"day_of_week\") / 7))\n",
    "\n",
    "# Binary feature: is_weekend\n",
    "df_features = df_features \\\n",
    "    .withColumn(\"is_weekend\", F.when(F.col(\"day_of_week\").isin([1, 7]), 1).otherwise(0))\n",
    "\n",
    "# X√≥a c√°c c·ªôt trung gian\n",
    "df_features = df_features.drop(\"hour\", \"month\", \"day_of_week\")\n",
    "\n",
    "print(\"‚úì Time features added successfully!\")\n",
    "print(f\"‚úì Total records: {df_features.count():,}\")\n",
    "print(f\"‚úì Total columns: {len(df_features.columns)}\")\n",
    "\n",
    "print(\"\\nüìã Time Features Created:\")\n",
    "print(\"  Cyclic (sin/cos): hour, month, day_of_week ‚Üí Already in [-1, 1]\")\n",
    "print(\"  Binary: is_weekend ‚Üí Already in [0, 1]\")\n",
    "print(\"  ‚úÖ No normalization needed for time features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d159476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Step 2: Creating Lag Features for XGBoost model ONLY...\n",
      "\n",
      "üìã Creating lag features for XGBoost:\n",
      "   Deep Learning models (CNN1D-BLSTM, LSTM): No lags needed\n",
      "   XGBoost: 6 lags √ó 8 variables = 48 features\n",
      "  ‚úì Created PM2_5_lag1\n",
      "  ‚úì Created PM2_5_lag2\n",
      "  ‚úì Created PM2_5_lag3\n",
      "  ‚úì Created PM2_5_lag6\n",
      "  ‚úì Created PM2_5_lag12\n",
      "  ‚úì Created PM2_5_lag24\n",
      "  ‚úì Created PM10_lag1\n",
      "  ‚úì Created PM10_lag2\n",
      "  ‚úì Created PM10_lag3\n",
      "  ‚úì Created PM10_lag6\n",
      "  ‚úì Created PM10_lag3\n",
      "  ‚úì Created PM10_lag6\n",
      "  ‚úì Created PM10_lag12\n",
      "  ‚úì Created PM10_lag24\n",
      "  ‚úì Created NO2_lag1\n",
      "  ‚úì Created NO2_lag2\n",
      "  ‚úì Created NO2_lag3\n",
      "  ‚úì Created NO2_lag6\n",
      "  ‚úì Created NO2_lag12\n",
      "  ‚úì Created NO2_lag24\n",
      "  ‚úì Created SO2_lag1\n",
      "  ‚úì Created PM10_lag12\n",
      "  ‚úì Created PM10_lag24\n",
      "  ‚úì Created NO2_lag1\n",
      "  ‚úì Created NO2_lag2\n",
      "  ‚úì Created NO2_lag3\n",
      "  ‚úì Created NO2_lag6\n",
      "  ‚úì Created NO2_lag12\n",
      "  ‚úì Created NO2_lag24\n",
      "  ‚úì Created SO2_lag1\n",
      "  ‚úì Created SO2_lag2\n",
      "  ‚úì Created SO2_lag3\n",
      "  ‚úì Created SO2_lag2\n",
      "  ‚úì Created SO2_lag3\n",
      "  ‚úì Created SO2_lag6\n",
      "  ‚úì Created SO2_lag12\n",
      "  ‚úì Created SO2_lag24\n",
      "  ‚úì Created temperature_2m_lag1\n",
      "  ‚úì Created temperature_2m_lag2\n",
      "  ‚úì Created temperature_2m_lag3\n",
      "  ‚úì Created temperature_2m_lag6\n",
      "  ‚úì Created SO2_lag6\n",
      "  ‚úì Created SO2_lag12\n",
      "  ‚úì Created SO2_lag24\n",
      "  ‚úì Created temperature_2m_lag1\n",
      "  ‚úì Created temperature_2m_lag2\n",
      "  ‚úì Created temperature_2m_lag3\n",
      "  ‚úì Created temperature_2m_lag6\n",
      "  ‚úì Created temperature_2m_lag12\n",
      "  ‚úì Created temperature_2m_lag12\n",
      "  ‚úì Created temperature_2m_lag24\n",
      "  ‚úì Created relative_humidity_2m_lag1\n",
      "  ‚úì Created relative_humidity_2m_lag2\n",
      "  ‚úì Created relative_humidity_2m_lag3\n",
      "  ‚úì Created relative_humidity_2m_lag6\n",
      "  ‚úì Created relative_humidity_2m_lag12\n",
      "  ‚úì Created relative_humidity_2m_lag24\n",
      "  ‚úì Created temperature_2m_lag24\n",
      "  ‚úì Created relative_humidity_2m_lag1\n",
      "  ‚úì Created relative_humidity_2m_lag2\n",
      "  ‚úì Created relative_humidity_2m_lag3\n",
      "  ‚úì Created relative_humidity_2m_lag6\n",
      "  ‚úì Created relative_humidity_2m_lag12\n",
      "  ‚úì Created relative_humidity_2m_lag24\n",
      "  ‚úì Created wind_speed_10m_lag1\n",
      "  ‚úì Created wind_speed_10m_lag1\n",
      "  ‚úì Created wind_speed_10m_lag2\n",
      "  ‚úì Created wind_speed_10m_lag3\n",
      "  ‚úì Created wind_speed_10m_lag6\n",
      "  ‚úì Created wind_speed_10m_lag12\n",
      "  ‚úì Created wind_speed_10m_lag24\n",
      "  ‚úì Created precipitation_lag1\n",
      "  ‚úì Created precipitation_lag2\n",
      "  ‚úì Created wind_speed_10m_lag2\n",
      "  ‚úì Created wind_speed_10m_lag3\n",
      "  ‚úì Created wind_speed_10m_lag6\n",
      "  ‚úì Created wind_speed_10m_lag12\n",
      "  ‚úì Created wind_speed_10m_lag24\n",
      "  ‚úì Created precipitation_lag1\n",
      "  ‚úì Created precipitation_lag2\n",
      "  ‚úì Created precipitation_lag3\n",
      "  ‚úì Created precipitation_lag3\n",
      "  ‚úì Created precipitation_lag6\n",
      "  ‚úì Created precipitation_lag12\n",
      "  ‚úì Created precipitation_lag24\n",
      "\n",
      "‚úÖ Lag features created successfully!\n",
      "‚úÖ Added 48 lag features (FOR XGBOOST ONLY)\n",
      "‚úÖ Total columns now: 77\n",
      "\n",
      "üí° Note: Deep Learning models will use sequences instead of lags\n",
      "  ‚úì Created precipitation_lag6\n",
      "  ‚úì Created precipitation_lag12\n",
      "  ‚úì Created precipitation_lag24\n",
      "\n",
      "‚úÖ Lag features created successfully!\n",
      "‚úÖ Added 48 lag features (FOR XGBOOST ONLY)\n",
      "‚úÖ Total columns now: 77\n",
      "\n",
      "üí° Note: Deep Learning models will use sequences instead of lags\n"
     ]
    }
   ],
   "source": [
    "# B∆∞·ªõc 2: TEMPORAL SPLIT TR∆Ø·ªöC KHI NORMALIZE (Tr√°nh Data Leakage)\n",
    "print(\"\\nüîÑ Step 2: Temporal Train/Val/Test Split BEFORE Normalization...\")\n",
    "\n",
    "# T√≠nh to√°n ng√†y chia d·ª±a tr√™n percentile th·ªùi gian  \n",
    "time_stats = df_features.select(\n",
    "    F.min(\"datetime\").alias(\"min_time\"),\n",
    "    F.max(\"datetime\").alias(\"max_time\")\n",
    ").collect()[0]\n",
    "\n",
    "min_time = time_stats[\"min_time\"]\n",
    "max_time = time_stats[\"max_time\"]\n",
    "total_days = (max_time - min_time).days\n",
    "\n",
    "# 70% train, 15% validation, 15% test\n",
    "train_days = int(total_days * 0.70)\n",
    "val_days = int(total_days * 0.15)\n",
    "\n",
    "train_end = min_time + pd.Timedelta(days=train_days)\n",
    "val_end = train_end + pd.Timedelta(days=val_days)\n",
    "\n",
    "print(f\"üìÖ Temporal Split (Avoiding Data Leakage):\")\n",
    "print(f\"  üü¢ Train:      {min_time.strftime('%Y-%m-%d')} ‚Üí {train_end.strftime('%Y-%m-%d')} ({(train_end - min_time).days} days)\")\n",
    "print(f\"  üü° Validation: {train_end.strftime('%Y-%m-%d')} ‚Üí {val_end.strftime('%Y-%m-%d')} ({(val_end - train_end).days} days)\")\n",
    "print(f\"  üî¥ Test:       {val_end.strftime('%Y-%m-%d')} ‚Üí {max_time.strftime('%Y-%m-%d')} ({(max_time - val_end).days} days)\")\n",
    "\n",
    "# Split data - Count BEFORE caching to trigger evaluation\n",
    "df_train_raw = df_features.filter(F.col(\"datetime\") < train_end)\n",
    "df_val_raw = df_features.filter((F.col(\"datetime\") >= train_end) & (F.col(\"datetime\") < val_end))\n",
    "df_test_raw = df_features.filter(F.col(\"datetime\") >= val_end)\n",
    "\n",
    "train_count = df_train_raw.count()\n",
    "val_count = df_val_raw.count() \n",
    "test_count = df_test_raw.count()\n",
    "total_count = train_count + val_count + test_count\n",
    "\n",
    "print(f\"\\nüìä Split Results:\")\n",
    "print(f\"  üü¢ Train: {train_count:8,} ({train_count/total_count*100:.1f}%)\")\n",
    "print(f\"  üü° Val:   {val_count:8,} ({val_count/total_count*100:.1f}%)\")\n",
    "print(f\"  üî¥ Test:  {test_count:8,} ({test_count/total_count*100:.1f}%)\")\n",
    "\n",
    "# Now cache after counting (avoids double computation)\n",
    "df_train_raw = df_train_raw.cache()\n",
    "df_val_raw = df_val_raw.cache()\n",
    "df_test_raw = df_test_raw.cache()\n",
    "\n",
    "print(f\"\\n‚úÖ Temporal split completed!\")\n",
    "print(f\"   ‚ö†Ô∏è  Next: Normalize using TRAIN SET statistics ONLY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816ecee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Step 3: Temporal Train/Test Split BEFORE Normalization...\n",
      "üìÖ Temporal Split (Avoiding Data Leakage):\n",
      "  üü¢ Train:      2022-11-01 ‚Üí 2024-11-14 (744 days)\n",
      "  üü° Validation: 2024-11-14 ‚Üí 2025-04-22 (159 days)\n",
      "  üî¥ Test:       2025-04-22 ‚Üí 2025-09-30 (161 days)\n",
      "üìÖ Temporal Split (Avoiding Data Leakage):\n",
      "  üü¢ Train:      2022-11-01 ‚Üí 2024-11-14 (744 days)\n",
      "  üü° Validation: 2024-11-14 ‚Üí 2025-04-22 (159 days)\n",
      "  üî¥ Test:       2025-04-22 ‚Üí 2025-09-30 (161 days)\n",
      "\n",
      "üìä Split Results:\n",
      "  üü¢ Train:  187,532 (64.9%)\n",
      "  üü° Val:     49,831 (17.2%)\n",
      "  üî¥ Test:    51,687 (17.9%)\n",
      "\n",
      "‚úÖ Temporal split completed - Ready for proper normalization!\n",
      "\n",
      "üîÑ Step 4: Data Normalization using TRAIN SET ONLY (No Data Leakage)...\n",
      "üìä Normalizing 57 numerical features...\n",
      "   ‚ö†Ô∏è  Computing min/max from TRAIN SET ONLY (preventing data leakage)\n",
      "\n",
      "üìä Split Results:\n",
      "  üü¢ Train:  187,532 (64.9%)\n",
      "  üü° Val:     49,831 (17.2%)\n",
      "  üî¥ Test:    51,687 (17.9%)\n",
      "\n",
      "‚úÖ Temporal split completed - Ready for proper normalization!\n",
      "\n",
      "üîÑ Step 4: Data Normalization using TRAIN SET ONLY (No Data Leakage)...\n",
      "üìä Normalizing 57 numerical features...\n",
      "   ‚ö†Ô∏è  Computing min/max from TRAIN SET ONLY (preventing data leakage)\n",
      "  ‚úì PM2_5                         : [    0.00,   152.40] ‚Üí [0, 1]\n",
      "  ‚úì PM2_5                         : [    0.00,   152.40] ‚Üí [0, 1]\n",
      "  ‚úì PM10                          : [    0.00,   227.30] ‚Üí [0, 1]\n",
      "  ‚úì PM10                          : [    0.00,   227.30] ‚Üí [0, 1]\n",
      "  ‚úì NO2                           : [    0.00,   199.80] ‚Üí [0, 1]\n",
      "  ‚úì NO2                           : [    0.00,   199.80] ‚Üí [0, 1]\n",
      "  ‚úì SO2                           : [    0.00,    49.90] ‚Üí [0, 1]\n",
      "  ‚úì SO2                           : [    0.00,    49.90] ‚Üí [0, 1]\n",
      "  ‚úì temperature_2m                : [    1.90,    36.00] ‚Üí [0, 1]\n",
      "  ‚úì temperature_2m                : [    1.90,    36.00] ‚Üí [0, 1]\n",
      "  ‚úì relative_humidity_2m          : [   22.00,   100.00] ‚Üí [0, 1]\n",
      "  ‚úì relative_humidity_2m          : [   22.00,   100.00] ‚Üí [0, 1]\n",
      "  ‚úì wind_speed_10m                : [    0.00,    82.30] ‚Üí [0, 1]\n",
      "  ‚úì wind_speed_10m                : [    0.00,    82.30] ‚Üí [0, 1]\n",
      "  ‚úì wind_direction_10m            : [    0.00,   360.00] ‚Üí [0, 1]\n",
      "  ‚úì wind_direction_10m            : [    0.00,   360.00] ‚Üí [0, 1]\n",
      "  ‚úì precipitation                 : [    0.00,    53.20] ‚Üí [0, 1]\n",
      "  ‚úì precipitation                 : [    0.00,    53.20] ‚Üí [0, 1]\n",
      "  ‚úì PM2_5_lag1                    : [    0.00,   152.40] ‚Üí [0, 1]\n",
      "  ‚úì PM2_5_lag1                    : [    0.00,   152.40] ‚Üí [0, 1]\n",
      "  ‚úì PM2_5_lag2                    : [    0.00,   152.40] ‚Üí [0, 1]\n",
      "  ‚úì PM2_5_lag2                    : [    0.00,   152.40] ‚Üí [0, 1]\n",
      "  ‚úì PM2_5_lag3                    : [    0.00,   152.40] ‚Üí [0, 1]\n",
      "  ‚úì PM2_5_lag3                    : [    0.00,   152.40] ‚Üí [0, 1]\n",
      "  ‚úì PM2_5_lag6                    : [    0.00,   152.40] ‚Üí [0, 1]\n",
      "  ‚úì PM2_5_lag6                    : [    0.00,   152.40] ‚Üí [0, 1]\n",
      "  ‚úì PM2_5_lag12                   : [    0.00,   152.40] ‚Üí [0, 1]\n",
      "  ‚úì PM2_5_lag12                   : [    0.00,   152.40] ‚Üí [0, 1]\n",
      "  ‚úì PM2_5_lag24                   : [    0.00,   152.40] ‚Üí [0, 1]\n",
      "  ‚úì PM2_5_lag24                   : [    0.00,   152.40] ‚Üí [0, 1]\n",
      "  ‚úì PM10_lag1                     : [    0.00,   227.30] ‚Üí [0, 1]\n",
      "  ‚úì PM10_lag1                     : [    0.00,   227.30] ‚Üí [0, 1]\n",
      "  ‚úì PM10_lag2                     : [    0.00,   227.30] ‚Üí [0, 1]\n",
      "  ‚úì PM10_lag2                     : [    0.00,   227.30] ‚Üí [0, 1]\n",
      "  ‚úì PM10_lag3                     : [    0.00,   227.30] ‚Üí [0, 1]\n",
      "  ‚úì PM10_lag3                     : [    0.00,   227.30] ‚Üí [0, 1]\n",
      "  ‚úì PM10_lag6                     : [    0.00,   227.30] ‚Üí [0, 1]\n",
      "  ‚úì PM10_lag6                     : [    0.00,   227.30] ‚Üí [0, 1]\n",
      "  ‚úì PM10_lag12                    : [    0.00,   227.30] ‚Üí [0, 1]\n",
      "  ‚úì PM10_lag12                    : [    0.00,   227.30] ‚Üí [0, 1]\n",
      "  ‚úì PM10_lag24                    : [    0.00,   227.30] ‚Üí [0, 1]\n",
      "  ‚úì PM10_lag24                    : [    0.00,   227.30] ‚Üí [0, 1]\n",
      "  ‚úì NO2_lag1                      : [    0.00,   199.80] ‚Üí [0, 1]\n",
      "  ‚úì NO2_lag1                      : [    0.00,   199.80] ‚Üí [0, 1]\n",
      "  ‚úì NO2_lag2                      : [    0.00,   199.80] ‚Üí [0, 1]\n",
      "  ‚úì NO2_lag2                      : [    0.00,   199.80] ‚Üí [0, 1]\n",
      "  ‚úì NO2_lag3                      : [    0.00,   199.80] ‚Üí [0, 1]\n",
      "  ‚úì NO2_lag3                      : [    0.00,   199.80] ‚Üí [0, 1]\n",
      "  ‚úì NO2_lag6                      : [    0.00,   199.80] ‚Üí [0, 1]\n",
      "  ‚úì NO2_lag6                      : [    0.00,   199.80] ‚Üí [0, 1]\n",
      "  ‚úì NO2_lag12                     : [    0.00,   199.80] ‚Üí [0, 1]\n",
      "  ‚úì NO2_lag12                     : [    0.00,   199.80] ‚Üí [0, 1]\n",
      "  ‚úì NO2_lag24                     : [    0.00,   199.80] ‚Üí [0, 1]\n",
      "  ‚úì NO2_lag24                     : [    0.00,   199.80] ‚Üí [0, 1]\n",
      "  ‚úì SO2_lag1                      : [    0.00,    49.90] ‚Üí [0, 1]\n",
      "  ‚úì SO2_lag1                      : [    0.00,    49.90] ‚Üí [0, 1]\n",
      "  ‚úì SO2_lag2                      : [    0.00,    49.90] ‚Üí [0, 1]\n",
      "  ‚úì SO2_lag2                      : [    0.00,    49.90] ‚Üí [0, 1]\n",
      "  ‚úì SO2_lag3                      : [    0.00,    49.90] ‚Üí [0, 1]\n",
      "  ‚úì SO2_lag3                      : [    0.00,    49.90] ‚Üí [0, 1]\n",
      "  ‚úì SO2_lag6                      : [    0.00,    49.90] ‚Üí [0, 1]\n",
      "  ‚úì SO2_lag6                      : [    0.00,    49.90] ‚Üí [0, 1]\n",
      "  ‚úì SO2_lag12                     : [    0.00,    49.90] ‚Üí [0, 1]\n",
      "  ‚úì SO2_lag12                     : [    0.00,    49.90] ‚Üí [0, 1]\n",
      "  ‚úì SO2_lag24                     : [    0.00,    49.90] ‚Üí [0, 1]\n",
      "  ‚úì SO2_lag24                     : [    0.00,    49.90] ‚Üí [0, 1]\n",
      "  ‚úì temperature_2m_lag1           : [    1.90,    36.00] ‚Üí [0, 1]\n",
      "  ‚úì temperature_2m_lag1           : [    1.90,    36.00] ‚Üí [0, 1]\n",
      "  ‚úì temperature_2m_lag2           : [    1.90,    36.00] ‚Üí [0, 1]\n",
      "  ‚úì temperature_2m_lag2           : [    1.90,    36.00] ‚Üí [0, 1]\n",
      "  ‚úì temperature_2m_lag3           : [    1.90,    36.00] ‚Üí [0, 1]\n",
      "  ‚úì temperature_2m_lag3           : [    1.90,    36.00] ‚Üí [0, 1]\n",
      "  ‚úì temperature_2m_lag6           : [    1.90,    36.00] ‚Üí [0, 1]\n",
      "  ‚úì temperature_2m_lag6           : [    1.90,    36.00] ‚Üí [0, 1]\n",
      "  ‚úì temperature_2m_lag12          : [    1.90,    36.00] ‚Üí [0, 1]\n",
      "  ‚úì temperature_2m_lag12          : [    1.90,    36.00] ‚Üí [0, 1]\n",
      "  ‚úì temperature_2m_lag24          : [    1.90,    36.00] ‚Üí [0, 1]\n",
      "  ‚úì temperature_2m_lag24          : [    1.90,    36.00] ‚Üí [0, 1]\n",
      "  ‚úì relative_humidity_2m_lag1     : [   22.00,   100.00] ‚Üí [0, 1]\n",
      "  ‚úì relative_humidity_2m_lag1     : [   22.00,   100.00] ‚Üí [0, 1]\n",
      "  ‚úì relative_humidity_2m_lag2     : [   22.00,   100.00] ‚Üí [0, 1]\n",
      "  ‚úì relative_humidity_2m_lag2     : [   22.00,   100.00] ‚Üí [0, 1]\n",
      "  ‚úì relative_humidity_2m_lag3     : [   22.00,   100.00] ‚Üí [0, 1]\n",
      "  ‚úì relative_humidity_2m_lag3     : [   22.00,   100.00] ‚Üí [0, 1]\n",
      "  ‚úì relative_humidity_2m_lag6     : [   22.00,   100.00] ‚Üí [0, 1]\n",
      "  ‚úì relative_humidity_2m_lag6     : [   22.00,   100.00] ‚Üí [0, 1]\n",
      "  ‚úì relative_humidity_2m_lag12    : [   22.00,   100.00] ‚Üí [0, 1]\n",
      "  ‚úì relative_humidity_2m_lag12    : [   22.00,   100.00] ‚Üí [0, 1]\n",
      "  ‚úì relative_humidity_2m_lag24    : [   22.00,   100.00] ‚Üí [0, 1]\n",
      "  ‚úì relative_humidity_2m_lag24    : [   22.00,   100.00] ‚Üí [0, 1]\n",
      "  ‚úì wind_speed_10m_lag1           : [    0.00,    82.30] ‚Üí [0, 1]\n",
      "  ‚úì wind_speed_10m_lag1           : [    0.00,    82.30] ‚Üí [0, 1]\n",
      "  ‚úì wind_speed_10m_lag2           : [    0.00,    82.30] ‚Üí [0, 1]\n",
      "  ‚úì wind_speed_10m_lag2           : [    0.00,    82.30] ‚Üí [0, 1]\n",
      "  ‚úì wind_speed_10m_lag3           : [    0.00,    82.30] ‚Üí [0, 1]\n",
      "  ‚úì wind_speed_10m_lag3           : [    0.00,    82.30] ‚Üí [0, 1]\n",
      "  ‚úì wind_speed_10m_lag6           : [    0.00,    82.30] ‚Üí [0, 1]\n",
      "  ‚úì wind_speed_10m_lag6           : [    0.00,    82.30] ‚Üí [0, 1]\n",
      "  ‚úì wind_speed_10m_lag12          : [    0.00,    82.30] ‚Üí [0, 1]\n",
      "  ‚úì wind_speed_10m_lag12          : [    0.00,    82.30] ‚Üí [0, 1]\n",
      "  ‚úì wind_speed_10m_lag24          : [    0.00,    82.30] ‚Üí [0, 1]\n",
      "  ‚úì wind_speed_10m_lag24          : [    0.00,    82.30] ‚Üí [0, 1]\n",
      "  ‚úì precipitation_lag1            : [    0.00,    53.20] ‚Üí [0, 1]\n",
      "  ‚úì precipitation_lag1            : [    0.00,    53.20] ‚Üí [0, 1]\n",
      "  ‚úì precipitation_lag2            : [    0.00,    53.20] ‚Üí [0, 1]\n",
      "  ‚úì precipitation_lag2            : [    0.00,    53.20] ‚Üí [0, 1]\n",
      "  ‚úì precipitation_lag3            : [    0.00,    53.20] ‚Üí [0, 1]\n",
      "  ‚úì precipitation_lag3            : [    0.00,    53.20] ‚Üí [0, 1]\n",
      "  ‚úì precipitation_lag6            : [    0.00,    53.20] ‚Üí [0, 1]\n",
      "  ‚úì precipitation_lag6            : [    0.00,    53.20] ‚Üí [0, 1]\n",
      "  ‚úì precipitation_lag12           : [    0.00,    53.20] ‚Üí [0, 1]\n",
      "  ‚úì precipitation_lag12           : [    0.00,    53.20] ‚Üí [0, 1]\n",
      "  ‚úì precipitation_lag24           : [    0.00,    53.20] ‚Üí [0, 1]\n",
      "\n",
      "‚úÖ Scaler parameters computed from TRAIN SET only!\n",
      "‚úÖ Will apply same scaling to train/val/test sets\n",
      "\n",
      "üîÑ Applying scaling to all splits...\n",
      "  ‚úì precipitation_lag24           : [    0.00,    53.20] ‚Üí [0, 1]\n",
      "\n",
      "‚úÖ Scaler parameters computed from TRAIN SET only!\n",
      "‚úÖ Will apply same scaling to train/val/test sets\n",
      "\n",
      "üîÑ Applying scaling to all splits...\n",
      "‚úÖ Normalization completed without data leakage!\n",
      "   üìä Train/Val/Test all normalized using train statistics only\n",
      "‚úÖ Normalization completed without data leakage!\n",
      "   üìä Train/Val/Test all normalized using train statistics only\n"
     ]
    }
   ],
   "source": [
    "# B∆∞·ªõc 3: Normalize NUMERICAL G·ªêC (CH·ªà g·ªëc, KH√îNG c√≥ lag features)\n",
    "print(f\"\\nüîÑ Step 3: Normalize NUMERICAL BASE FEATURES using TRAIN SET ONLY...\")\n",
    "\n",
    "# ‚ö†Ô∏è QUAN TR·ªåNG: CH·ªà normalize c√°c c·ªôt G·ªêC, KH√îNG bao g·ªìm lag features\n",
    "# Lag features s·∫Ω t·∫°o SAU t·ª´ c√°c c·ªôt ƒë√£ scale\n",
    "numerical_base_cols = [\n",
    "    # Pollutants (current values only)\n",
    "    \"PM2_5\", \"PM10\", \"NO2\", \"SO2\",\n",
    "    # Weather features (current values only)\n",
    "    \"temperature_2m\", \"relative_humidity_2m\", \"wind_speed_10m\", \n",
    "    \"wind_direction_10m\", \"precipitation\"\n",
    "]\n",
    "\n",
    "print(f\"üìä Normalizing {len(numerical_base_cols)} BASE features (NO lag features yet)...\")\n",
    "print(f\"   Features to normalize: {numerical_base_cols}\")\n",
    "print(f\"   ‚ö†Ô∏è  Computing min/max from TRAIN SET ONLY (preventing data leakage)\")\n",
    "\n",
    "# T√≠nh min/max CH·ªà T·ª™ TRAIN SET\n",
    "scaler_params = {}\n",
    "\n",
    "for col_name in numerical_base_cols:\n",
    "    if col_name in df_train_raw.columns:\n",
    "        # CH·ªà D√ôNG TRAIN SET ƒê·ªÇ T√çNH MIN/MAX  \n",
    "        stats = df_train_raw.select(\n",
    "            F.min(col_name).alias(\"min\"),\n",
    "            F.max(col_name).alias(\"max\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        min_val = stats[\"min\"]\n",
    "        max_val = stats[\"max\"]\n",
    "        \n",
    "        # ‚ö†Ô∏è CRITICAL: Handle None values from null columns\n",
    "        if min_val is None or max_val is None:\n",
    "            print(f\"  ‚ö†Ô∏è  Skipping {col_name}: All values are null\")\n",
    "            continue\n",
    "        \n",
    "        # ‚ö†Ô∏è CRITICAL: Tr√°nh chia 0 khi min = max\n",
    "        if max_val == min_val:\n",
    "            max_val = min_val + 1\n",
    "        \n",
    "        scaler_params[col_name] = {\"min\": min_val, \"max\": max_val}\n",
    "        print(f\"  ‚úì {col_name:30s}: [{min_val:8.2f}, {max_val:8.2f}] ‚Üí [0, 1]\")\n",
    "\n",
    "print(f\"\\n‚úÖ Scaler parameters computed from TRAIN SET only!\")\n",
    "\n",
    "# √Åp d·ª•ng normalization cho t·∫•t c·∫£ splits\n",
    "def apply_scaling(df, scaler_params):\n",
    "    \"\"\"Apply Min-Max scaling using precomputed parameters\"\"\"\n",
    "    df_scaled = df\n",
    "    for col_name, params in scaler_params.items():\n",
    "        if col_name in df.columns:\n",
    "            min_val = params[\"min\"]\n",
    "            max_val = params[\"max\"]\n",
    "            df_scaled = df_scaled.withColumn(\n",
    "                f\"{col_name}_scaled\",\n",
    "                (F.col(col_name) - min_val) / (max_val - min_val)\n",
    "            )\n",
    "    return df_scaled\n",
    "\n",
    "print(f\"\\n\udd04 Applying Min-Max scaling [0, 1] to all splits...\")\n",
    "\n",
    "# Apply scaling and trigger computation\n",
    "df_train = apply_scaling(df_train_raw, scaler_params)\n",
    "df_val = apply_scaling(df_val_raw, scaler_params)\n",
    "df_test = apply_scaling(df_test_raw, scaler_params)\n",
    "\n",
    "# Trigger computation and cache\n",
    "_ = df_train.count()\n",
    "_ = df_val.count()\n",
    "_ = df_test.count()\n",
    "\n",
    "df_train = df_train.cache()\n",
    "df_val = df_val.cache()\n",
    "df_test = df_test.cache()\n",
    "\n",
    "# Unpersist raw versions to free memory\n",
    "df_train_raw.unpersist()\n",
    "df_val_raw.unpersist()\n",
    "df_test_raw.unpersist()\n",
    "\n",
    "print(f\"‚úÖ Base feature normalization completed!\")\n",
    "print(f\"   üìä All splits normalized using train statistics only\")\n",
    "print(f\"   ‚ö†Ô∏è  Next: Create lag features FROM SCALED COLUMNS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1c6a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Scaler parameters saved to: ..\\data\\processed\\scaler_params.json\n",
      "   (Computed from TRAIN SET only - no data leakage)\n",
      "   (D√πng ƒë·ªÉ denormalize predictions khi inference)\n",
      "\n",
      "üìã Example scaler params (from train set):\n",
      "  PM2_5               : min=0.00, max=152.40\n",
      "  temperature_2m      : min=1.90, max=36.00\n",
      "  wind_speed_10m      : min=0.00, max=82.30\n"
     ]
    }
   ],
   "source": [
    "# B∆∞·ªõc 4: L∆∞u Scaler Parameters\n",
    "print(f\"\\nüíæ Step 4: Saving Scaler Parameters...\")\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "scaler_json = {\n",
    "    col: {\"min\": float(params[\"min\"]), \"max\": float(params[\"max\"])} \n",
    "    for col, params in scaler_params.items()\n",
    "}\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c processed n·∫øu ch∆∞a c√≥\n",
    "processed_dir = Path(\"../data/processed\")\n",
    "processed_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# L∆∞u ra file JSON\n",
    "scaler_path = processed_dir / \"scaler_params.json\"\n",
    "with open(scaler_path, 'w') as f:\n",
    "    json.dump(scaler_json, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Scaler parameters saved to: {scaler_path}\")\n",
    "print(f\"   - Computed from TRAIN SET only (no data leakage)\")\n",
    "print(f\"   - Used for denormalizing predictions during inference\")\n",
    "print(f\"   - Contains {len(scaler_params)} base features\")\n",
    "\n",
    "# Hi·ªÉn th·ªã v√≠ d·ª•\n",
    "print(f\"\\nüìã Example scaler params (from train set):\")\n",
    "example_cols = [\"PM2_5\", \"temperature_2m\", \"wind_speed_10m\"]\n",
    "for col in example_cols:\n",
    "    if col in scaler_params:\n",
    "        params = scaler_params[col]\n",
    "        print(f\"  {col:20s}: min={params['min']:.2f}, max={params['max']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e754fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B∆∞·ªõc 5: T·∫°o Lag Features T·ª™ C√ÅC C·ªòT ƒê√É SCALE (CH·ªà CHO XGBOOST)\n",
    "print(f\"\\nüîÑ Step 5: Creating Lag Features FROM SCALED COLUMNS (XGBoost only)...\")\n",
    "\n",
    "# ‚ö†Ô∏è QUAN TR·ªåNG: Lag features ƒë∆∞·ª£c t·∫°o T·ª™ C√ÅC C·ªòT ƒê√É SCALE\n",
    "# ‚Üí ƒê·∫£m b·∫£o lag v√† g·ªëc c√≥ C√ôNG SCALE PARAMETERS\n",
    "# ‚Üí Gi·ªØ ƒë√∫ng m·ªëi quan h·ªá gi·ªØa gi√° tr·ªã hi·ªán t·∫°i v√† qu√° kh·ª©\n",
    "\n",
    "LAG_STEPS = [1, 2, 3, 6, 12, 24]  # 1h, 2h, 3h, 6h, 12h, 24h tr∆∞·ªõc\n",
    "\n",
    "# Columns c·∫ßn t·∫°o lag (s·ª≠ d·ª•ng b·∫£n SCALED)\n",
    "lag_base_columns = [\"PM2_5\", \"PM10\", \"NO2\", \"SO2\", \n",
    "                    \"temperature_2m\", \"relative_humidity_2m\", \n",
    "                    \"wind_speed_10m\", \"precipitation\"]\n",
    "\n",
    "print(f\"\\nüìã Creating lag features:\")\n",
    "print(f\"   Deep Learning models: No lags needed (learn from sequences)\")\n",
    "print(f\"   XGBoost: {len(LAG_STEPS)} lags √ó {len(lag_base_columns)} variables = {len(LAG_STEPS) * len(lag_base_columns)} features\")\n",
    "print(f\"   ‚úÖ Using SCALED columns as source (proper scale relationship)\")\n",
    "\n",
    "# Window cho t·ª´ng location (s·∫Øp x·∫øp theo th·ªùi gian)\n",
    "w_lag = Window.partitionBy(\"location_id\").orderBy(\"datetime\")\n",
    "\n",
    "# T·∫°o lag features cho t·ª´ng split (train, val, test)\n",
    "def create_lag_features(df, lag_base_columns, lag_steps):\n",
    "    \"\"\"Create lag features from SCALED columns\"\"\"\n",
    "    df_with_lags = df\n",
    "    \n",
    "    for col_name in lag_base_columns:\n",
    "        col_scaled = f\"{col_name}_scaled\"\n",
    "        \n",
    "        if col_scaled in df.columns:\n",
    "            for lag in lag_steps:\n",
    "                lag_col_name = f\"{col_name}_lag{lag}_scaled\"\n",
    "                \n",
    "                # ‚úÖ T·∫°o lag T·ª™ C·ªòT ƒê√É SCALE\n",
    "                df_with_lags = df_with_lags.withColumn(\n",
    "                    lag_col_name,\n",
    "                    F.lag(col_scaled, lag).over(w_lag)\n",
    "                )\n",
    "    \n",
    "    return df_with_lags\n",
    "\n",
    "# Apply to all splits\n",
    "print(f\"\\nüîÑ Creating lag features for all splits...\")\n",
    "df_train = create_lag_features(df_train, lag_base_columns, LAG_STEPS)\n",
    "df_val = create_lag_features(df_val, lag_base_columns, LAG_STEPS)\n",
    "df_test = create_lag_features(df_test, lag_base_columns, LAG_STEPS)\n",
    "\n",
    "print(f\"  ‚úì Train: {len(LAG_STEPS) * len(lag_base_columns)} lag features created\")\n",
    "print(f\"  ‚úì Val:   {len(LAG_STEPS) * len(lag_base_columns)} lag features created\")\n",
    "print(f\"  ‚úì Test:  {len(LAG_STEPS) * len(lag_base_columns)} lag features created\")\n",
    "\n",
    "# Trigger computation and cache\n",
    "_ = df_train.count()\n",
    "_ = df_val.count()\n",
    "_ = df_test.count()\n",
    "\n",
    "df_train = df_train.cache()\n",
    "df_val = df_val.cache()\n",
    "df_test = df_test.cache()\n",
    "\n",
    "print(f\"\\n‚úÖ Lag features created successfully!\")\n",
    "print(f\"   ‚úÖ All lags created FROM SCALED columns\")\n",
    "print(f\"   ‚úÖ Lag and base features have SAME scale parameters\")\n",
    "print(f\"   ‚úÖ Proper temporal relationship preserved\")\n",
    "\n",
    "# ========================================\n",
    "# X·ª¨ L√ù NULL VALUES TRONG LAG FEATURES\n",
    "# ========================================\n",
    "print(f\"\\nüîÑ Handling null values in lag features...\")\n",
    "\n",
    "# T·∫°o list t·∫•t c·∫£ lag feature names\n",
    "lag_feature_names = [f\"{col}_lag{lag}_scaled\" for col in lag_base_columns for lag in LAG_STEPS]\n",
    "\n",
    "# ƒê·∫øm nulls TR∆Ø·ªöC khi x·ª≠ l√Ω\n",
    "print(f\"\\nüìä Null counts BEFORE handling:\")\n",
    "sample_lag_features = lag_feature_names[:3]\n",
    "for lag_col in sample_lag_features:\n",
    "    if lag_col in df_train.columns:\n",
    "        null_count = df_train.filter(F.col(lag_col).isNull()).count()\n",
    "        total_count = df_train.count()\n",
    "        print(f\"  {lag_col:35s}: {null_count:8,} nulls ({null_count/total_count*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Reason: First {max(LAG_STEPS)} hours of each location have no previous data\")\n",
    "print(f\"   Strategy: DROP records with ANY null lag feature\")\n",
    "\n",
    "# Track counts before drop\n",
    "train_before = df_train.count()\n",
    "val_before = df_val.count()\n",
    "test_before = df_test.count()\n",
    "\n",
    "# Function to drop nulls\n",
    "def drop_lag_nulls(df, lag_features):\n",
    "    \"\"\"Drop records with any null lag feature\"\"\"\n",
    "    df_clean = df\n",
    "    for col in lag_features:\n",
    "        if col in df.columns:\n",
    "            df_clean = df_clean.filter(F.col(col).isNotNull())\n",
    "    return df_clean\n",
    "\n",
    "# Apply to all splits\n",
    "print(f\"\\nüóëÔ∏è  Dropping records with null lag features...\")\n",
    "df_train_clean = drop_lag_nulls(df_train, lag_feature_names)\n",
    "df_val_clean = drop_lag_nulls(df_val, lag_feature_names)\n",
    "df_test_clean = drop_lag_nulls(df_test, lag_feature_names)\n",
    "\n",
    "# Count after\n",
    "train_after = df_train_clean.count()\n",
    "val_after = df_val_clean.count()\n",
    "test_after = df_test_clean.count()\n",
    "\n",
    "# Cache cleaned datasets\n",
    "df_train_clean = df_train_clean.cache()\n",
    "df_val_clean = df_val_clean.cache()\n",
    "df_test_clean = df_test_clean.cache()\n",
    "\n",
    "# Unpersist old ones\n",
    "df_train.unpersist()\n",
    "df_val.unpersist()\n",
    "df_test.unpersist()\n",
    "\n",
    "# Reassign\n",
    "df_train = df_train_clean\n",
    "df_val = df_val_clean\n",
    "df_test = df_test_clean\n",
    "\n",
    "print(f\"\\nüìä Records dropped (null lag features):\")\n",
    "print(f\"  üü¢ Train: {train_before:,} ‚Üí {train_after:,} (dropped {train_before - train_after:,}, {(train_before - train_after)/train_before*100:.2f}%)\")\n",
    "print(f\"  üü° Val:   {val_before:,} ‚Üí {val_after:,} (dropped {val_before - val_after:,}, {(val_before - val_after)/val_before*100:.2f}%)\")\n",
    "print(f\"  üî¥ Test:  {test_before:,} ‚Üí {test_after:,} (dropped {test_before - test_after:,}, {(test_before - test_after)/test_before*100:.2f}%)\")\n",
    "\n",
    "# Verify no nulls\n",
    "print(f\"\\n‚úÖ Verification - checking for remaining nulls...\")\n",
    "sample_check = lag_feature_names[:3]\n",
    "total_nulls_after = 0\n",
    "for lag_col in sample_check:\n",
    "    if lag_col in df_train.columns:\n",
    "        null_count = df_train.filter(F.col(lag_col).isNull()).count()\n",
    "        total_nulls_after += null_count\n",
    "        status = \"‚úÖ\" if null_count == 0 else \"‚ùå\"\n",
    "        print(f\"  {lag_col:35s}: {null_count:8,} nulls {status}\")\n",
    "\n",
    "if total_nulls_after == 0:\n",
    "    print(f\"\\n‚úÖ All lag features are clean!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Still {total_nulls_after} nulls found!\")\n",
    "\n",
    "print(f\"\\n‚úÖ Lag features + Null handling completed!\")\n",
    "print(f\"   - Created {len(lag_feature_names)} lag features FROM SCALED columns\")\n",
    "print(f\"   - Lost only first {max(LAG_STEPS)} hours per location\")\n",
    "print(f\"   - All lag features now have valid values\")\n",
    "print(f\"   - Data quality ensured for XGBoost training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45295509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Step 5: Preparing Model-Specific Features...\n",
      "üß† DEEP LEARNING Features: 15 features\n",
      "   - Current pollutants: 3\n",
      "   - Weather: 5\n",
      "   - Time (cyclic): 6\n",
      "   - Time (linear): 1\n",
      "   - NO LAG FEATURES (models learn from sequences)\n",
      "\n",
      "üìä XGBOOST Features: 63 features\n",
      "   - Deep Learning base features: 15\n",
      "   - Lag features: 48\n",
      "   - Total: 63 features\n",
      "\n",
      "‚úÖ Model-specific features prepared:\n",
      "  üß† CNN1D-BLSTM-Attention: 15 features\n",
      "  üß† LSTM: 15 features\n",
      "  üìä XGBoost: 63 features\n",
      "  üéØ Target: PM2_5_scaled\n",
      "\n",
      "‚úÖ All feature columns exist in datasets!\n"
     ]
    }
   ],
   "source": [
    "# B∆∞·ªõc 6: Chu·∫©n b·ªã Features cho t·ª´ng Model\n",
    "print(\"\\nüîÑ Step 6: Preparing Model-Specific Features...\")\n",
    "\n",
    "# ========================================\n",
    "# FEATURES CHO DEEP LEARNING MODELS (CNN1D-BLSTM, LSTM)\n",
    "# ========================================\n",
    "# Kh√¥ng c·∫ßn lag features v√¨ models t·ª± h·ªçc temporal patterns t·ª´ sequences\n",
    "\n",
    "dl_input_features = []\n",
    "\n",
    "# 1. Pollutants scaled (tr·ª´ PM2_5 - ƒë√¢y l√† target)\n",
    "dl_input_features.extend([\"PM10_scaled\", \"NO2_scaled\", \"SO2_scaled\"])\n",
    "\n",
    "# 2. Weather features scaled (core features)\n",
    "dl_input_features.extend([\n",
    "    \"temperature_2m_scaled\", \"relative_humidity_2m_scaled\", \n",
    "    \"wind_speed_10m_scaled\", \"wind_direction_10m_scaled\", \"precipitation_scaled\"\n",
    "])\n",
    "\n",
    "# 3. Time features (cyclic encoding - ƒë√£ ·ªü d·∫°ng sin/cos trong [-1, 1])\n",
    "dl_input_features.extend([\n",
    "    \"hour_sin\", \"hour_cos\", \n",
    "    \"month_sin\", \"month_cos\",\n",
    "    \"day_of_week_sin\", \"day_of_week_cos\"\n",
    "])\n",
    "\n",
    "# 4. Time features (binary)\n",
    "dl_input_features.extend([\"is_weekend\"])\n",
    "\n",
    "print(f\"üß† DEEP LEARNING Features: {len(dl_input_features)} features\")\n",
    "print(f\"   - Current pollutants (scaled): 3\")\n",
    "print(f\"   - Weather (scaled): 5\") \n",
    "print(f\"   - Time (cyclic): 6\")\n",
    "print(f\"   - Time (binary): 1\")\n",
    "print(f\"   - NO LAG FEATURES (models learn from sequences)\")\n",
    "\n",
    "# ========================================  \n",
    "# FEATURES CHO XGBOOST\n",
    "# ========================================\n",
    "# C·∫ßn lag features v√¨ kh√¥ng c√≥ kh·∫£ nƒÉng x·ª≠ l√Ω sequences\n",
    "\n",
    "xgb_input_features = dl_input_features.copy()  # Start with DL features\n",
    "\n",
    "# Th√™m lag features CH·ªà CHO XGBOOST (ƒë√£ ƒë∆∞·ª£c t·∫°o t·ª´ scaled columns)\n",
    "for col_name in lag_base_columns:\n",
    "    for lag in LAG_STEPS:\n",
    "        lag_col_name = f\"{col_name}_lag{lag}_scaled\"\n",
    "        xgb_input_features.append(lag_col_name)\n",
    "\n",
    "print(f\"\\nüìä XGBOOST Features: {len(xgb_input_features)} features\")\n",
    "print(f\"   - Deep Learning base features: {len(dl_input_features)}\")\n",
    "print(f\"   - Lag features (from scaled columns): {len(lag_base_columns) * len(LAG_STEPS)}\")\n",
    "print(f\"   - Total: {len(xgb_input_features)} features\")\n",
    "\n",
    "# Target variable (ƒë√£ scaled)\n",
    "target_feature = \"PM2_5_scaled\"\n",
    "\n",
    "print(f\"\\n‚úÖ Model-specific features prepared:\")\n",
    "print(f\"  üß† CNN1D-BLSTM-Attention: {len(dl_input_features)} features\")\n",
    "print(f\"  üß† LSTM: {len(dl_input_features)} features\")  \n",
    "print(f\"  üìä XGBoost: {len(xgb_input_features)} features\")\n",
    "print(f\"  üéØ Target: {target_feature}\")\n",
    "\n",
    "# ‚ö†Ô∏è CRITICAL: Verify ALL columns exist\n",
    "missing_dl = [col for col in dl_input_features if col not in df_train.columns]\n",
    "missing_xgb = [col for col in xgb_input_features if col not in df_train.columns]\n",
    "missing_target = target_feature not in df_train.columns\n",
    "\n",
    "if missing_dl or missing_xgb or missing_target:\n",
    "    print(f\"\\n‚ùå MISSING COLUMNS DETECTED:\")\n",
    "    if missing_dl: \n",
    "        print(f\"  DL models: {missing_dl}\")\n",
    "    if missing_xgb: \n",
    "        print(f\"  XGBoost: {missing_xgb[:5]}...\")  # Show first 5\n",
    "    if missing_target:\n",
    "        print(f\"  Target: {target_feature}\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  Available scaled columns:\")\n",
    "    scaled_cols = [c for c in df_train.columns if c.endswith('_scaled')]\n",
    "    print(f\"  {scaled_cols[:10]}...\")\n",
    "    \n",
    "    raise ValueError(\"Missing required feature columns! Check normalization step.\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ All feature columns exist in datasets!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700d4016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä FEATURE ENGINEERING SUMMARY\n",
      "======================================================================\n",
      "\n",
      "1Ô∏è‚É£ Dataset Statistics:\n",
      "   - Total records: 289,050\n",
      "   - Total records: 289,050\n",
      "   - Total locations: 14\n",
      "   - Time range: 2022-11-01 ‚Üí 2025-09-30\n",
      "\n",
      "2Ô∏è‚É£ Feature Categories:\n",
      "   üìå Deep Learning Features: 15 features\n",
      "      - Current Pollutants (scaled): PM10, NO2, SO2\n",
      "      - Current Weather (scaled): temperature_2m, relative_humidity_2m, wind_speed_10m, wind_direction_10m, precipitation\n",
      "      - Time Features (cyclic): hour_sin/cos, month_sin/cos, day_of_week_sin/cos\n",
      "      - Time Features (linear): is_weekend\n",
      "   üìå XGBoost Features: 63 features\n",
      "      - Deep Learning features: 15\n",
      "      - Lag Features: 48 features\n",
      "      - Variables: PM2_5, PM10, NO2, SO2, temperature_2m, relative_humidity_2m, wind_speed_10m, precipitation\n",
      "      - Lag steps: [1, 2, 3, 6, 12, 24]\n",
      "\n",
      "3Ô∏è‚É£ Target Variable:\n",
      "   üéØ PM2_5_scaled (normalized PM2.5)\n",
      "\n",
      "4Ô∏è‚É£ Normalization:\n",
      "   ‚úÖ All numerical features scaled to [0, 1] using train set only\n",
      "   ‚úÖ Scaler params saved to: scaler_params.json\n",
      "   ‚úÖ No data leakage (temporal split before normalization)\n",
      "\n",
      "5Ô∏è‚É£ Data Quality:\n",
      "   ‚úÖ No missing values in target\n",
      "   ‚úÖ No missing values in features\n",
      "   ‚úÖ Outliers removed\n",
      "   ‚úÖ Time series continuous\n",
      "\n",
      "6Ô∏è‚É£ Model-Specific Datasets:\n",
      "   üß† Deep Learning (CNN1D-BLSTM & LSTM): 15 base features\n",
      "   üìä XGBoost: 63 features (with lag features)\n",
      "\n",
      "üíæ Feature metadata saved to: ..\\data\\processed\\feature_metadata.json\n",
      "======================================================================\n",
      "   - Total locations: 14\n",
      "   - Time range: 2022-11-01 ‚Üí 2025-09-30\n",
      "\n",
      "2Ô∏è‚É£ Feature Categories:\n",
      "   üìå Deep Learning Features: 15 features\n",
      "      - Current Pollutants (scaled): PM10, NO2, SO2\n",
      "      - Current Weather (scaled): temperature_2m, relative_humidity_2m, wind_speed_10m, wind_direction_10m, precipitation\n",
      "      - Time Features (cyclic): hour_sin/cos, month_sin/cos, day_of_week_sin/cos\n",
      "      - Time Features (linear): is_weekend\n",
      "   üìå XGBoost Features: 63 features\n",
      "      - Deep Learning features: 15\n",
      "      - Lag Features: 48 features\n",
      "      - Variables: PM2_5, PM10, NO2, SO2, temperature_2m, relative_humidity_2m, wind_speed_10m, precipitation\n",
      "      - Lag steps: [1, 2, 3, 6, 12, 24]\n",
      "\n",
      "3Ô∏è‚É£ Target Variable:\n",
      "   üéØ PM2_5_scaled (normalized PM2.5)\n",
      "\n",
      "4Ô∏è‚É£ Normalization:\n",
      "   ‚úÖ All numerical features scaled to [0, 1] using train set only\n",
      "   ‚úÖ Scaler params saved to: scaler_params.json\n",
      "   ‚úÖ No data leakage (temporal split before normalization)\n",
      "\n",
      "5Ô∏è‚É£ Data Quality:\n",
      "   ‚úÖ No missing values in target\n",
      "   ‚úÖ No missing values in features\n",
      "   ‚úÖ Outliers removed\n",
      "   ‚úÖ Time series continuous\n",
      "\n",
      "6Ô∏è‚É£ Model-Specific Datasets:\n",
      "   üß† Deep Learning (CNN1D-BLSTM & LSTM): 15 base features\n",
      "   üìä XGBoost: 63 features (with lag features)\n",
      "\n",
      "üíæ Feature metadata saved to: ..\\data\\processed\\feature_metadata.json\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# B∆∞·ªõc 7: Prepare Final Model Datasets\n",
    "print(\"\\nüîÑ Step 7: Preparing Final Model-Specific Datasets...\")\n",
    "\n",
    "# ========================================\n",
    "# DEEP LEARNING DATASETS (CNN1D-BLSTM & LSTM)\n",
    "# ========================================\n",
    "# Kh√¥ng c·∫ßn lag features, ch·ªâ c·∫ßn base features + time features\n",
    "\n",
    "print(f\"\\nüß† Deep Learning datasets (no lag features):\")\n",
    "\n",
    "# Select only DL features + target\n",
    "dl_train = df_train.select(\"location_id\", \"datetime\", target_feature, *dl_input_features)\n",
    "dl_val = df_val.select(\"location_id\", \"datetime\", target_feature, *dl_input_features)\n",
    "dl_test = df_test.select(\"location_id\", \"datetime\", target_feature, *dl_input_features)\n",
    "\n",
    "# Cache\n",
    "dl_train = dl_train.cache()\n",
    "dl_val = dl_val.cache()\n",
    "dl_test = dl_test.cache()\n",
    "\n",
    "dl_train_count = dl_train.count()\n",
    "dl_val_count = dl_val.count()\n",
    "dl_test_count = dl_test.count()\n",
    "\n",
    "print(f\"  ‚úì Train: {dl_train_count:,} records, {len(dl_input_features)} features\")\n",
    "print(f\"  ‚úì Val:   {dl_val_count:,} records, {len(dl_input_features)} features\")\n",
    "print(f\"  ‚úì Test:  {dl_test_count:,} records, {len(dl_input_features)} features\")\n",
    "\n",
    "# ========================================\n",
    "# XGBOOST DATASETS\n",
    "# ========================================\n",
    "# C·∫ßn c·∫£ base features + lag features\n",
    "\n",
    "print(f\"\\nüìä XGBoost datasets (with lag features):\")\n",
    "\n",
    "# Select XGB features + target\n",
    "xgb_train = df_train.select(\"location_id\", \"datetime\", target_feature, *xgb_input_features)\n",
    "xgb_val = df_val.select(\"location_id\", \"datetime\", target_feature, *xgb_input_features)\n",
    "xgb_test = df_test.select(\"location_id\", \"datetime\", target_feature, *xgb_input_features)\n",
    "\n",
    "# Cache\n",
    "xgb_train = xgb_train.cache()\n",
    "xgb_val = xgb_val.cache()\n",
    "xgb_test = xgb_test.cache()\n",
    "\n",
    "xgb_train_count = xgb_train.count()\n",
    "xgb_val_count = xgb_val.count()\n",
    "xgb_test_count = xgb_test.count()\n",
    "\n",
    "print(f\"  ‚úì Train: {xgb_train_count:,} records, {len(xgb_input_features)} features\")\n",
    "print(f\"  ‚úì Val:   {xgb_val_count:,} records, {len(xgb_input_features)} features\")\n",
    "print(f\"  ‚úì Test:  {xgb_test_count:,} records, {len(xgb_input_features)} features\")\n",
    "\n",
    "print(f\"\\n‚úÖ Final datasets prepared!\")\n",
    "print(f\"   üß† Deep Learning: {len(dl_input_features)} features (no lags)\")\n",
    "print(f\"   üìä XGBoost: {len(xgb_input_features)} features (with {len(lag_base_columns) * len(LAG_STEPS)} lags)\")\n",
    "print(f\"   üéØ Target: {target_feature}\")\n",
    "print(f\"   ‚úÖ All datasets cleaned and ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebc9c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B∆∞·ªõc 8: Feature Engineering Summary + Metadata Saving\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä FEATURE ENGINEERING PIPELINE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n‚úÖ PIPELINE EXECUTION ORDER (Correct - No Data Leakage):\")\n",
    "print(f\"   1Ô∏è‚É£ Time Features ‚Üí Added cyclic (sin/cos) + is_weekend\")\n",
    "print(f\"   2Ô∏è‚É£ Temporal Split ‚Üí 70% train / 15% val / 15% test\")\n",
    "print(f\"   3Ô∏è‚É£ Normalization ‚Üí Min-Max [0,1] using TRAIN statistics ONLY\")\n",
    "print(f\"   4Ô∏è‚É£ Lag Features + Null Handling ‚Üí Created FROM SCALED columns, dropped nulls\")\n",
    "print(f\"   5Ô∏è‚É£ Scaler Params ‚Üí Saved for inference\")\n",
    "print(f\"   6Ô∏è‚É£ Model Features ‚Üí Prepared for Deep Learning & XGBoost\")\n",
    "print(f\"   7Ô∏è‚É£ Final Datasets ‚Üí Ready for training\")\n",
    "\n",
    "print(f\"\\nüìä DATASET STATISTICS:\")\n",
    "print(f\"   Total records: {dl_train_count + dl_val_count + dl_test_count:,}\")\n",
    "print(f\"   Total locations: {df_train.select('location_id').distinct().count()}\")\n",
    "print(f\"   Time range: {min_time.strftime('%Y-%m-%d')} ‚Üí {max_time.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(f\"\\nüìã FEATURE BREAKDOWN:\")\n",
    "print(f\"   üß† Deep Learning (CNN1D-BLSTM & LSTM): {len(dl_input_features)} features\")\n",
    "print(f\"      ‚îú‚îÄ Pollutants (scaled): 3 (PM10, NO2, SO2)\")\n",
    "print(f\"      ‚îú‚îÄ Weather (scaled): 5 (temp, humidity, wind, precipitation)\")\n",
    "print(f\"      ‚îú‚îÄ Time (cyclic): 6 (hour, month, day_of_week ‚Üí sin/cos)\")\n",
    "print(f\"      ‚îî‚îÄ Time (binary): 1 (is_weekend)\")\n",
    "print(f\"   \")\n",
    "print(f\"   üìä XGBoost: {len(xgb_input_features)} features\")\n",
    "print(f\"      ‚îú‚îÄ Deep Learning features: {len(dl_input_features)}\")\n",
    "print(f\"      ‚îî‚îÄ Lag features: {len(lag_base_columns) * len(LAG_STEPS)} ({len(lag_base_columns)} vars √ó {len(LAG_STEPS)} lags)\")\n",
    "\n",
    "print(f\"\\nüéØ TARGET VARIABLE:\")\n",
    "print(f\"   {target_feature} (normalized PM2.5 in [0, 1])\")\n",
    "\n",
    "print(f\"\\n‚úÖ DATA QUALITY CHECKS:\")\n",
    "print(f\"   ‚úì No missing values in target\")\n",
    "print(f\"   ‚úì No missing values in features\")\n",
    "print(f\"   ‚úì No outliers (removed by WHO/EPA standards)\")\n",
    "print(f\"   ‚úì Proper temporal ordering\")\n",
    "print(f\"   ‚úì No data leakage (train/val/test temporally separated)\")\n",
    "print(f\"   ‚úì Correct scale relationship (lag from scaled columns)\")\n",
    "print(f\"   ‚úì No nulls in lag features (first {max(LAG_STEPS)}h dropped)\")\n",
    "\n",
    "print(f\"\\nüíæ SAVED ARTIFACTS:\")\n",
    "print(f\"   üìÅ scaler_params.json ‚Üí Min-Max parameters (train set only)\")\n",
    "print(f\"   üìÅ feature_metadata.json ‚Üí Feature lists & configuration\")\n",
    "\n",
    "print(f\"\\nüöÄ READY FOR NEXT PHASE:\")\n",
    "print(f\"   Variables in memory:\")\n",
    "print(f\"   - Deep Learning: dl_train, dl_val, dl_test\")\n",
    "print(f\"   - XGBoost: xgb_train, xgb_val, xgb_test\")\n",
    "print(f\"   Next step: Sequence creation for Deep Learning models\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========================================\n",
    "# SAVE FEATURE METADATA\n",
    "# ========================================\n",
    "# L∆∞u metadata v·ªÅ feature engineering ƒë·ªÉ tham kh·∫£o trong t∆∞∆°ng lai\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Metadata cho feature engineering\n",
    "dataset_metadata = {\n",
    "    \"project\": \"PM2.5 Prediction\",\n",
    "    \"preprocessing_version\": \"2.0_refactored\",\n",
    "    \"pipeline_order\": [\n",
    "        \"Time Features (cyclic encoding)\",\n",
    "        \"Temporal Split (70/15/15)\",\n",
    "        \"Normalization (train stats only)\",\n",
    "        \"Lag Features (from scaled columns)\",\n",
    "        \"Null Handling (drop first 24h per location)\"\n",
    "    ],\n",
    "    \"deep_learning_features\": dl_input_features,\n",
    "    \"xgboost_features\": xgb_input_features,\n",
    "    \"target_feature\": target_feature,\n",
    "    \"lag_config\": {\n",
    "        \"lag_steps\": LAG_STEPS,\n",
    "        \"lag_base_columns\": lag_base_columns,\n",
    "        \"total_lag_features\": len(lag_base_columns) * len(LAG_STEPS)\n",
    "    },\n",
    "    \"temporal_split\": {\n",
    "        \"train_end\": train_end.isoformat(),\n",
    "        \"val_end\": val_end.isoformat(),\n",
    "        \"min_time\": min_time.isoformat(),\n",
    "        \"max_time\": max_time.isoformat()\n",
    "    },\n",
    "    \"dataset_counts\": {\n",
    "        \"dl_train\": dl_train_count,\n",
    "        \"dl_val\": dl_val_count,\n",
    "        \"dl_test\": dl_test_count,\n",
    "        \"xgb_train\": xgb_train_count,\n",
    "        \"xgb_val\": xgb_val_count,\n",
    "        \"xgb_test\": xgb_test_count\n",
    "    },\n",
    "    \"total_features\": {\n",
    "        \"deep_learning\": len(dl_input_features),\n",
    "        \"xgboost\": len(xgb_input_features)\n",
    "    }\n",
    "}\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c v√† l∆∞u\n",
    "processed_dir = Path(\"../data/processed\")\n",
    "processed_dir.mkdir(exist_ok=True)\n",
    "\n",
    "metadata_path = processed_dir / \"feature_metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(dataset_metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Feature metadata saved to: {metadata_path}\")\n",
    "print(f\"   ‚úÖ Pipeline version: 2.0 (refactored - no data leakage)\")\n",
    "print(f\"   ‚úÖ Contains: feature lists, lag config, split info, dataset counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523011ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Step 7: Creating Sequence Data for Deep Learning Models...\n",
      "‚òÅÔ∏è CLOUD ENVIRONMENT: Using optimal sequences\n",
      "üìä Creating sequences for each model...\n",
      "\n",
      "üß† CNN1D-BLSTM-Attention (48 timesteps):\n",
      "    Creating 48-step sequences...\n",
      "        Processing batch 1/5...\n",
      "        Processing batch 2/5...\n",
      "        Processing batch 2/5...\n",
      "        Processing batch 3/5...\n",
      "        Processing batch 3/5...\n",
      "        Processing batch 4/5...\n",
      "        Processing batch 4/5...\n",
      "        Processing batch 5/5...\n",
      "        Processing batch 5/5...\n",
      "    ‚ùå CNN sequence creation failed: An error occurred while calling o16487.count.\n",
      ": org.apache.spark.SparkException: Job aborted due to ...\n",
      "\n",
      "üîÑ LSTM (24 timesteps):\n",
      "    Creating 24-step sequences...\n",
      "    ‚ùå CNN sequence creation failed: An error occurred while calling o16487.count.\n",
      ": org.apache.spark.SparkException: Job aborted due to ...\n",
      "\n",
      "üîÑ LSTM (24 timesteps):\n",
      "    Creating 24-step sequences...\n",
      "        Processing batch 1/5...\n",
      "        Processing batch 1/5...\n",
      "    ‚ùå LSTM sequence creation failed: An error occurred while calling o17353.cache.\n",
      ": java.lang.IllegalStateException: Cannot call methods...\n",
      "\n",
      "‚úÖ Sequence data preparation completed!\n",
      "    ‚ùå LSTM sequence creation failed: An error occurred while calling o17353.cache.\n",
      ": java.lang.IllegalStateException: Cannot call methods...\n",
      "\n",
      "‚úÖ Sequence data preparation completed!\n"
     ]
    }
   ],
   "source": [
    "# B∆∞·ªõc 9: Create Sequence Data for Deep Learning Models\n",
    "print(\"\\nüîÑ Step 9: Creating Sequence Data for Deep Learning Models...\")\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "# Sequence configuration (optimized for Colab)\n",
    "CNN_SEQUENCE_LENGTH = 48  # Optimal for long-term patterns\n",
    "LSTM_SEQUENCE_LENGTH = 24  # Optimal for medium-term patterns\n",
    "\n",
    "print(f\"‚öôÔ∏è  Sequence Configuration:\")\n",
    "print(f\"   - CNN1D-BLSTM-Attention: {CNN_SEQUENCE_LENGTH} timesteps\")\n",
    "print(f\"   - LSTM: {LSTM_SEQUENCE_LENGTH} timesteps\")\n",
    "\n",
    "def create_sequences_optimized(df, feature_cols, target_col, sequence_length):\n",
    "    \"\"\"\n",
    "    Memory-efficient sequence creation with batch processing\n",
    "    \n",
    "    üîë Null Handling Strategy (2-Layer Protection):\n",
    "    Layer 1: Drop first {sequence_length} records per location (incomplete history)\n",
    "    Layer 2: Drop records with ANY null in lag features (data gaps in middle)\n",
    "    Result: 100% clean sequences with ZERO nulls\n",
    "    \"\"\"\n",
    "    print(f\"    Creating {sequence_length}-step sequences...\")\n",
    "    \n",
    "    window_spec = Window.partitionBy(\"location_id\").orderBy(\"datetime\")\n",
    "    df_base = df.select(\"location_id\", \"datetime\", target_col, *feature_cols).repartition(4, \"location_id\")\n",
    "    \n",
    "    # ========================================\n",
    "    # LAYER 1: Drop first N records (incomplete history)\n",
    "    # ========================================\n",
    "    window_rank = Window.partitionBy(\"location_id\").orderBy(\"datetime\")\n",
    "    df_base = df_base.withColumn(\"row_num\", F.row_number().over(window_rank))\n",
    "    \n",
    "    records_before_layer1 = df_base.count()\n",
    "    df_base = df_base.filter(F.col(\"row_num\") > sequence_length).drop(\"row_num\")\n",
    "    records_after_layer1 = df_base.count()\n",
    "    \n",
    "    print(f\"      üõ°Ô∏è  Layer 1: Dropped first {sequence_length} records/location\")\n",
    "    print(f\"         Records: {records_before_layer1:,} ‚Üí {records_after_layer1:,}\")\n",
    "    \n",
    "    # Process features in small batches to avoid deep logical plans\n",
    "    FEATURE_BATCH_SIZE = 3\n",
    "    feature_batches = [feature_cols[i:i+FEATURE_BATCH_SIZE] for i in range(0, len(feature_cols), FEATURE_BATCH_SIZE)]\n",
    "    sequence_dfs = []\n",
    "    \n",
    "    for batch_idx, feature_batch in enumerate(feature_batches):\n",
    "        print(f\"        Processing batch {batch_idx+1}/{len(feature_batches)}...\")\n",
    "        batch_df = df_base.select(\"location_id\", \"datetime\", target_col, *feature_batch)\n",
    "        \n",
    "        # Create lag features for this batch\n",
    "        batch_lag_cols = []\n",
    "        for step in range(1, sequence_length + 1):\n",
    "            for col in feature_batch:\n",
    "                lag_col_name = f\"{col}_lag{step}\"\n",
    "                batch_df = batch_df.withColumn(lag_col_name, F.lag(col, step).over(window_spec))\n",
    "                batch_lag_cols.append(lag_col_name)\n",
    "        \n",
    "        # ========================================\n",
    "        # LAYER 2: Check for nulls in ANY lag column of THIS batch\n",
    "        # ========================================\n",
    "        # T·∫°o c·ªôt flag: True n·∫øu T·∫§T C·∫¢ lag columns NOT NULL\n",
    "        null_checks = [F.col(lag_col).isNotNull() for lag_col in batch_lag_cols]\n",
    "        \n",
    "        # K·∫øt h·ª£p t·∫•t c·∫£ ƒëi·ªÅu ki·ªán b·∫±ng AND\n",
    "        complete_condition = null_checks[0]\n",
    "        for check in null_checks[1:]:\n",
    "            complete_condition = complete_condition & check\n",
    "        \n",
    "        batch_df = batch_df.withColumn(f\"_complete_batch{batch_idx}\", complete_condition)\n",
    "        \n",
    "        # Convert to sequences (NO coalesce - will filter nulls later)\n",
    "        for col in feature_batch:\n",
    "            lag_cols = [f\"{col}_lag{step}\" for step in range(1, sequence_length + 1)]\n",
    "            batch_df = batch_df.withColumn(f\"{col}_sequence\", F.array(*[F.col(c) for c in lag_cols])).drop(*lag_cols)\n",
    "        \n",
    "        sequence_cols = [f\"{col}_sequence\" for col in feature_batch]\n",
    "        batch_result = batch_df.select(\"location_id\", \"datetime\", f\"_complete_batch{batch_idx}\", *sequence_cols).cache()\n",
    "        batch_result.count()  # Force materialization\n",
    "        sequence_dfs.append(batch_result)\n",
    "    \n",
    "    # Join all sequence batches\n",
    "    final_df = df_base.select(\"location_id\", \"datetime\", target_col)\n",
    "    for seq_df in sequence_dfs:\n",
    "        final_df = final_df.join(seq_df, [\"location_id\", \"datetime\"], \"inner\")\n",
    "    \n",
    "    # ========================================\n",
    "    # LAYER 2 FILTER: Keep only records with complete sequences in ALL batches\n",
    "    # ========================================\n",
    "    records_before_layer2 = final_df.count()\n",
    "    \n",
    "    # T√¨m t·∫•t c·∫£ c·ªôt _complete_batchX\n",
    "    complete_cols = [c for c in final_df.columns if c.startswith(\"_complete_batch\")]\n",
    "    \n",
    "    if complete_cols:\n",
    "        # K·∫øt h·ª£p t·∫•t c·∫£ ƒëi·ªÅu ki·ªán: T·∫§T C·∫¢ batches ph·∫£i complete\n",
    "        filter_condition = F.col(complete_cols[0])\n",
    "        for col in complete_cols[1:]:\n",
    "            filter_condition = filter_condition & F.col(col)\n",
    "        \n",
    "        # Apply filter v√† drop c√°c c·ªôt flag\n",
    "        final_df = final_df.filter(filter_condition).drop(*complete_cols)\n",
    "    \n",
    "    records_after_layer2 = final_df.count()\n",
    "    dropped_by_layer2 = records_before_layer2 - records_after_layer2\n",
    "    \n",
    "    if dropped_by_layer2 > 0:\n",
    "        print(f\"      üõ°Ô∏è  Layer 2: Dropped {dropped_by_layer2:,} records with nulls in middle\")\n",
    "        print(f\"         Records: {records_before_layer2:,} ‚Üí {records_after_layer2:,}\")\n",
    "    else:\n",
    "        print(f\"      ‚úÖ Layer 2: No data gaps detected (all sequences complete)\")\n",
    "    \n",
    "    # Filter target nulls v√† rename\n",
    "    final_df = final_df.filter(F.col(target_col).isNotNull()).withColumnRenamed(target_col, \"target_value\").cache()\n",
    "    \n",
    "    final_count = final_df.count()\n",
    "    total_dropped = records_before_layer1 - final_count\n",
    "    retention_rate = (final_count / records_before_layer1) * 100\n",
    "    \n",
    "    print(f\"      ‚úÖ Final: {final_count:,} records ({retention_rate:.1f}% retained)\")\n",
    "    print(f\"      üìä Total dropped: {total_dropped:,} (incomplete or null sequences)\")\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "print(\"\\nüìä Creating sequences for each model...\")\n",
    "\n",
    "# Create CNN1D-BLSTM sequences\n",
    "print(f\"\\nüß† CNN1D-BLSTM-Attention ({CNN_SEQUENCE_LENGTH} timesteps):\")\n",
    "try:\n",
    "    cnn_train_clean = create_sequences_optimized(dl_train, dl_input_features, target_feature, CNN_SEQUENCE_LENGTH)\n",
    "    cnn_val_clean = create_sequences_optimized(dl_val, dl_input_features, target_feature, CNN_SEQUENCE_LENGTH)\n",
    "    cnn_test_clean = create_sequences_optimized(dl_test, dl_input_features, target_feature, CNN_SEQUENCE_LENGTH)\n",
    "    print(f\"    ‚úÖ CNN sequences created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"    ‚ùå CNN sequence creation failed: {str(e)[:100]}...\")\n",
    "    cnn_train_clean = cnn_val_clean = cnn_test_clean = None\n",
    "\n",
    "# Create LSTM sequences  \n",
    "print(f\"\\nüîÑ LSTM ({LSTM_SEQUENCE_LENGTH} timesteps):\")\n",
    "try:\n",
    "    lstm_train_clean = create_sequences_optimized(dl_train, dl_input_features, target_feature, LSTM_SEQUENCE_LENGTH)\n",
    "    lstm_val_clean = create_sequences_optimized(dl_val, dl_input_features, target_feature, LSTM_SEQUENCE_LENGTH)\n",
    "    lstm_test_clean = create_sequences_optimized(dl_test, dl_input_features, target_feature, LSTM_SEQUENCE_LENGTH)\n",
    "    print(f\"    ‚úÖ LSTM sequences created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"    ‚ùå LSTM sequence creation failed: {str(e)[:100]}...\")\n",
    "    lstm_train_clean = lstm_val_clean = lstm_test_clean = None\n",
    "\n",
    "print(f\"\\n‚úÖ Sequence data preparation completed!\")\n",
    "print(f\"\\nüìã Data Quality Guarantee:\")\n",
    "print(f\"   ‚úì Layer 1: No incomplete history (first {CNN_SEQUENCE_LENGTH}/{LSTM_SEQUENCE_LENGTH} records dropped)\")\n",
    "print(f\"   ‚úì Layer 2: No data gaps in middle (nulls filtered out)\")\n",
    "print(f\"   ‚úì Result: 100% clean sequences with ZERO nulls\")\n",
    "print(f\"   ‚úì Ready for high-quality model training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a99d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Step 8: Creating Final Dataset Summary...\n",
      "üìä Dataset Status:\n",
      "  CNN1D-BLSTM: ‚ùå Not Ready\n",
      "  LSTM: ‚ùå Not Ready\n",
      "  XGBoost: ‚úÖ Ready\n",
      "\n",
      "üíæ Metadata saved to: ..\\data\\processed\\datasets_ready.json\n",
      "\n",
      "‚úÖ DATA PREPROCESSING COMPLETE!\n",
      "   üìä Variables available in memory for model training:\n",
      "   - CNN1D-BLSTM: cnn_train_clean, cnn_val_clean, cnn_test_clean\n",
      "   - LSTM: lstm_train_clean, lstm_val_clean, lstm_test_clean\n",
      "   - XGBoost: xgb_train, xgb_val, xgb_test\n",
      "   üöÄ Ready for model training phase!\n"
     ]
    }
   ],
   "source": [
    "# B∆∞·ªõc 10: Export Final Datasets to Disk\n",
    "print(\"\\nüì¶ Step 10: Exporting Final Datasets to Disk...\")\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output directory\n",
    "processed_dir = Path(\"../data/processed\")\n",
    "processed_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Check dataset availability\n",
    "datasets_ready = {\n",
    "    \"cnn\": cnn_train_clean is not None and cnn_val_clean is not None and cnn_test_clean is not None,\n",
    "    \"lstm\": lstm_train_clean is not None and lstm_val_clean is not None and lstm_test_clean is not None,\n",
    "    \"xgb\": xgb_train is not None and xgb_val is not None and xgb_test is not None\n",
    "}\n",
    "\n",
    "print(f\"üìä Dataset Status:\")\n",
    "for model, ready in datasets_ready.items():\n",
    "    model_name = {\"cnn\": \"CNN1D-BLSTM\", \"lstm\": \"LSTM\", \"xgb\": \"XGBoost\"}[model]\n",
    "    status = \"‚úÖ Ready\" if ready else \"‚ùå Not Ready\"\n",
    "    print(f\"  {model_name}: {status}\")\n",
    "\n",
    "# ========================================\n",
    "# EXPORT DATASETS TO PARQUET\n",
    "# ========================================\n",
    "print(f\"\\nüíæ Exporting datasets to Parquet format...\")\n",
    "\n",
    "export_summary = {\n",
    "    \"cnn\": {\"train\": 0, \"val\": 0, \"test\": 0},\n",
    "    \"lstm\": {\"train\": 0, \"val\": 0, \"test\": 0},\n",
    "    \"xgb\": {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "}\n",
    "\n",
    "# Export CNN1D-BLSTM datasets\n",
    "if datasets_ready[\"cnn\"]:\n",
    "    print(f\"\\n  üß† Exporting CNN1D-BLSTM datasets...\")\n",
    "    cnn_dir = processed_dir / \"cnn_sequences\"\n",
    "    cnn_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    cnn_train_clean.write.mode(\"overwrite\").parquet(str(cnn_dir / \"train\"))\n",
    "    cnn_val_clean.write.mode(\"overwrite\").parquet(str(cnn_dir / \"val\"))\n",
    "    cnn_test_clean.write.mode(\"overwrite\").parquet(str(cnn_dir / \"test\"))\n",
    "    \n",
    "    export_summary[\"cnn\"][\"train\"] = cnn_train_clean.count()\n",
    "    export_summary[\"cnn\"][\"val\"] = cnn_val_clean.count()\n",
    "    export_summary[\"cnn\"][\"test\"] = cnn_test_clean.count()\n",
    "    \n",
    "    print(f\"     ‚úÖ Saved to: {cnn_dir}/\")\n",
    "    print(f\"        - train: {export_summary['cnn']['train']:,} records\")\n",
    "    print(f\"        - val:   {export_summary['cnn']['val']:,} records\")\n",
    "    print(f\"        - test:  {export_summary['cnn']['test']:,} records\")\n",
    "\n",
    "# Export LSTM datasets\n",
    "if datasets_ready[\"lstm\"]:\n",
    "    print(f\"\\n  üîÑ Exporting LSTM datasets...\")\n",
    "    lstm_dir = processed_dir / \"lstm_sequences\"\n",
    "    lstm_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    lstm_train_clean.write.mode(\"overwrite\").parquet(str(lstm_dir / \"train\"))\n",
    "    lstm_val_clean.write.mode(\"overwrite\").parquet(str(lstm_dir / \"val\"))\n",
    "    lstm_test_clean.write.mode(\"overwrite\").parquet(str(lstm_dir / \"test\"))\n",
    "    \n",
    "    export_summary[\"lstm\"][\"train\"] = lstm_train_clean.count()\n",
    "    export_summary[\"lstm\"][\"val\"] = lstm_val_clean.count()\n",
    "    export_summary[\"lstm\"][\"test\"] = lstm_test_clean.count()\n",
    "    \n",
    "    print(f\"     ‚úÖ Saved to: {lstm_dir}/\")\n",
    "    print(f\"        - train: {export_summary['lstm']['train']:,} records\")\n",
    "    print(f\"        - val:   {export_summary['lstm']['val']:,} records\")\n",
    "    print(f\"        - test:  {export_summary['lstm']['test']:,} records\")\n",
    "\n",
    "# Export XGBoost datasets\n",
    "if datasets_ready[\"xgb\"]:\n",
    "    print(f\"\\n  üìä Exporting XGBoost datasets...\")\n",
    "    xgb_dir = processed_dir / \"xgboost\"\n",
    "    xgb_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    xgb_train.write.mode(\"overwrite\").parquet(str(xgb_dir / \"train\"))\n",
    "    xgb_val.write.mode(\"overwrite\").parquet(str(xgb_dir / \"val\"))\n",
    "    xgb_test.write.mode(\"overwrite\").parquet(str(xgb_dir / \"test\"))\n",
    "    \n",
    "    export_summary[\"xgb\"][\"train\"] = xgb_train.count()\n",
    "    export_summary[\"xgb\"][\"val\"] = xgb_val.count()\n",
    "    export_summary[\"xgb\"][\"test\"] = xgb_test.count()\n",
    "    \n",
    "    print(f\"     ‚úÖ Saved to: {xgb_dir}/\")\n",
    "    print(f\"        - train: {export_summary['xgb']['train']:,} records\")\n",
    "    print(f\"        - val:   {export_summary['xgb']['val']:,} records\")\n",
    "    print(f\"        - test:  {export_summary['xgb']['test']:,} records\")\n",
    "\n",
    "# ========================================\n",
    "# SAVE METADATA\n",
    "# ========================================\n",
    "print(f\"\\nüíæ Saving metadata...\")\n",
    "\n",
    "# Create comprehensive metadata\n",
    "final_metadata = {\n",
    "    \"project\": \"PM2.5 Prediction\",\n",
    "    \"preprocessing_completed\": True,\n",
    "    \"export_timestamp\": str(pd.Timestamp.now()),\n",
    "    \"models\": {\n",
    "        \"cnn1d_blstm\": {\n",
    "            \"sequence_length\": CNN_SEQUENCE_LENGTH,\n",
    "            \"features\": len(dl_input_features),\n",
    "            \"ready\": datasets_ready[\"cnn\"],\n",
    "            \"export_path\": str(processed_dir / \"cnn_sequences\"),\n",
    "            \"record_counts\": export_summary[\"cnn\"]\n",
    "        },\n",
    "        \"lstm\": {\n",
    "            \"sequence_length\": LSTM_SEQUENCE_LENGTH, \n",
    "            \"features\": len(dl_input_features),\n",
    "            \"ready\": datasets_ready[\"lstm\"],\n",
    "            \"export_path\": str(processed_dir / \"lstm_sequences\"),\n",
    "            \"record_counts\": export_summary[\"lstm\"]\n",
    "        },\n",
    "        \"xgboost\": {\n",
    "            \"features\": len(xgb_input_features),\n",
    "            \"lag_steps\": LAG_STEPS,\n",
    "            \"ready\": datasets_ready[\"xgb\"],\n",
    "            \"export_path\": str(processed_dir / \"xgboost\"),\n",
    "            \"record_counts\": export_summary[\"xgb\"]\n",
    "        }\n",
    "    },\n",
    "    \"feature_details\": {\n",
    "        \"deep_learning_features\": dl_input_features,\n",
    "        \"xgboost_features\": xgb_input_features,\n",
    "        \"target\": target_feature\n",
    "    },\n",
    "    \"data_format\": \"parquet\",\n",
    "    \"null_handling\": {\n",
    "        \"strategy\": \"2-layer protection\",\n",
    "        \"layer1\": f\"Dropped first {CNN_SEQUENCE_LENGTH}/{LSTM_SEQUENCE_LENGTH} records per location\",\n",
    "        \"layer2\": \"Filtered records with nulls in sequence history\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = processed_dir / \"datasets_ready.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(final_metadata, f, indent=2)\n",
    "\n",
    "print(f\"   ‚úÖ Metadata saved to: {metadata_path}\")\n",
    "\n",
    "# ========================================\n",
    "# FINAL SUMMARY\n",
    "# ========================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ DATA PREPROCESSING & EXPORT COMPLETE!\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\nüìÇ Exported Directory Structure:\")\n",
    "print(f\"   {processed_dir}/\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ cnn_sequences/\")\n",
    "print(f\"   ‚îÇ   ‚îú‚îÄ‚îÄ train/  ({export_summary['cnn']['train']:,} records)\")\n",
    "print(f\"   ‚îÇ   ‚îú‚îÄ‚îÄ val/    ({export_summary['cnn']['val']:,} records)\")\n",
    "print(f\"   ‚îÇ   ‚îî‚îÄ‚îÄ test/   ({export_summary['cnn']['test']:,} records)\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ lstm_sequences/\")\n",
    "print(f\"   ‚îÇ   ‚îú‚îÄ‚îÄ train/  ({export_summary['lstm']['train']:,} records)\")\n",
    "print(f\"   ‚îÇ   ‚îú‚îÄ‚îÄ val/    ({export_summary['lstm']['val']:,} records)\")\n",
    "print(f\"   ‚îÇ   ‚îî‚îÄ‚îÄ test/   ({export_summary['lstm']['test']:,} records)\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ xgboost/\")\n",
    "print(f\"   ‚îÇ   ‚îú‚îÄ‚îÄ train/  ({export_summary['xgb']['train']:,} records)\")\n",
    "print(f\"   ‚îÇ   ‚îú‚îÄ‚îÄ val/    ({export_summary['xgb']['val']:,} records)\")\n",
    "print(f\"   ‚îÇ   ‚îî‚îÄ‚îÄ test/   ({export_summary['xgb']['test']:,} records)\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ scaler_params.json\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ feature_metadata.json\")\n",
    "print(f\"   ‚îî‚îÄ‚îÄ datasets_ready.json\")\n",
    "\n",
    "print(f\"\\nüìä Total Dataset Sizes:\")\n",
    "total_cnn = sum(export_summary['cnn'].values())\n",
    "total_lstm = sum(export_summary['lstm'].values())\n",
    "total_xgb = sum(export_summary['xgb'].values())\n",
    "print(f\"   - CNN1D-BLSTM: {total_cnn:,} records ({CNN_SEQUENCE_LENGTH} timesteps, {len(dl_input_features)} features)\")\n",
    "print(f\"   - LSTM:        {total_lstm:,} records ({LSTM_SEQUENCE_LENGTH} timesteps, {len(dl_input_features)} features)\")\n",
    "print(f\"   - XGBoost:     {total_xgb:,} records ({len(xgb_input_features)} features)\")\n",
    "\n",
    "print(f\"\\nüöÄ Ready for Model Training Phase!\")\n",
    "print(f\"   Load datasets using: spark.read.parquet('path/to/dataset')\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

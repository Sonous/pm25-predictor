{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "127b4678",
   "metadata": {
    "papermill": {
     "duration": 0.009195,
     "end_time": "2025-11-20T10:28:15.667780",
     "exception": false,
     "start_time": "2025-11-20T10:28:15.658585",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PM2.5 Prediction - Data Preprocessing\n",
    "\n",
    "Notebook này thực hiện tiền xử lý dữ liệu với PySpark:\n",
    "1. Kết nối Spark cluster\n",
    "2. Đọc và khám phá dữ liệu\n",
    "3. Tổng quan về dataset\n",
    "4. **Làm sạch dữ liệu** (Outlier Removal -> Missing Value Imputation)\n",
    "5. Feature engineering\n",
    "6. Data summary & statistics\n",
    "7. Lưu dữ liệu đã xử lý"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37105250",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:28:15.684187Z",
     "iopub.status.busy": "2025-11-20T10:28:15.683866Z",
     "iopub.status.idle": "2025-11-20T10:28:24.329812Z",
     "shell.execute_reply": "2025-11-20T10:28:24.328878Z"
    },
    "papermill": {
     "duration": 8.655759,
     "end_time": "2025-11-20T10:28:24.331187",
     "exception": false,
     "start_time": "2025-11-20T10:28:15.675428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KAGGLE] Running on Kaggle\n",
      "[INSTALL] Installing PySpark...\n",
      "[OK] PySpark installed\n",
      "[OK] Java: /usr/lib/jvm/java-11-openjdk-amd64\n",
      "[OK] All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Detect environment (Kaggle vs Colab vs Local)\n",
    "IN_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    print(\"[KAGGLE] Running on Kaggle\")\n",
    "    \n",
    "    # Kaggle has Java pre-installed, just set JAVA_HOME\n",
    "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "    os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ.get(\"PATH\", \"\")\n",
    "    \n",
    "    # Install PySpark\n",
    "    print(\"[INSTALL] Installing PySpark...\")\n",
    "    !pip install -q pyspark\n",
    "    print(\"[OK] PySpark installed\")\n",
    "    print(f\"[OK] Java: {os.environ['JAVA_HOME']}\")\n",
    "    \n",
    "elif IN_COLAB:\n",
    "    print(\"[COLAB] Running on Google Colab\")\n",
    "    \n",
    "    # Install Java 11 (required for PySpark)\n",
    "    print(\"[INSTALL] Installing Java 11...\")\n",
    "    !apt-get install -y openjdk-11-jdk-headless -qq > /dev/null\n",
    "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "    os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ.get(\"PATH\", \"\")\n",
    "    print(f\"[OK] Java installed: {os.environ['JAVA_HOME']}\")\n",
    "    \n",
    "    # Install PySpark\n",
    "    print(\"[INSTALL] Installing PySpark...\")\n",
    "    !pip install -q pyspark\n",
    "    print(\"[OK] PySpark installed\")\n",
    "    \n",
    "    # Mount Google Drive (optional - if data is in Drive)\n",
    "    # from google.colab import drive\n",
    "    # drive.mount('/content/drive')\n",
    "    \n",
    "else:\n",
    "    print(\"[LOCAL] Running on Local Machine\")\n",
    "    \n",
    "    # Set Java 21 for PySpark (local only)\n",
    "    os.environ['JAVA_HOME'] = r'C:\\Program Files\\Java\\jdk-21'\n",
    "    os.environ['PATH'] = os.environ['JAVA_HOME'] + r'\\bin;' + os.environ.get('PATH', '')\n",
    "    print(f\"[OK] Using Java: {os.environ['JAVA_HOME']}\")\n",
    "\n",
    "# Common imports\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"[OK] All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253a87a4",
   "metadata": {
    "papermill": {
     "duration": 0.007426,
     "end_time": "2025-11-20T10:28:24.346377",
     "exception": false,
     "start_time": "2025-11-20T10:28:24.338951",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Kết nối Spark Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74255cdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:28:24.362997Z",
     "iopub.status.busy": "2025-11-20T10:28:24.362350Z",
     "iopub.status.idle": "2025-11-20T10:28:32.604082Z",
     "shell.execute_reply": "2025-11-20T10:28:32.603018Z"
    },
    "papermill": {
     "duration": 8.251884,
     "end_time": "2025-11-20T10:28:32.605710",
     "exception": false,
     "start_time": "2025-11-20T10:28:24.353826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/20 10:28:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KAGGLE] Spark running on Kaggle (4 cores, 12GB total)\n",
      "[OK] Spark version: 3.5.1\n",
      "[OK] Spark mode: local[4]\n",
      "[OK] Application ID: local-1763634510297\n",
      "[OK] Cores: 8\n",
      "[OK] Parallelism: 8\n"
     ]
    }
   ],
   "source": [
    "# Tạo Spark Session với cấu hình tùy theo môi trường\n",
    "if IN_KAGGLE:\n",
    "    # Kaggle configuration - Balanced (4 cores, 16GB RAM)\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PM25-Preprocessing\") \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"6g\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "        .config(\"spark.default.parallelism\", \"8\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(\"[KAGGLE] Spark running on Kaggle (4 cores, 12GB total)\")\n",
    "    \n",
    "elif IN_COLAB:\n",
    "    # Colab configuration - Lighter settings (2 cores, 12GB RAM)\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PM25-Preprocessing\") \\\n",
    "        .master(\"local[2]\") \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .config(\"spark.executor.memory\", \"2g\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(\"[COLAB] Spark running on Colab (2 cores, 4GB total)\")\n",
    "    \n",
    "else:\n",
    "    # Local configuration - OPTIMIZED for 8-core AMD Ryzen + 15.7GB RAM\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PM25-Preprocessing\") \\\n",
    "        .master(\"local[8]\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"16\") \\\n",
    "        .config(\"spark.default.parallelism\", \"16\") \\\n",
    "        .config(\"spark.python.worker.timeout\", \"600\") \\\n",
    "        .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "        .config(\"spark.network.timeout\", \"600s\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(\"[LOCAL] Spark running on Local (8 cores, 12GB total - OPTIMIZED)\")\n",
    "\n",
    "print(f\"[OK] Spark version: {spark.version}\")\n",
    "print(f\"[OK] Spark mode: {spark.sparkContext.master}\")\n",
    "print(f\"[OK] Application ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"[OK] Cores: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"[OK] Parallelism: {spark.conf.get('spark.default.parallelism', 'default')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e26a404",
   "metadata": {
    "papermill": {
     "duration": 0.0083,
     "end_time": "2025-11-20T10:28:32.622759",
     "exception": false,
     "start_time": "2025-11-20T10:28:32.614459",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Định nghĩa Schema và Scan Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71adc98b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:28:32.640311Z",
     "iopub.status.busy": "2025-11-20T10:28:32.639992Z",
     "iopub.status.idle": "2025-11-20T10:28:32.647798Z",
     "shell.execute_reply": "2025-11-20T10:28:32.646881Z"
    },
    "papermill": {
     "duration": 0.018341,
     "end_time": "2025-11-20T10:28:32.649081",
     "exception": false,
     "start_time": "2025-11-20T10:28:32.630740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Schemas defined\n"
     ]
    }
   ],
   "source": [
    "# Schema cho dữ liệu OpenAQ\n",
    "openaq_schema = StructType([\n",
    "    StructField(\"location_id\", StringType(), True),\n",
    "    StructField(\"sensors_id\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"datetime\", TimestampType(), True),\n",
    "    StructField(\"lat\", DoubleType(), True),\n",
    "    StructField(\"lon\", DoubleType(), True),\n",
    "    StructField(\"parameter\", StringType(), True),\n",
    "    StructField(\"units\", StringType(), True),\n",
    "    StructField(\"value\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Schema cho dữ liệu Weather\n",
    "weather_schema = StructType([\n",
    "    StructField(\"time\", TimestampType(), True),\n",
    "    StructField(\"temperature_2m\", DoubleType(), True),\n",
    "    StructField(\"relative_humidity_2m\", DoubleType(), True),\n",
    "    StructField(\"wind_speed_10m\", DoubleType(), True),\n",
    "    StructField(\"wind_direction_10m\", DoubleType(), True),\n",
    "    StructField(\"surface_pressure\", DoubleType(), True),\n",
    "    StructField(\"precipitation\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "print(\"[OK] Schemas defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d29e480",
   "metadata": {
    "papermill": {
     "duration": 0.007921,
     "end_time": "2025-11-20T10:28:32.664951",
     "exception": false,
     "start_time": "2025-11-20T10:28:32.657030",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.1 Scan và Map Files theo Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d66ca1e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:28:32.682635Z",
     "iopub.status.busy": "2025-11-20T10:28:32.682344Z",
     "iopub.status.idle": "2025-11-20T10:28:32.818185Z",
     "shell.execute_reply": "2025-11-20T10:28:32.817209Z"
    },
    "papermill": {
     "duration": 0.146926,
     "end_time": "2025-11-20T10:28:32.819742",
     "exception": false,
     "start_time": "2025-11-20T10:28:32.672816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hong-kong-raw-pollutant-and-weather-dataset\r\n"
     ]
    }
   ],
   "source": [
    "!ls /kaggle/input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c083ac8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:28:32.837573Z",
     "iopub.status.busy": "2025-11-20T10:28:32.837272Z",
     "iopub.status.idle": "2025-11-20T10:28:32.884503Z",
     "shell.execute_reply": "2025-11-20T10:28:32.883468Z"
    },
    "papermill": {
     "duration": 0.057939,
     "end_time": "2025-11-20T10:28:32.885885",
     "exception": false,
     "start_time": "2025-11-20T10:28:32.827946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KAGGLE] Using Kaggle dataset: /kaggle/input/hong-kong-raw-pollutant-and-weather-dataset\n",
      "[FILES] Found 14 pollutant files:\n",
      "  [OK] Location 233335: pollutant_location_233335.csv + weather_location_233335.csv\n",
      "  [OK] Location 7728: pollutant_location_7728.csv + weather_location_7728.csv\n",
      "  [OK] Location 7735: pollutant_location_7735.csv + weather_location_7735.csv\n",
      "  [OK] Location 7742: pollutant_location_7742.csv + weather_location_7742.csv\n",
      "  [OK] Location 7734: pollutant_location_7734.csv + weather_location_7734.csv\n",
      "  [OK] Location 7740: pollutant_location_7740.csv + weather_location_7740.csv\n",
      "  [OK] Location 7736: pollutant_location_7736.csv + weather_location_7736.csv\n",
      "  [OK] Location 7739: pollutant_location_7739.csv + weather_location_7739.csv\n",
      "  [OK] Location 7733: pollutant_location_7733.csv + weather_location_7733.csv\n",
      "  [OK] Location 7732: pollutant_location_7732.csv + weather_location_7732.csv\n",
      "  [OK] Location 7730: pollutant_location_7730.csv + weather_location_7730.csv\n",
      "  [OK] Location 7737: pollutant_location_7737.csv + weather_location_7737.csv\n",
      "  [OK] Location 7727: pollutant_location_7727.csv + weather_location_7727.csv\n",
      "  [OK] Location 233336: pollutant_location_233336.csv + weather_location_233336.csv\n",
      "\n",
      "[SUCCESS] Total locations to process: 14\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# ========================================\n",
    "# KAGGLE: Sử dụng đường dẫn Kaggle dataset\n",
    "# ========================================\n",
    "if IN_KAGGLE:\n",
    "    # [KAGGLE] Kaggle paths\n",
    "    # Format: /kaggle/input/{dataset-name}/\n",
    "    raw_data_path = Path(\"/kaggle/input/hong-kong-raw-pollutant-and-weather-dataset\")  # ← Thay tên dataset của bạn\n",
    "    print(f\"[KAGGLE] Using Kaggle dataset: {raw_data_path}\")\n",
    "    \n",
    "elif IN_COLAB:\n",
    "    # [COLAB] Colab: Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    raw_data_path = Path(\"/content/drive/MyDrive/pm25-data/raw\")  # ← Thay đường dẫn Drive của bạn\n",
    "    print(f\"[COLAB] Using Google Drive: {raw_data_path}\")\n",
    "    \n",
    "else:\n",
    "    # [LOCAL] Local path (giữ nguyên)\n",
    "    raw_data_path = Path(\"../data/raw\")\n",
    "    print(f\"[LOCAL] Using local path: {raw_data_path}\")\n",
    "\n",
    "# Tìm tất cả các file pollutant\n",
    "pollutant_files = list(raw_data_path.glob(\"pollutant_location_*.csv\"))\n",
    "\n",
    "print(f\"[FILES] Found {len(pollutant_files)} pollutant files:\")\n",
    "\n",
    "# Tạo mapping giữa pollutant và weather files\n",
    "location_mapping = {}\n",
    "\n",
    "for pollutant_file in pollutant_files:\n",
    "    # Extract location_id từ tên file: pollutant_location_7727.csv -> 7727\n",
    "    match = re.search(r'pollutant_location_(\\d+)\\.csv', pollutant_file.name)\n",
    "    \n",
    "    if match:\n",
    "        location_id = match.group(1)\n",
    "        weather_file = raw_data_path / f\"weather_location_{location_id}.csv\"\n",
    "        \n",
    "        # Kiểm tra file weather tương ứng có tồn tại không\n",
    "        if weather_file.exists():\n",
    "            location_mapping[location_id] = {\n",
    "                'pollutant': str(pollutant_file),\n",
    "                'weather': str(weather_file)\n",
    "            }\n",
    "            print(f\"  [OK] Location {location_id}: {pollutant_file.name} + {weather_file.name}\")\n",
    "        else:\n",
    "            print(f\"  [WARNING]  Location {location_id}: Missing weather file!\")\n",
    "\n",
    "print(f\"\\n[SUCCESS] Total locations to process: {len(location_mapping)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c7e70b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:28:32.903387Z",
     "iopub.status.busy": "2025-11-20T10:28:32.903100Z",
     "iopub.status.idle": "2025-11-20T10:28:32.909385Z",
     "shell.execute_reply": "2025-11-20T10:28:32.908747Z"
    },
    "papermill": {
     "duration": 0.016296,
     "end_time": "2025-11-20T10:28:32.910603",
     "exception": false,
     "start_time": "2025-11-20T10:28:32.894307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/kaggle/input/hong-kong-raw-pollutant-and-weather-dataset/pollutant_location_233335.csv'),\n",
       " PosixPath('/kaggle/input/hong-kong-raw-pollutant-and-weather-dataset/pollutant_location_7728.csv'),\n",
       " PosixPath('/kaggle/input/hong-kong-raw-pollutant-and-weather-dataset/pollutant_location_7735.csv'),\n",
       " PosixPath('/kaggle/input/hong-kong-raw-pollutant-and-weather-dataset/pollutant_location_7742.csv'),\n",
       " PosixPath('/kaggle/input/hong-kong-raw-pollutant-and-weather-dataset/pollutant_location_7734.csv'),\n",
       " PosixPath('/kaggle/input/hong-kong-raw-pollutant-and-weather-dataset/pollutant_location_7740.csv'),\n",
       " PosixPath('/kaggle/input/hong-kong-raw-pollutant-and-weather-dataset/pollutant_location_7736.csv'),\n",
       " PosixPath('/kaggle/input/hong-kong-raw-pollutant-and-weather-dataset/pollutant_location_7739.csv'),\n",
       " PosixPath('/kaggle/input/hong-kong-raw-pollutant-and-weather-dataset/pollutant_location_7733.csv'),\n",
       " PosixPath('/kaggle/input/hong-kong-raw-pollutant-and-weather-dataset/pollutant_location_7732.csv'),\n",
       " PosixPath('/kaggle/input/hong-kong-raw-pollutant-and-weather-dataset/pollutant_location_7730.csv'),\n",
       " PosixPath('/kaggle/input/hong-kong-raw-pollutant-and-weather-dataset/pollutant_location_7737.csv'),\n",
       " PosixPath('/kaggle/input/hong-kong-raw-pollutant-and-weather-dataset/pollutant_location_7727.csv'),\n",
       " PosixPath('/kaggle/input/hong-kong-raw-pollutant-and-weather-dataset/pollutant_location_233336.csv')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pollutant_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6c2886",
   "metadata": {
    "papermill": {
     "duration": 0.008175,
     "end_time": "2025-11-20T10:28:32.926869",
     "exception": false,
     "start_time": "2025-11-20T10:28:32.918694",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.2 Xử lý từng Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "687bf81e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:28:32.944607Z",
     "iopub.status.busy": "2025-11-20T10:28:32.944365Z",
     "iopub.status.idle": "2025-11-20T10:29:08.239451Z",
     "shell.execute_reply": "2025-11-20T10:29:08.238442Z"
    },
    "papermill": {
     "duration": 35.305843,
     "end_time": "2025-11-20T10:29:08.240953",
     "exception": false,
     "start_time": "2025-11-20T10:28:32.935110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PROCESSING] Processing Location 233335...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [DATA] Air quality (PM2.5, PM10, SO2, NO2): 83,970 records\n",
      "  [?]  Weather: 25,560 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] After join: 21,679 records\n",
      "\n",
      "[PROCESSING] Processing Location 7728...\n",
      "  [DATA] Air quality (PM2.5, PM10, SO2, NO2): 84,638 records\n",
      "  [?]  Weather: 25,560 records\n",
      "  [OK] After join: 21,676 records\n",
      "\n",
      "[PROCESSING] Processing Location 7735...\n",
      "  [DATA] Air quality (PM2.5, PM10, SO2, NO2): 83,373 records\n",
      "  [?]  Weather: 25,560 records\n",
      "  [OK] After join: 21,270 records\n",
      "\n",
      "[PROCESSING] Processing Location 7742...\n",
      "  [DATA] Air quality (PM2.5, PM10, SO2, NO2): 82,234 records\n",
      "  [?]  Weather: 25,560 records\n",
      "  [OK] After join: 21,087 records\n",
      "\n",
      "[PROCESSING] Processing Location 7734...\n",
      "  [DATA] Air quality (PM2.5, PM10, SO2, NO2): 82,570 records\n",
      "  [?]  Weather: 25,560 records\n",
      "  [OK] After join: 21,247 records\n",
      "\n",
      "[PROCESSING] Processing Location 7740...\n",
      "  [DATA] Air quality (PM2.5, PM10, SO2, NO2): 84,504 records\n",
      "  [?]  Weather: 25,560 records\n",
      "  [OK] After join: 21,687 records\n",
      "\n",
      "[PROCESSING] Processing Location 7736...\n",
      "  [DATA] Air quality (PM2.5, PM10, SO2, NO2): 83,295 records\n",
      "  [?]  Weather: 25,560 records\n",
      "  [OK] After join: 21,274 records\n",
      "\n",
      "[PROCESSING] Processing Location 7739...\n",
      "  [DATA] Air quality (PM2.5, PM10, SO2, NO2): 84,686 records\n",
      "  [?]  Weather: 25,560 records\n",
      "  [OK] After join: 21,721 records\n",
      "\n",
      "[PROCESSING] Processing Location 7733...\n",
      "  [DATA] Air quality (PM2.5, PM10, SO2, NO2): 83,047 records\n",
      "  [?]  Weather: 25,560 records\n",
      "  [OK] After join: 21,228 records\n",
      "\n",
      "[PROCESSING] Processing Location 7732...\n",
      "  [DATA] Air quality (PM2.5, PM10, SO2, NO2): 83,386 records\n",
      "  [?]  Weather: 25,560 records\n",
      "  [OK] After join: 21,299 records\n",
      "\n",
      "[PROCESSING] Processing Location 7730...\n",
      "  [DATA] Air quality (PM2.5, PM10, SO2, NO2): 82,776 records\n",
      "  [?]  Weather: 25,560 records\n",
      "  [OK] After join: 21,267 records\n",
      "\n",
      "[PROCESSING] Processing Location 7737...\n",
      "  [DATA] Air quality (PM2.5, PM10, SO2, NO2): 83,079 records\n",
      "  [?]  Weather: 25,560 records\n",
      "  [OK] After join: 21,268 records\n",
      "\n",
      "[PROCESSING] Processing Location 7727...\n",
      "  [DATA] Air quality (PM2.5, PM10, SO2, NO2): 86,684 records\n",
      "  [?]  Weather: 25,560 records\n",
      "  [OK] After join: 22,360 records\n",
      "\n",
      "[PROCESSING] Processing Location 233336...\n",
      "  [DATA] Air quality (PM2.5, PM10, SO2, NO2): 83,818 records\n",
      "  [?]  Weather: 25,560 records\n",
      "  [OK] After join: 21,255 records\n",
      "\n",
      "[SUCCESS] Processed 14 locations successfully!\n"
     ]
    }
   ],
   "source": [
    "# List để chứa dataframes của từng location\n",
    "all_locations_data = []\n",
    "\n",
    "for location_id, files in location_mapping.items():\n",
    "    print(f\"\\n[PROCESSING] Processing Location {location_id}...\")\n",
    "    \n",
    "    # Đọc pollutant data\n",
    "    df_air = spark.read.csv(\n",
    "        files['pollutant'],\n",
    "        header=True,\n",
    "        schema=openaq_schema\n",
    "    )\n",
    "    \n",
    "    # [?] LỌC CHỈ LẤY CÁC CHỈ SỐ QUAN TÂM: PM2.5, PM10, SO2, NO2\n",
    "    df_air = df_air.filter(\n",
    "        F.col(\"parameter\").isin([\"pm25\", \"pm10\", \"so2\", \"no2\"])\n",
    "    )\n",
    "    \n",
    "    # Đọc weather data\n",
    "    df_weather = spark.read.csv(\n",
    "        files['weather'],\n",
    "        header=True,\n",
    "        schema=weather_schema\n",
    "    )\n",
    "    \n",
    "    print(f\"  [DATA] Air quality (PM2.5, PM10, SO2, NO2): {df_air.count():,} records\")\n",
    "    print(f\"  [?]  Weather: {df_weather.count():,} records\")\n",
    "    \n",
    "    # Weather data - drop missing (ít missing)\n",
    "    df_weather_clean = df_weather.na.drop()\n",
    "    \n",
    "    # Pivot pollutant data\n",
    "    df_air_pivot = df_air.groupBy(\n",
    "        \"location_id\", \"location\", \"datetime\", \"lat\", \"lon\"\n",
    "    ).pivot(\"parameter\").agg(F.first(\"value\"))\n",
    "    \n",
    "    # Rename columns\n",
    "    column_mapping = {\n",
    "        \"pm25\": \"PM2_5\",\n",
    "        \"pm10\": \"PM10\",\n",
    "        \"no2\": \"NO2\",\n",
    "        \"so2\": \"SO2\"\n",
    "    }\n",
    "    \n",
    "    for old_name, new_name in column_mapping.items():\n",
    "        if old_name in df_air_pivot.columns:\n",
    "            df_air_pivot = df_air_pivot.withColumnRenamed(old_name, new_name)\n",
    "    \n",
    "    # Join với weather data (theo datetime)\n",
    "    df_location = df_air_pivot.join(\n",
    "        df_weather_clean,\n",
    "        df_air_pivot.datetime == df_weather_clean.time,\n",
    "        \"inner\"\n",
    "    ).drop(\"time\")\n",
    "    \n",
    "    print(f\"  [OK] After join: {df_location.count():,} records\")\n",
    "    \n",
    "    # Thêm vào list\n",
    "    all_locations_data.append(df_location)\n",
    "\n",
    "print(f\"\\n[SUCCESS] Processed {len(all_locations_data)} locations successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c16e11a",
   "metadata": {
    "papermill": {
     "duration": 0.009653,
     "end_time": "2025-11-20T10:29:08.260972",
     "exception": false,
     "start_time": "2025-11-20T10:29:08.251319",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.3 Gộp tất cả Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5564f56f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:29:08.281887Z",
     "iopub.status.busy": "2025-11-20T10:29:08.281255Z",
     "iopub.status.idle": "2025-11-20T10:29:39.586651Z",
     "shell.execute_reply": "2025-11-20T10:29:39.581582Z"
    },
    "papermill": {
     "duration": 31.318232,
     "end_time": "2025-11-20T10:29:39.588783",
     "exception": false,
     "start_time": "2025-11-20T10:29:08.270551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROCESSING] Combining 14 locations...\n",
      "⏳ Computing combined dataset (this may take a moment)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Combined dataset: 300,318 total records\n",
      "[SUCCESS] Number of locations: 14\n",
      "\n",
      "[METADATA] Sample records (unsorted):\n",
      "+-----------+------------+-------------------+--------+---------+----+----+-----+---+--------------+--------------------+--------------+------------------+----------------+-------------+\n",
      "|location_id|location    |datetime           |lat     |lon      |NO2 |PM10|PM2_5|SO2|temperature_2m|relative_humidity_2m|wind_speed_10m|wind_direction_10m|surface_pressure|precipitation|\n",
      "+-----------+------------+-------------------+--------+---------+----+----+-----+---+--------------+--------------------+--------------+------------------+----------------+-------------+\n",
      "|233335     |North-245631|2022-11-02 06:00:00|22.49671|114.12824|42.5|15.4|11.3 |1.8|18.0          |80.0                |22.8          |18.0              |1005.4          |0.5          |\n",
      "|233335     |North-245631|2022-11-03 15:00:00|22.49671|114.12824|20.4|10.6|6.7  |0.9|22.2          |91.0                |16.2          |102.0             |1010.5          |3.3          |\n",
      "|233335     |North-245631|2022-11-04 16:00:00|22.49671|114.12824|21.3|24.6|17.1 |2.3|23.0          |87.0                |12.2          |56.0              |1013.9          |0.1          |\n",
      "|233335     |North-245631|2022-11-05 11:00:00|22.49671|114.12824|50.2|27.0|14.1 |1.9|21.1          |78.0                |13.1          |32.0              |1019.0          |0.0          |\n",
      "|233335     |North-245631|2022-11-06 13:00:00|22.49671|114.12824|37.3|17.8|14.5 |1.2|20.4          |82.0                |10.8          |37.0              |1017.3          |0.4          |\n",
      "|233335     |North-245631|2022-11-06 15:00:00|22.49671|114.12824|35.8|18.7|14.1 |1.3|19.8          |86.0                |10.2          |42.0              |1015.6          |0.7          |\n",
      "|233335     |North-245631|2022-11-06 03:00:00|22.49671|114.12824|27.4|NULL|NULL |1.6|18.5          |87.0                |11.2          |48.0              |1017.0          |0.0          |\n",
      "|233335     |North-245631|2022-11-06 19:00:00|22.49671|114.12824|23.6|21.2|17.2 |0.9|19.1          |91.0                |11.5          |46.0              |1015.9          |0.0          |\n",
      "|233335     |North-245631|2022-11-08 07:00:00|22.49671|114.12824|37.5|17.3|15.9 |2.6|19.5          |92.0                |9.7           |39.0              |1016.1          |0.2          |\n",
      "|233335     |North-245631|2022-11-08 05:00:00|22.49671|114.12824|35.6|20.8|19.4 |2.3|19.5          |93.0                |8.8           |35.0              |1014.8          |0.0          |\n",
      "+-----------+------------+-------------------+--------+---------+----+----+-----+---+--------------+--------------------+--------------+------------------+----------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gộp tất cả locations lại\n",
    "print(f\"[PROCESSING] Combining {len(all_locations_data)} locations...\")\n",
    "\n",
    "df_combined = all_locations_data[0]\n",
    "for df in all_locations_data[1:]:\n",
    "    df_combined = df_combined.union(df)\n",
    "\n",
    "# OPTIMIZE: Cache để tránh recompute nhiều lần\n",
    "df_combined = df_combined.cache()\n",
    "\n",
    "# OPTIMIZE: Trigger action 1 lần, tránh count() nhiều lần\n",
    "print(\"⏳ Computing combined dataset (this may take a moment)...\")\n",
    "total_records = df_combined.count()\n",
    "num_locations = df_combined.select('location_id').distinct().count()\n",
    "\n",
    "print(f\"[SUCCESS] Combined dataset: {total_records:,} total records\")\n",
    "print(f\"[SUCCESS] Number of locations: {num_locations}\")\n",
    "\n",
    "# OPTIMIZE: Chỉ show sample, không orderBy toàn bộ dataset (rất chậm!)\n",
    "print(\"\\n[METADATA] Sample records (unsorted):\")\n",
    "df_combined.show(10, truncate=False)\n",
    "\n",
    "# OPTIONAL: Nếu cần sort, chỉ sort 1 partition nhỏ để xem\n",
    "# df_combined.orderBy(\"location_id\", \"datetime\").limit(50).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62606d40",
   "metadata": {
    "papermill": {
     "duration": 0.022358,
     "end_time": "2025-11-20T10:29:39.637407",
     "exception": false,
     "start_time": "2025-11-20T10:29:39.615049",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Tổng quan Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "491c3206",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:29:39.685461Z",
     "iopub.status.busy": "2025-11-20T10:29:39.683736Z",
     "iopub.status.idle": "2025-11-20T10:29:45.307486Z",
     "shell.execute_reply": "2025-11-20T10:29:45.303761Z"
    },
    "papermill": {
     "duration": 5.649894,
     "end_time": "2025-11-20T10:29:45.309716",
     "exception": false,
     "start_time": "2025-11-20T10:29:39.659822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATA] Dataset Overview by Location:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----+\n",
      "|location_id|location            |count|\n",
      "+-----------+--------------------+-----+\n",
      "|233335     |North-245631        |21679|\n",
      "|233336     |Southern-245632     |21255|\n",
      "|7727       |Tung Chung-7727     |22360|\n",
      "|7728       |Mong Kok-7728       |21676|\n",
      "|7730       |Central/Western-7730|21267|\n",
      "|7732       |Causeway Bay-7732   |21299|\n",
      "|7733       |Sha Tin-7733        |21228|\n",
      "|7734       |Sham Shui Po-7734   |21247|\n",
      "|7735       |Kwun Tong-7735      |21270|\n",
      "|7736       |Kwai Chung-7736     |21274|\n",
      "|7737       |Tai Po-7737         |21268|\n",
      "|7739       |Yuen Long-7739      |21721|\n",
      "|7740       |Tsuen Wan-7740      |21687|\n",
      "|7742       |Tuen Mun-7742       |2368 |\n",
      "|7742       |Tuen Mun-932161     |18719|\n",
      "+-----------+--------------------+-----+\n",
      "\n",
      "\n",
      "[?] Time Range by Location:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 644:===============================================>      (99 + 6) / 112]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-------------------+-------+\n",
      "|location_id|start_date         |end_date           |records|\n",
      "+-----------+-------------------+-------------------+-------+\n",
      "|233335     |2022-11-01 00:00:00|2025-09-30 16:00:00|21679  |\n",
      "|233336     |2022-11-01 00:00:00|2025-09-30 16:00:00|21255  |\n",
      "|7727       |2022-11-01 00:00:00|2025-09-30 16:00:00|22360  |\n",
      "|7728       |2022-11-01 00:00:00|2025-09-30 16:00:00|21676  |\n",
      "|7730       |2022-11-01 00:00:00|2025-09-30 16:00:00|21267  |\n",
      "|7732       |2022-11-01 00:00:00|2025-09-30 16:00:00|21299  |\n",
      "|7733       |2022-11-01 00:00:00|2025-09-30 16:00:00|21228  |\n",
      "|7734       |2022-11-01 00:00:00|2025-09-30 16:00:00|21247  |\n",
      "|7735       |2022-11-01 00:00:00|2025-09-30 16:00:00|21270  |\n",
      "|7736       |2022-11-01 00:00:00|2025-09-30 16:00:00|21274  |\n",
      "|7737       |2022-11-01 00:00:00|2025-09-30 16:00:00|21268  |\n",
      "|7739       |2022-11-01 00:00:00|2025-09-30 16:00:00|21721  |\n",
      "|7740       |2022-11-01 00:00:00|2025-09-30 16:00:00|21687  |\n",
      "|7742       |2022-11-01 00:00:00|2025-09-30 16:00:00|21087  |\n",
      "+-----------+-------------------+-------------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Thống kê theo location\n",
    "print(\"[DATA] Dataset Overview by Location:\")\n",
    "df_combined.groupBy(\"location_id\", \"location\").count().orderBy(\"location_id\").show(truncate=False)\n",
    "\n",
    "# Time range của từng location\n",
    "print(\"\\n[?] Time Range by Location:\")\n",
    "df_combined.groupBy(\"location_id\").agg(\n",
    "    F.min(\"datetime\").alias(\"start_date\"),\n",
    "    F.max(\"datetime\").alias(\"end_date\"),\n",
    "    F.count(\"*\").alias(\"records\")\n",
    ").orderBy(\"location_id\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65ccbc31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:29:45.416610Z",
     "iopub.status.busy": "2025-11-20T10:29:45.414264Z",
     "iopub.status.idle": "2025-11-20T10:30:31.301971Z",
     "shell.execute_reply": "2025-11-20T10:30:31.301127Z"
    },
    "papermill": {
     "duration": 45.938252,
     "end_time": "2025-11-20T10:30:31.304069",
     "exception": false,
     "start_time": "2025-11-20T10:29:45.365817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING]  Missing Values Summary:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NO2                      :    7,612 (  2.53%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PM10                     :    3,391 (  1.13%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PM2_5                    :   11,161 (  3.72%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  SO2                      :    7,424 (  2.47%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Kiểm tra missing values\n",
    "print(\"[WARNING]  Missing Values Summary:\")\n",
    "for col_name in df_combined.columns:\n",
    "    null_count = df_combined.filter(F.col(col_name).isNull()).count()\n",
    "    total = df_combined.count()\n",
    "    pct = (null_count / total) * 100\n",
    "    if null_count > 0:  # Chỉ hiển thị cột có missing\n",
    "        print(f\"  {col_name:25s}: {null_count:8,} ({pct:6.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a9d922a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:30:31.369855Z",
     "iopub.status.busy": "2025-11-20T10:30:31.369550Z",
     "iopub.status.idle": "2025-11-20T10:30:34.013522Z",
     "shell.execute_reply": "2025-11-20T10:30:34.012633Z"
    },
    "papermill": {
     "duration": 2.68063,
     "end_time": "2025-11-20T10:30:34.017878",
     "exception": false,
     "start_time": "2025-11-20T10:30:31.337248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[?] Overall Statistics:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/20 10:30:31 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 2473:==================================================> (108 + 4) / 112]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+-----------------+--------------------+------------------+------------------+\n",
      "|summary|             PM2_5|              PM10|               NO2|               SO2|   temperature_2m|relative_humidity_2m|    wind_speed_10m|     precipitation|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+--------------------+------------------+------------------+\n",
      "|  count|            289157|            296927|            292706|            292894|           300318|              300318|            300318|            300318|\n",
      "|   mean|15.684073703904803|25.418589417600963|  39.2034498780346|3.7197607325517086|23.17203064751364|   79.55055974000892|12.503190950925351|0.2767030281235236|\n",
      "| stddev|10.897151413820698| 19.75489546779464|26.134835003178324|2.4372186299768455|5.550822884231528|   15.46350485998542| 6.256373957792993|1.1696306615920375|\n",
      "|    min|               0.0|               0.0|               0.0|               0.0|              1.9|                16.0|               0.0|               0.0|\n",
      "|    max|             182.5|             401.7|             292.6|              76.9|             36.5|               100.0|              86.6|              53.2|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+--------------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Statistics tổng quan\n",
    "print(\"[?] Overall Statistics:\")\n",
    "df_combined.select(\n",
    "    \"PM2_5\", \"PM10\", \"NO2\", \"SO2\",\n",
    "    \"temperature_2m\", \"relative_humidity_2m\", \"wind_speed_10m\", \"precipitation\"\n",
    ").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90e44ae",
   "metadata": {
    "papermill": {
     "duration": 0.030872,
     "end_time": "2025-11-20T10:30:34.087938",
     "exception": false,
     "start_time": "2025-11-20T10:30:34.057066",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Làm sạch Dữ liệu\n",
    "\n",
    "**Quy trình làm sạch:**\n",
    "1. **Loại bỏ Outliers trước** - Để tránh giá trị cực đoan ảnh hưởng đến tính toán statistics\n",
    "2. **Fill Missing Values sau** - Imputation dựa trên dữ liệu đã loại bỏ outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d41c18",
   "metadata": {
    "papermill": {
     "duration": 0.03182,
     "end_time": "2025-11-20T10:30:34.151425",
     "exception": false,
     "start_time": "2025-11-20T10:30:34.119605",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.1. Loại bỏ Outliers\n",
    "\n",
    "Loại bỏ các giá trị cực đoan trước khi imputation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "160defdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:30:34.216370Z",
     "iopub.status.busy": "2025-11-20T10:30:34.215621Z",
     "iopub.status.idle": "2025-11-20T10:30:52.351622Z",
     "shell.execute_reply": "2025-11-20T10:30:52.349087Z"
    },
    "papermill": {
     "duration": 18.170165,
     "end_time": "2025-11-20T10:30:52.353326",
     "exception": false,
     "start_time": "2025-11-20T10:30:34.183161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATA] Outlier Removal:\n",
      "  Before: 300,318 records\n",
      "  After:  289,157 records\n",
      "  Removed: 11,161 records (3.72%)\n",
      "\n",
      "  [WARNING]  Note: Records with PM2.5 = null are REMOVED (target variable must have real values)\n",
      "\n",
      "[WARNING]  Missing values after outlier removal:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PM2_5     :        0 (  0.00%) [SUCCESS] (Target - must be 0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PM10      :      296 (  0.10%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NO2       :    7,361 (  2.55%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3063:=========================================>           (87 + 4) / 112]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  SO2       :    7,178 (  2.48%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Loại bỏ outliers theo WHO/EPA International Standards (cho dữ liệu Hong Kong)\n",
    "# [WARNING]  QUAN TRỌNG: PM2.5 là TARGET variable - PHẢI có giá trị thật!\n",
    "#     -> Records có PM2.5 = null sẽ BỊ LOẠI BỎ\n",
    "#     -> Chỉ các features khác (PM10, NO2, SO2) mới được phép null và impute sau\n",
    "\n",
    "df_no_outliers = df_combined.filter(\n",
    "    # [TARGET] TARGET: PM2.5 theo WHO Emergency threshold (không cho phép null)\n",
    "    (F.col(\"PM2_5\").isNotNull()) & \n",
    "    (F.col(\"PM2_5\") >= 0) & (F.col(\"PM2_5\") < 250) &  # WHO Emergency: 250 μg/m³\n",
    "    \n",
    "    # [DATA] FEATURES: WHO/EPA International Standards - Cho phép null, chỉ loại outliers\n",
    "    ((F.col(\"PM10\").isNull()) | ((F.col(\"PM10\") >= 0) & (F.col(\"PM10\") < 430))) &  # WHO Emergency: 430 μg/m³\n",
    "    ((F.col(\"NO2\").isNull()) | ((F.col(\"NO2\") >= 0) & (F.col(\"NO2\") < 400))) &     # WHO/EU: 400 μg/m³ (1-hour)\n",
    "    ((F.col(\"SO2\").isNull()) | ((F.col(\"SO2\") >= 0) & (F.col(\"SO2\") < 500))) &     # WHO/EU: 500 μg/m³ (10-min)\n",
    "    \n",
    "    # [?] WEATHER: WMO standards cho Hong Kong\n",
    "    (F.col(\"precipitation\") >= 0) & (F.col(\"precipitation\") < 100)  # WMO: 100mm/h extreme rain\n",
    ")\n",
    "\n",
    "records_before = df_combined.count()\n",
    "records_after = df_no_outliers.count()\n",
    "removed = records_before - records_after\n",
    "\n",
    "print(f\"[DATA] Outlier Removal:\")\n",
    "print(f\"  Before: {records_before:,} records\")\n",
    "print(f\"  After:  {records_after:,} records\")\n",
    "print(f\"  Removed: {removed:,} records ({removed/records_before*100:.2f}%)\")\n",
    "print(f\"\\n  [WARNING]  Note: Records with PM2.5 = null are REMOVED (target variable must have real values)\")\n",
    "\n",
    "# Kiểm tra missing values sau khi loại outliers\n",
    "print(\"\\n[WARNING]  Missing values after outlier removal:\")\n",
    "for col_name in [\"PM2_5\", \"PM10\", \"NO2\", \"SO2\"]:\n",
    "    if col_name in df_no_outliers.columns:\n",
    "        null_count = df_no_outliers.filter(F.col(col_name).isNull()).count()\n",
    "        total = df_no_outliers.count()\n",
    "        pct = (null_count / total) * 100\n",
    "        if null_count > 0:\n",
    "            print(f\"  {col_name:10s}: {null_count:8,} ({pct:6.2f}%)\")\n",
    "        elif col_name == \"PM2_5\":\n",
    "            print(f\"  {col_name:10s}: {null_count:8,} ({pct:6.2f}%) [SUCCESS] (Target - must be 0%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fd856a",
   "metadata": {
    "papermill": {
     "duration": 0.019614,
     "end_time": "2025-11-20T10:30:52.403875",
     "exception": false,
     "start_time": "2025-11-20T10:30:52.384261",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.2. Xử lý Missing Values (Interpolation)\n",
    "\n",
    "**Chiến lược Imputation cho Time Series:**\n",
    "- **PM2.5**: Đã loại bỏ tất cả records có null (target variable)\n",
    "- **PM10, NO2, SO2**: Sử dụng **Linear Interpolation** (tốt nhất cho time series)\n",
    "  - Bước 1: **Linear Interpolation** - Nội suy tuyến tính dựa trên giá trị trước & sau\n",
    "  - Bước 2: **Forward Fill** - Xử lý missing ở cuối chuỗi (không có giá trị sau)\n",
    "  - Bước 3: **Backward Fill** - Xử lý missing ở đầu chuỗi (không có giá trị trước)\n",
    "  - Bước 4: **Mean** - Backup cuối cùng (nếu còn missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68031ae3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:30:52.444624Z",
     "iopub.status.busy": "2025-11-20T10:30:52.444030Z",
     "iopub.status.idle": "2025-11-20T10:31:03.962066Z",
     "shell.execute_reply": "2025-11-20T10:31:03.960687Z"
    },
    "papermill": {
     "duration": 11.540306,
     "end_time": "2025-11-20T10:31:03.963703",
     "exception": false,
     "start_time": "2025-11-20T10:30:52.423397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROCESSING] Time Series Imputation Strategy (PySpark Native):\n",
      "   1. True Linear Interpolation - y = y₁ + (y₂-y₁) × (t-t₁)/(t₂-t₁)\n",
      "   2. Forward Fill - If only prev value available\n",
      "   3. Backward Fill - If only next value available\n",
      "   4. Null - If no surrounding values (rare)\n",
      "\n",
      "   Columns to impute: ['PM10', 'NO2', 'SO2']\n",
      "   PM2.5 NOT imputed (target variable - already removed nulls)\n",
      "   [?] Safe: Window partitioned by location_id (no cross-location interpolation)\n",
      "\n",
      "[WARNING]  Missing values BEFORE interpolation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PM10      :      296 (  0.10%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NO2       :    7,361 (  2.55%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3446:========================================>            (85 + 4) / 112]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  SO2       :    7,178 (  2.48%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Chiến lược Imputation cho Time Series Data\n",
    "# Sử dụng PySpark Window Functions - Nội suy tuyến tính dựa trên khoảng cách thời gian\n",
    "\n",
    "# List các cột FEATURES cần impute (KHÔNG bao gồm PM2.5 - target variable)\n",
    "pollutant_cols = [\"PM10\", \"NO2\", \"SO2\"]  # [WARNING] Không có PM2.5!\n",
    "\n",
    "print(f\"[PROCESSING] Time Series Imputation Strategy (PySpark Native):\")\n",
    "print(f\"   1. True Linear Interpolation - y = y₁ + (y₂-y₁) × (t-t₁)/(t₂-t₁)\")\n",
    "print(f\"   2. Forward Fill - If only prev value available\")\n",
    "print(f\"   3. Backward Fill - If only next value available\")\n",
    "print(f\"   4. Null - If no surrounding values (rare)\")\n",
    "print(f\"\\n   Columns to impute: {pollutant_cols}\")\n",
    "print(f\"   PM2.5 NOT imputed (target variable - already removed nulls)\")\n",
    "print(f\"   [?] Safe: Window partitioned by location_id (no cross-location interpolation)\\n\")\n",
    "\n",
    "# Cache để tăng performance\n",
    "df_filled = df_no_outliers.cache()\n",
    "\n",
    "# Kiểm tra missing TRƯỚC khi interpolate\n",
    "print(\"[WARNING]  Missing values BEFORE interpolation:\")\n",
    "for col_name in pollutant_cols:\n",
    "    if col_name in df_filled.columns:\n",
    "        null_count = df_filled.filter(F.col(col_name).isNull()).count()\n",
    "        total = df_filled.count()\n",
    "        pct = (null_count / total) * 100\n",
    "        if null_count > 0:\n",
    "            print(f\"  {col_name:10s}: {null_count:8,} ({pct:6.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45ca8801",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:31:04.013416Z",
     "iopub.status.busy": "2025-11-20T10:31:04.012877Z",
     "iopub.status.idle": "2025-11-20T10:39:36.323842Z",
     "shell.execute_reply": "2025-11-20T10:39:36.322645Z"
    },
    "papermill": {
     "duration": 512.335766,
     "end_time": "2025-11-20T10:39:36.325299",
     "exception": false,
     "start_time": "2025-11-20T10:31:03.989533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROCESSING] Applying true linear interpolation per location (PySpark native)...\n",
      "  ▶ Interpolating PM10... [OK]\n",
      "  ▶ Interpolating NO2... [OK]\n",
      "  ▶ Interpolating SO2... [OK]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3535:=================================================>      (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUCCESS] Linear interpolation completed! Total records: 289,157\n",
      "   [GEAR]  Method: True linear interpolation based on time distance (epoch)\n",
      "   [?] Safe: No cross-location interpolation (partitioned by location_id)\n",
      "   [RUN] Optimized: Native PySpark (no Pandas conversion)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Áp dụng True Linear Interpolation với PySpark (không dùng Pandas)\n",
    "# Nội suy tuyến tính dựa trên khoảng cách thời gian THỰC (epoch)\n",
    "# Window function đảm bảo KHÔNG nội suy chéo giữa các locations\n",
    "\n",
    "print(\"[PROCESSING] Applying true linear interpolation per location (PySpark native)...\")\n",
    "\n",
    "# Tạo cột epoch (timestamp dạng số) để tính toán khoảng cách thời gian\n",
    "df_filled = df_filled.withColumn(\"epoch\", F.col(\"datetime\").cast(\"long\"))\n",
    "\n",
    "# Định nghĩa Window cho từng location\n",
    "w_forward = (\n",
    "    Window.partitionBy(\"location_id\")\n",
    "    .orderBy(\"epoch\")\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    ")\n",
    "\n",
    "w_backward = (\n",
    "    Window.partitionBy(\"location_id\")\n",
    "    .orderBy(\"epoch\")\n",
    "    .rowsBetween(Window.currentRow, Window.unboundedFollowing)\n",
    ")\n",
    "\n",
    "# Xử lý từng pollutant column\n",
    "for col_name in pollutant_cols:\n",
    "    if col_name not in df_filled.columns:\n",
    "        continue\n",
    "    \n",
    "    print(f\"  ▶ Interpolating {col_name}...\", end=\" \", flush=True)\n",
    "    \n",
    "    # Bước 1: Tìm giá trị & timestamp TRƯỚC và SAU gần nhất (có giá trị non-null)\n",
    "    df_filled = (\n",
    "        df_filled\n",
    "        .withColumn(f\"{col_name}_prev_value\", F.last(col_name, True).over(w_forward))\n",
    "        .withColumn(f\"{col_name}_next_value\", F.first(col_name, True).over(w_backward))\n",
    "        .withColumn(f\"{col_name}_prev_time\", F.last(F.when(F.col(col_name).isNotNull(), F.col(\"epoch\")), True).over(w_forward))\n",
    "        .withColumn(f\"{col_name}_next_time\", F.first(F.when(F.col(col_name).isNotNull(), F.col(\"epoch\")), True).over(w_backward))\n",
    "    )\n",
    "    \n",
    "    # Bước 2: Tính toán Linear Interpolation theo công thức:\n",
    "    # y = y₁ + (y₂ - y₁) * (t - t₁) / (t₂ - t₁)\n",
    "    interpolated_value = (\n",
    "        F.col(f\"{col_name}_prev_value\") +\n",
    "        (F.col(f\"{col_name}_next_value\") - F.col(f\"{col_name}_prev_value\")) *\n",
    "        ((F.col(\"epoch\") - F.col(f\"{col_name}_prev_time\")) /\n",
    "         (F.col(f\"{col_name}_next_time\") - F.col(f\"{col_name}_prev_time\")))\n",
    "    )\n",
    "    \n",
    "    # Bước 3: Logic chọn giá trị cuối cùng với fallback\n",
    "    df_filled = df_filled.withColumn(\n",
    "        col_name,\n",
    "        F.when(F.col(col_name).isNotNull(), F.col(col_name))  # Giữ nguyên nếu có giá trị\n",
    "         .when(\n",
    "             # Linear interpolation nếu có cả prev & next và không chia 0\n",
    "             (F.col(f\"{col_name}_prev_value\").isNotNull()) &\n",
    "             (F.col(f\"{col_name}_next_value\").isNotNull()) &\n",
    "             ((F.col(f\"{col_name}_next_time\") - F.col(f\"{col_name}_prev_time\")) != 0),\n",
    "             interpolated_value\n",
    "         )\n",
    "         .when(F.col(f\"{col_name}_prev_value\").isNotNull(), F.col(f\"{col_name}_prev_value\"))  # Forward fill\n",
    "         .when(F.col(f\"{col_name}_next_value\").isNotNull(), F.col(f\"{col_name}_next_value\"))  # Backward fill\n",
    "         .otherwise(None)  # Vẫn null nếu không có data nào\n",
    "    )\n",
    "    \n",
    "    # Bước 4: Xóa các cột phụ để giảm memory\n",
    "    df_filled = df_filled.drop(\n",
    "        f\"{col_name}_prev_value\", f\"{col_name}_next_value\",\n",
    "        f\"{col_name}_prev_time\", f\"{col_name}_next_time\"\n",
    "    )\n",
    "    \n",
    "    print(\"[OK]\")\n",
    "\n",
    "# Cache kết quả sau khi interpolation\n",
    "df_filled = df_filled.cache()\n",
    "\n",
    "# Trigger computation và đếm records\n",
    "count = df_filled.count()\n",
    "print(f\"\\n[SUCCESS] Linear interpolation completed! Total records: {count:,}\")\n",
    "print(f\"   [GEAR]  Method: True linear interpolation based on time distance (epoch)\")\n",
    "print(f\"   [?] Safe: No cross-location interpolation (partitioned by location_id)\")\n",
    "print(f\"   [RUN] Optimized: Native PySpark (no Pandas conversion)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9740de10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:39:36.373211Z",
     "iopub.status.busy": "2025-11-20T10:39:36.372888Z",
     "iopub.status.idle": "2025-11-20T10:39:41.194978Z",
     "shell.execute_reply": "2025-11-20T10:39:41.193835Z"
    },
    "papermill": {
     "duration": 4.847058,
     "end_time": "2025-11-20T10:39:41.196521",
     "exception": false,
     "start_time": "2025-11-20T10:39:36.349463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[METADATA] Final Missing Values Check (After Interpolation):\n",
      "  PM2_5 (Target): 0 nulls [SUCCESS] (Must be 0)\n",
      "  PM10      : 0 nulls [SUCCESS]\n",
      "  NO2       : 0 nulls [SUCCESS]\n",
      "  SO2       : 0 nulls [SUCCESS]\n",
      "\n",
      "  [SUCCESS] No missing values in any feature columns!\n",
      "\n",
      "[DATA] Final Verification:\n",
      "  PM2_5     : 0 nulls [SUCCESS]\n",
      "  PM10      : 0 nulls [SUCCESS]\n",
      "  NO2       : 0 nulls [SUCCESS]\n",
      "  SO2       : 0 nulls [SUCCESS]\n",
      "\n",
      "[SUCCESS] Data cleaning completed with True Linear Interpolation!\n",
      "   Final dataset: 289,157 records\n",
      "   [WARNING]  All records have REAL PM2.5 values (target variable)\n",
      "   [SUCCESS] Features interpolated smoothly (time-based linear interpolation)\n",
      "   [SUCCESS] Edge cases (no surrounding data) removed\n",
      "   [RUN] Performance: Native PySpark (no Pandas conversion)\n"
     ]
    }
   ],
   "source": [
    "# Verify: PM2.5 không có null, các features khác không có null\n",
    "print(\"\\n[METADATA] Final Missing Values Check (After Interpolation):\")\n",
    "\n",
    "# Kiểm tra PM2.5 (target)\n",
    "pm25_nulls = df_filled.filter(F.col(\"PM2_5\").isNull()).count()\n",
    "print(f\"  PM2_5 (Target): {pm25_nulls:,} nulls [SUCCESS] (Must be 0)\")\n",
    "\n",
    "# Kiểm tra features\n",
    "total_nulls = 0\n",
    "for col_name in pollutant_cols:\n",
    "    if col_name in df_filled.columns:\n",
    "        null_count = df_filled.filter(F.col(col_name).isNull()).count()\n",
    "        total_nulls += null_count\n",
    "        if null_count > 0:\n",
    "            print(f\"  {col_name:10s}: {null_count:,} nulls [WARNING]\")\n",
    "        else:\n",
    "            print(f\"  {col_name:10s}: {null_count:,} nulls [SUCCESS]\")\n",
    "\n",
    "# Xử lý edge case: Drop records còn null (không có giá trị xung quanh để interpolate)\n",
    "if total_nulls > 0:\n",
    "    print(f\"\\n[WARNING]  Found {total_nulls} remaining nulls (edge cases with no surrounding data)\")\n",
    "    print(f\"   -> Dropping these records to ensure data quality...\")\n",
    "    \n",
    "    records_before_drop = df_filled.count()\n",
    "    \n",
    "    # Drop records có bất kỳ feature nào còn null\n",
    "    for col_name in pollutant_cols:\n",
    "        df_filled = df_filled.filter(F.col(col_name).isNotNull())\n",
    "    \n",
    "    records_after_drop = df_filled.count()\n",
    "    dropped = records_before_drop - records_after_drop\n",
    "    \n",
    "    print(f\"   Before drop: {records_before_drop:,} records\")\n",
    "    print(f\"   After drop:  {records_after_drop:,} records\")\n",
    "    print(f\"   Dropped:     {dropped:,} records ({dropped/records_before_drop*100:.2f}%)\")\n",
    "    print(f\"\\n   [SUCCESS] All feature columns now have 0 nulls!\")\n",
    "else:\n",
    "    print(\"\\n  [SUCCESS] No missing values in any feature columns!\")\n",
    "\n",
    "# Xóa cột epoch (đã dùng xong)\n",
    "df_filled = df_filled.drop(\"epoch\")\n",
    "\n",
    "# Verify lần cuối\n",
    "print(f\"\\n[DATA] Final Verification:\")\n",
    "for col_name in [\"PM2_5\"] + pollutant_cols:\n",
    "    if col_name in df_filled.columns:\n",
    "        null_count = df_filled.filter(F.col(col_name).isNull()).count()\n",
    "        print(f\"  {col_name:10s}: {null_count:,} nulls [SUCCESS]\")\n",
    "\n",
    "print(f\"\\n[SUCCESS] Data cleaning completed with True Linear Interpolation!\")\n",
    "print(f\"   Final dataset: {df_filled.count():,} records\")\n",
    "print(f\"   [WARNING]  All records have REAL PM2.5 values (target variable)\")\n",
    "print(f\"   [SUCCESS] Features interpolated smoothly (time-based linear interpolation)\")\n",
    "print(f\"   [SUCCESS] Edge cases (no surrounding data) removed\")\n",
    "print(f\"   [RUN] Performance: Native PySpark (no Pandas conversion)\")\n",
    "\n",
    "# Cập nhật df_combined với dữ liệu đã clean và sắp xếp\n",
    "df_combined = df_filled.orderBy(\"location_id\", \"datetime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b993e8",
   "metadata": {
    "papermill": {
     "duration": 0.033824,
     "end_time": "2025-11-20T10:39:41.253233",
     "exception": false,
     "start_time": "2025-11-20T10:39:41.219409",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Feature Engineering & Normalization\n",
    "\n",
    "**Quy trình ĐÚNG để tránh Data Leakage:**\n",
    "1. **Time Features** - Thêm cyclic encoding (sin/cos) và is_weekend (không cần normalize)\n",
    "2. **Temporal Split** - Chia train/validation/test theo thời gian (70/15/15)\n",
    "3. **Normalization** - Chuẩn hóa **CHỈ numerical GỐC** bằng Min-Max từ train set\n",
    "4. **Lag Features** - Tạo lag TỪ CÁC CỘT ĐÃ SCALE (giữ đúng scale relationship)\n",
    "5. **Model-Specific Datasets** - Chuẩn bị riêng cho Deep Learning và XGBoost\n",
    "6. **Null Handling** - Xử lý nulls trong lag features cuối cùng\n",
    "\n",
    "**[WARNING] QUAN TRỌNG:** Lag features phải tạo SAU khi normalize để giữ đúng mối quan hệ scale!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0eb422a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:39:41.306099Z",
     "iopub.status.busy": "2025-11-20T10:39:41.305807Z",
     "iopub.status.idle": "2025-11-20T10:39:42.234285Z",
     "shell.execute_reply": "2025-11-20T10:39:42.232495Z"
    },
    "papermill": {
     "duration": 0.953853,
     "end_time": "2025-11-20T10:39:42.236159",
     "exception": false,
     "start_time": "2025-11-20T10:39:41.282306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROCESSING] Step 1: Adding Time Features (No normalization needed)...\n",
      "[OK] Time features added successfully!\n",
      "[OK] Total records: 289,157\n",
      "[OK] Total columns: 23\n",
      "\n",
      "[METADATA] Time Features Created:\n",
      "  Cyclic (sin/cos): hour, month, day_of_week -> Already in [-1, 1]\n",
      "  Binary: is_weekend -> Already in [0, 1]\n",
      "  [SUCCESS] No normalization needed for time features!\n"
     ]
    }
   ],
   "source": [
    "# Bước 1: Thêm Time Features từ dữ liệu đã clean\n",
    "print(\"[PROCESSING] Step 1: Adding Time Features (No normalization needed)...\")\n",
    "\n",
    "import math\n",
    "\n",
    "df_features = df_combined \\\n",
    "    .withColumn(\"hour\", F.hour(\"datetime\")) \\\n",
    "    .withColumn(\"month\", F.month(\"datetime\")) \\\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(\"datetime\"))\n",
    "\n",
    "# Cyclic encoding cho hour (24h cycle)\n",
    "df_features = df_features \\\n",
    "    .withColumn(\"hour_sin\", F.sin(2 * math.pi * F.col(\"hour\") / 24)) \\\n",
    "    .withColumn(\"hour_cos\", F.cos(2 * math.pi * F.col(\"hour\") / 24))\n",
    "\n",
    "# Cyclic encoding cho month (12 month cycle)\n",
    "df_features = df_features \\\n",
    "    .withColumn(\"month_sin\", F.sin(2 * math.pi * F.col(\"month\") / 12)) \\\n",
    "    .withColumn(\"month_cos\", F.cos(2 * math.pi * F.col(\"month\") / 12))\n",
    "\n",
    "# Cyclic encoding cho day_of_week (7 day cycle)\n",
    "df_features = df_features \\\n",
    "    .withColumn(\"day_of_week_sin\", F.sin(2 * math.pi * F.col(\"day_of_week\") / 7)) \\\n",
    "    .withColumn(\"day_of_week_cos\", F.cos(2 * math.pi * F.col(\"day_of_week\") / 7))\n",
    "\n",
    "# Binary feature: is_weekend\n",
    "df_features = df_features \\\n",
    "    .withColumn(\"is_weekend\", F.when(F.col(\"day_of_week\").isin([1, 7]), 1).otherwise(0))\n",
    "\n",
    "# ✅ FIX: Thêm cyclic encoding cho wind_direction\n",
    "df_features = df_features \\\n",
    "    .withColumn(\"wind_direction_sin\", F.sin(2 * math.pi * F.col(\"wind_direction_10m\") / 360)) \\\n",
    "    .withColumn(\"wind_direction_cos\", F.cos(2 * math.pi * F.col(\"wind_direction_10m\") / 360))\n",
    "\n",
    "# Xóa các cột trung gian\n",
    "df_features = df_features.drop(\"hour\", \"month\", \"day_of_week\", \"wind_direction_10m\")\n",
    "\n",
    "print(\"[OK] Time features added successfully!\")\n",
    "print(f\"[OK] Total records: {df_features.count():,}\")\n",
    "print(f\"[OK] Total columns: {len(df_features.columns)}\")\n",
    "\n",
    "print(\"\\n[METADATA] Time Features Created:\")\n",
    "print(\"  Cyclic (sin/cos): hour, month, day_of_week -> Already in [-1, 1]\")\n",
    "print(\"  Binary: is_weekend -> Already in [0, 1]\")\n",
    "print(\"  [SUCCESS] No normalization needed for time features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d48dff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:39:42.306322Z",
     "iopub.status.busy": "2025-11-20T10:39:42.305954Z",
     "iopub.status.idle": "2025-11-20T10:39:45.261353Z",
     "shell.execute_reply": "2025-11-20T10:39:45.260566Z"
    },
    "papermill": {
     "duration": 2.987047,
     "end_time": "2025-11-20T10:39:45.262958",
     "exception": false,
     "start_time": "2025-11-20T10:39:42.275911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PROCESSING] Step 2: Temporal Train/Val/Test Split BEFORE Normalization...\n",
      "[?] Temporal Split (Avoiding Data Leakage):\n",
      "  [?] Train:      2022-11-01 -> 2024-11-14 (744 days)\n",
      "  [?] Validation: 2024-11-14 -> 2025-04-22 (159 days)\n",
      "  [?] Test:       2025-04-22 -> 2025-09-30 (161 days)\n",
      "\n",
      "[DATA] Split Results:\n",
      "  [?] Train:  187,587 (64.9%)\n",
      "  [?] Val:     49,956 (17.3%)\n",
      "  [?] Test:    51,614 (17.8%)\n",
      "\n",
      "[SUCCESS] Temporal split completed!\n",
      "   [WARNING]  Next: Normalize using TRAIN SET statistics ONLY\n"
     ]
    }
   ],
   "source": [
    "# Bước 2: STRATIFIED TEMPORAL SPLIT (Giải quyết Distribution Shift)\n",
    "print(\"\\n[PROCESSING] Step 2: STRATIFIED Temporal Train/Val/Test Split...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ========================================\n",
    "# STRATIFIED TEMPORAL SPLIT\n",
    "# ========================================\n",
    "# Vấn đề với Simple Temporal Split:\n",
    "# - Train: Nov 2022 - Nov 2024 (chủ yếu mùa đông/xuân)\n",
    "# - Val: Nov 2024 - Apr 2025 (mùa đông - ô nhiễm CAO)\n",
    "# - Test: Apr 2025 - Sep 2025 (mùa hè - ô nhiễm THẤP)\n",
    "# => Phân phối KHÔNG đồng nhất giữa các tập\n",
    "#\n",
    "# Giải pháp: Stratified Temporal Split\n",
    "# - Chia theo THÁNG, mỗi tháng chia 70/15/15\n",
    "# - Đảm bảo mỗi tập có đủ dữ liệu từ TẤT CẢ các mùa\n",
    "# - Vẫn giữ temporal order trong từng tháng (tránh data leakage)\n",
    "\n",
    "print(\"[STRATEGY] Stratified Temporal Split:\")\n",
    "print(\"   - Split by MONTH: Each month divided 70/15/15\")\n",
    "print(\"   - Ensures ALL seasons represented in train/val/test\")\n",
    "print(\"   - Maintains temporal order WITHIN each month\")\n",
    "print(\"   - Reduces distribution shift between splits\")\n",
    "\n",
    "# Thêm cột month để stratify\n",
    "df_with_month = df_features.withColumn(\"split_month\", F.date_format(\"datetime\", \"yyyy-MM\"))\n",
    "\n",
    "# Lấy danh sách các tháng\n",
    "months = df_with_month.select(\"split_month\").distinct().orderBy(\"split_month\").collect()\n",
    "months = [row[\"split_month\"] for row in months]\n",
    "\n",
    "print(f\"\\n[DATA] Total months in dataset: {len(months)}\")\n",
    "print(f\"   First month: {months[0]}\")\n",
    "print(f\"   Last month: {months[-1]}\")\n",
    "\n",
    "# Hàm chia stratified cho mỗi tháng\n",
    "def stratified_split_by_month(df, train_ratio=0.70, val_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Split data by month, ensuring each month contributes to all splits\n",
    "    Maintains temporal order within each month\n",
    "    \"\"\"\n",
    "    from pyspark.sql.window import Window\n",
    "    \n",
    "    # Window để rank theo thời gian trong từng tháng và location\n",
    "    w = Window.partitionBy(\"split_month\", \"location_id\").orderBy(\"datetime\")\n",
    "    \n",
    "    # Thêm row number và tổng số rows trong mỗi group\n",
    "    df_ranked = df.withColumn(\"row_num\", F.row_number().over(w))\n",
    "    \n",
    "    # Tính tổng số records trong mỗi tháng-location\n",
    "    w_count = Window.partitionBy(\"split_month\", \"location_id\")\n",
    "    df_ranked = df_ranked.withColumn(\"total_rows\", F.count(\"*\").over(w_count))\n",
    "    \n",
    "    # Tính cutoff points\n",
    "    df_ranked = df_ranked.withColumn(\n",
    "        \"train_cutoff\", (F.col(\"total_rows\") * train_ratio).cast(\"int\")\n",
    "    ).withColumn(\n",
    "        \"val_cutoff\", (F.col(\"total_rows\") * (train_ratio + val_ratio)).cast(\"int\")\n",
    "    )\n",
    "    \n",
    "    # Assign split based on row position\n",
    "    df_split = df_ranked.withColumn(\n",
    "        \"split_type\",\n",
    "        F.when(F.col(\"row_num\") <= F.col(\"train_cutoff\"), \"train\")\n",
    "         .when(F.col(\"row_num\") <= F.col(\"val_cutoff\"), \"val\")\n",
    "         .otherwise(\"test\")\n",
    "    )\n",
    "    \n",
    "    # Drop helper columns\n",
    "    df_split = df_split.drop(\"row_num\", \"total_rows\", \"train_cutoff\", \"val_cutoff\", \"split_month\")\n",
    "    \n",
    "    return df_split\n",
    "\n",
    "print(\"\\n[PROCESSING] Applying stratified split...\")\n",
    "df_stratified = stratified_split_by_month(df_with_month, train_ratio=0.70, val_ratio=0.15)\n",
    "\n",
    "# Cache và split\n",
    "df_stratified = df_stratified.cache()\n",
    "\n",
    "df_train_raw = df_stratified.filter(F.col(\"split_type\") == \"train\").drop(\"split_type\")\n",
    "df_val_raw = df_stratified.filter(F.col(\"split_type\") == \"val\").drop(\"split_type\")\n",
    "df_test_raw = df_stratified.filter(F.col(\"split_type\") == \"test\").drop(\"split_type\")\n",
    "\n",
    "# Count\n",
    "train_count = df_train_raw.count()\n",
    "val_count = df_val_raw.count()\n",
    "test_count = df_test_raw.count()\n",
    "total_count = train_count + val_count + test_count\n",
    "\n",
    "print(f\"\\n[DATA] Stratified Split Results:\")\n",
    "print(f\"  Train: {train_count:8,} ({train_count/total_count*100:.1f}%)\")\n",
    "print(f\"  Val:   {val_count:8,} ({val_count/total_count*100:.1f}%)\")\n",
    "print(f\"  Test:  {test_count:8,} ({test_count/total_count*100:.1f}%)\")\n",
    "\n",
    "# Verify distribution similarity\n",
    "print(f\"\\n[VERIFY] Checking PM2.5 distribution across splits...\")\n",
    "train_stats = df_train_raw.select(F.mean(\"PM2_5\").alias(\"mean\"), F.stddev(\"PM2_5\").alias(\"std\")).collect()[0]\n",
    "val_stats = df_val_raw.select(F.mean(\"PM2_5\").alias(\"mean\"), F.stddev(\"PM2_5\").alias(\"std\")).collect()[0]\n",
    "test_stats = df_test_raw.select(F.mean(\"PM2_5\").alias(\"mean\"), F.stddev(\"PM2_5\").alias(\"std\")).collect()[0]\n",
    "\n",
    "print(f\"   Train: mean={train_stats['mean']:.2f}, std={train_stats['std']:.2f}\")\n",
    "print(f\"   Val:   mean={val_stats['mean']:.2f}, std={val_stats['std']:.2f}\")\n",
    "print(f\"   Test:  mean={test_stats['mean']:.2f}, std={test_stats['std']:.2f}\")\n",
    "\n",
    "# Calculate improvement\n",
    "mean_diff_val = abs(val_stats['mean'] - train_stats['mean']) / train_stats['mean'] * 100\n",
    "mean_diff_test = abs(test_stats['mean'] - train_stats['mean']) / train_stats['mean'] * 100\n",
    "\n",
    "print(f\"\\n[IMPROVEMENT] Distribution alignment:\")\n",
    "print(f\"   Train-Val difference:  {mean_diff_val:.1f}% (target: < 10%)\")\n",
    "print(f\"   Train-Test difference: {mean_diff_test:.1f}% (target: < 10%)\")\n",
    "\n",
    "if mean_diff_val < 15 and mean_diff_test < 15:\n",
    "    print(f\"   [SUCCESS] Stratified split significantly reduced distribution shift!\")\n",
    "else:\n",
    "    print(f\"   [WARNING] Some distribution shift remains - consider additional techniques\")\n",
    "\n",
    "# Cache final splits\n",
    "df_train_raw = df_train_raw.cache()\n",
    "df_val_raw = df_val_raw.cache()\n",
    "df_test_raw = df_test_raw.cache()\n",
    "\n",
    "# Cleanup\n",
    "df_stratified.unpersist()\n",
    "\n",
    "print(f\"\\n[SUCCESS] Stratified temporal split completed!\")\n",
    "print(f\"   Each split now contains data from ALL seasons\")\n",
    "print(f\"   Next: Normalize using TRAIN SET statistics ONLY\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16ac8fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:39:45.310079Z",
     "iopub.status.busy": "2025-11-20T10:39:45.309779Z",
     "iopub.status.idle": "2025-11-20T10:39:58.673424Z",
     "shell.execute_reply": "2025-11-20T10:39:58.672446Z"
    },
    "papermill": {
     "duration": 13.388816,
     "end_time": "2025-11-20T10:39:58.674831",
     "exception": false,
     "start_time": "2025-11-20T10:39:45.286015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PROCESSING] Step 3: Normalize NUMERICAL BASE FEATURES using TRAIN SET ONLY...\n",
      "[DATA] Normalizing 9 BASE features (NO lag features yet)...\n",
      "   Features to normalize: ['PM2_5', 'PM10', 'NO2', 'SO2', 'temperature_2m', 'relative_humidity_2m', 'wind_speed_10m', 'surface_pressure', 'precipitation']\n",
      "   [WARNING]  Computing min/max from TRAIN SET ONLY (preventing data leakage)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] PM2_5                         : [    0.00,   152.40] -> [0, 1]\n",
      "  [OK] PM10                          : [    0.00,   227.30] -> [0, 1]\n",
      "  [OK] NO2                           : [    0.00,   292.60] -> [0, 1]\n",
      "  [OK] SO2                           : [    0.00,    76.90] -> [0, 1]\n",
      "  [OK] temperature_2m                : [    1.90,    36.00] -> [0, 1]\n",
      "  [OK] relative_humidity_2m          : [   22.00,   100.00] -> [0, 1]\n",
      "  [OK] wind_speed_10m                : [    0.00,    82.30] -> [0, 1]\n",
      "  [OK] surface_pressure              : [  981.50,  1032.50] -> [0, 1]\n",
      "  [OK] precipitation                 : [    0.00,    53.20] -> [0, 1]\n",
      "\n",
      "[SUCCESS] Scaler parameters computed from TRAIN SET only!\n",
      "\n",
      " Applying Min-Max scaling [0, 1] to all splits...\n",
      "[SUCCESS] Base feature normalization completed!\n",
      "   [DATA] All splits normalized using train statistics only\n",
      "   [WARNING]  Next: Create lag features FROM SCALED COLUMNS\n"
     ]
    }
   ],
   "source": [
    "# Bước 3: Normalize NUMERICAL GỐC (CHỈ gốc, KHÔNG có lag features)\n",
    "print(f\"\\n[PROCESSING] Step 3: Normalize NUMERICAL BASE FEATURES using TRAIN SET ONLY...\")\n",
    "\n",
    "# ========================================\n",
    "# LOG TRANSFORMATION FOR SKEWED TARGET (PM2.5)\n",
    "# ========================================\n",
    "# PM2.5 distribution is highly skewed (skewness=1.53)\n",
    "# Apply log1p transformation to reduce skewness and improve model learning\n",
    "print(f\"[LOG TRANSFORM] Applying log1p transformation to PM2.5 (target)...\")\n",
    "print(f\"   Reason: PM2.5 distribution is highly skewed (skewness > 1.5)\")\n",
    "print(f\"   Formula: PM2_5_log = log(1 + PM2_5)\")\n",
    "\n",
    "df_train_raw = df_train_raw.withColumn(\"PM2_5_log\", F.log1p(F.col(\"PM2_5\")))\n",
    "df_val_raw = df_val_raw.withColumn(\"PM2_5_log\", F.log1p(F.col(\"PM2_5\")))\n",
    "df_test_raw = df_test_raw.withColumn(\"PM2_5_log\", F.log1p(F.col(\"PM2_5\")))\n",
    "\n",
    "# Verify log transformation\n",
    "log_stats = df_train_raw.select(\n",
    "    F.mean(\"PM2_5\").alias(\"original_mean\"),\n",
    "    F.stddev(\"PM2_5\").alias(\"original_std\"),\n",
    "    F.mean(\"PM2_5_log\").alias(\"log_mean\"),\n",
    "    F.stddev(\"PM2_5_log\").alias(\"log_std\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"   Original PM2.5: mean={log_stats['original_mean']:.2f}, std={log_stats['original_std']:.2f}\")\n",
    "print(f\"   Log PM2.5:      mean={log_stats['log_mean']:.2f}, std={log_stats['log_std']:.2f}\")\n",
    "print(f\"[SUCCESS] Log transformation applied!\")\n",
    "\n",
    "# [WARNING] QUAN TRỌNG: CHỈ normalize các cột GỐC, KHÔNG bao gồm lag features\n",
    "# Lag features sẽ tạo SAU từ các cột đã scale\n",
    "# Use PM2_5_log instead of PM2_5 for target normalization\n",
    "numerical_base_cols = [\n",
    "    # Target with log transformation\n",
    "    \"PM2_5_log\",\n",
    "    # Other Pollutants (current values only)\n",
    "    \"PM10\", \"NO2\", \"SO2\",\n",
    "    # Weather features (current values only)\n",
    "    \"temperature_2m\", \"relative_humidity_2m\", \"wind_speed_10m\",\n",
    "    \"surface_pressure\", \"precipitation\"\n",
    "]\n",
    "\n",
    "print(f\"[DATA] Normalizing {len(numerical_base_cols)} BASE features (NO lag features yet)...\")\n",
    "print(f\"   Features to normalize: {numerical_base_cols}\")\n",
    "print(f\"   [WARNING]  Computing min/max from TRAIN SET ONLY (preventing data leakage)\")\n",
    "\n",
    "# Tính min/max CHỈ TỪ TRAIN SET\n",
    "scaler_params = {}\n",
    "\n",
    "for col_name in numerical_base_cols:\n",
    "    if col_name in df_train_raw.columns:\n",
    "        # CHỈ DÙNG TRAIN SET ĐỂ TÍNH MIN/MAX  \n",
    "        stats = df_train_raw.select(\n",
    "            F.min(col_name).alias(\"min\"),\n",
    "            F.max(col_name).alias(\"max\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        min_val = stats[\"min\"]\n",
    "        max_val = stats[\"max\"]\n",
    "        \n",
    "        # [WARNING] CRITICAL: Handle None values from null columns\n",
    "        if min_val is None or max_val is None:\n",
    "            print(f\"  [WARNING]  Skipping {col_name}: All values are null\")\n",
    "            continue\n",
    "        \n",
    "        # [WARNING] CRITICAL: Tránh chia 0 khi min = max\n",
    "        if max_val == min_val:\n",
    "            max_val = min_val + 1\n",
    "        \n",
    "        scaler_params[col_name] = {\"min\": min_val, \"max\": max_val}\n",
    "        print(f\"  [OK] {col_name:30s}: [{min_val:8.2f}, {max_val:8.2f}] -> [0, 1]\")\n",
    "\n",
    "print(f\"\\n[SUCCESS] Scaler parameters computed from TRAIN SET only!\")\n",
    "\n",
    "# Áp dụng normalization cho tất cả splits\n",
    "def apply_scaling(df, scaler_params):\n",
    "    \"\"\"Apply Min-Max scaling using precomputed parameters\"\"\"\n",
    "    df_scaled = df\n",
    "    for col_name, params in scaler_params.items():\n",
    "        if col_name in df.columns:\n",
    "            min_val = params[\"min\"]\n",
    "            max_val = params[\"max\"]\n",
    "            df_scaled = df_scaled.withColumn(\n",
    "                f\"{col_name}_scaled\",\n",
    "                (F.col(col_name) - min_val) / (max_val - min_val)\n",
    "            )\n",
    "    return df_scaled\n",
    "\n",
    "print(f\"\\n Applying Min-Max scaling [0, 1] to all splits...\")\n",
    "\n",
    "# Apply scaling and trigger computation\n",
    "df_train = apply_scaling(df_train_raw, scaler_params)\n",
    "df_val = apply_scaling(df_val_raw, scaler_params)\n",
    "df_test = apply_scaling(df_test_raw, scaler_params)\n",
    "\n",
    "# Trigger computation and cache\n",
    "_ = df_train.count()\n",
    "_ = df_val.count()\n",
    "_ = df_test.count()\n",
    "\n",
    "df_train = df_train.cache()\n",
    "df_val = df_val.cache()\n",
    "df_test = df_test.cache()\n",
    "\n",
    "# Unpersist raw versions to free memory\n",
    "df_train_raw.unpersist()\n",
    "df_val_raw.unpersist()\n",
    "df_test_raw.unpersist()\n",
    "\n",
    "print(f\"[SUCCESS] Base feature normalization completed!\")\n",
    "print(f\"   [DATA] All splits normalized using train statistics only\")\n",
    "print(f\"   [WARNING]  Next: Create lag features FROM SCALED COLUMNS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a83d1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:39:58.736819Z",
     "iopub.status.busy": "2025-11-20T10:39:58.736537Z",
     "iopub.status.idle": "2025-11-20T10:39:58.745889Z",
     "shell.execute_reply": "2025-11-20T10:39:58.744994Z"
    },
    "papermill": {
     "duration": 0.047192,
     "end_time": "2025-11-20T10:39:58.747182",
     "exception": false,
     "start_time": "2025-11-20T10:39:58.699990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SAVE] Step 4: Saving Scaler Parameters...\n",
      "[KAGGLE] Kaggle mode: Saving to /kaggle/working/processed\n",
      "[SUCCESS] Scaler parameters saved to: /kaggle/working/processed/scaler_params.json\n",
      "   - Computed from TRAIN SET only (no data leakage)\n",
      "   - Used for denormalizing predictions during inference\n",
      "   - Contains 9 base features\n",
      "\n",
      "[METADATA] Example scaler params (from train set):\n",
      "  PM2_5               : min=0.00, max=152.40\n",
      "  temperature_2m      : min=1.90, max=36.00\n",
      "  wind_speed_10m      : min=0.00, max=82.30\n"
     ]
    }
   ],
   "source": [
    "# Bước 4: Lưu Scaler Parameters\n",
    "print(f\"\\n[SAVE] Step 4: Saving Scaler Parameters...\")\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Include log transformation info in scaler params\n",
    "scaler_json = {\n",
    "    col: {\"min\": float(params[\"min\"]), \"max\": float(params[\"max\"])} \n",
    "    for col, params in scaler_params.items()\n",
    "}\n",
    "\n",
    "# Add metadata for log transformation (needed for inverse transform during inference)\n",
    "scaler_json[\"_metadata\"] = {\n",
    "    \"log_transformed_features\": [\"PM2_5\"],  # Features that were log1p transformed\n",
    "    \"target_feature\": \"PM2_5_log_scaled\",\n",
    "    \"inverse_transform_order\": [\"denormalize\", \"expm1\"]  # Order: first denormalize, then exp(x)-1\n",
    "}\n",
    "\n",
    "# ========================================\n",
    "# ADAPTIVE PATH (Kaggle vs Colab vs Local)\n",
    "# ========================================\n",
    "if IN_KAGGLE:\n",
    "    # [KAGGLE] Kaggle: Write to /kaggle/working (auto-saved on commit)\n",
    "    processed_dir = Path(\"/kaggle/working/processed\")\n",
    "    print(f\"[KAGGLE] Kaggle mode: Saving to {processed_dir}\")\n",
    "    \n",
    "elif IN_COLAB:\n",
    "    # [COLAB] Colab: Write to Google Drive\n",
    "    processed_dir = Path(\"/content/drive/MyDrive/pm25-data/processed\")\n",
    "    print(f\"[COLAB] Colab mode: Saving to Google Drive\")\n",
    "    \n",
    "else:\n",
    "    # [LOCAL] Local: Write to project folder\n",
    "    processed_dir = Path(\"../data/processed\")\n",
    "    print(f\"[LOCAL] Local mode: Saving to {processed_dir}\")\n",
    "\n",
    "# Tạo thư mục với parents=True (tạo cả parent directories nếu chưa có)\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Lưu ra file JSON\n",
    "scaler_path = processed_dir / \"scaler_params.json\"\n",
    "with open(scaler_path, 'w') as f:\n",
    "    json.dump(scaler_json, f, indent=2)\n",
    "\n",
    "print(f\"[SUCCESS] Scaler parameters saved to: {scaler_path}\")\n",
    "print(f\"   - Computed from TRAIN SET only (no data leakage)\")\n",
    "print(f\"   - Used for denormalizing predictions during inference\")\n",
    "print(f\"   - Contains {len(scaler_params)} base features\")\n",
    "\n",
    "# Hiển thị ví dụ\n",
    "print(f\"\\n[METADATA] Example scaler params (from train set):\")\n",
    "example_cols = [\"PM2_5\", \"temperature_2m\", \"wind_speed_10m\"]\n",
    "for col in example_cols:\n",
    "    if col in scaler_params:\n",
    "        params = scaler_params[col]\n",
    "        print(f\"  {col:20s}: min={params['min']:.2f}, max={params['max']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a194c51a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:39:58.796181Z",
     "iopub.status.busy": "2025-11-20T10:39:58.795645Z",
     "iopub.status.idle": "2025-11-20T10:40:58.223457Z",
     "shell.execute_reply": "2025-11-20T10:40:58.222702Z"
    },
    "papermill": {
     "duration": 59.454423,
     "end_time": "2025-11-20T10:40:58.224966",
     "exception": false,
     "start_time": "2025-11-20T10:39:58.770543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PROCESSING] Step 5: Creating Lag Features FROM SCALED COLUMNS (XGBoost only)...\n",
      "\n",
      "[METADATA] Creating lag features:\n",
      "   Deep Learning models: No lags needed (learn from sequences)\n",
      "   XGBoost: 6 lags × 9 variables = 54 features\n",
      "   [SUCCESS] Using SCALED columns as source (proper scale relationship)\n",
      "\n",
      "[PROCESSING] Creating lag features for all splits...\n",
      "  [OK] Train: 54 lag features created\n",
      "  [OK] Val:   54 lag features created\n",
      "  [OK] Test:  54 lag features created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUCCESS] Lag features created successfully!\n",
      "   [SUCCESS] All lags created FROM SCALED columns\n",
      "   [SUCCESS] Lag and base features have SAME scale parameters\n",
      "   [SUCCESS] Proper temporal relationship preserved\n",
      "\n",
      "[PROCESSING] Handling null values in lag features...\n",
      "\n",
      "[DATA] Null counts BEFORE handling:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PM2_5_lag1_scaled                  :       14 nulls (0.01%)\n",
      "  PM2_5_lag2_scaled                  :       28 nulls (0.01%)\n",
      "  PM2_5_lag3_scaled                  :       42 nulls (0.02%)\n",
      "\n",
      "[WARNING]  Reason: First 24 hours of each location have no previous data\n",
      "   Strategy: DROP records with ANY null lag feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[?]  Dropping records with null lag features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DATA] Records dropped (null lag features):\n",
      "  [?] Train: 187,587 -> 187,251 (dropped 336, 0.18%)\n",
      "  [?] Val:   49,956 -> 49,620 (dropped 336, 0.67%)\n",
      "  [?] Test:  51,614 -> 51,278 (dropped 336, 0.65%)\n",
      "\n",
      "[SUCCESS] Verification - checking for remaining nulls...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PM2_5_lag1_scaled                  :        0 nulls [SUCCESS]\n",
      "  PM2_5_lag2_scaled                  :        0 nulls [SUCCESS]\n",
      "  PM2_5_lag3_scaled                  :        0 nulls [SUCCESS]\n",
      "\n",
      "[SUCCESS] All lag features are clean!\n",
      "\n",
      "[SUCCESS] Lag features + Null handling completed!\n",
      "   - Created 54 lag features FROM SCALED columns\n",
      "   - Lost only first 24 hours per location\n",
      "   - All lag features now have valid values\n",
      "   - Data quality ensured for XGBoost training\n"
     ]
    }
   ],
   "source": [
    "# Bước 5: Tạo Lag Features TỪ CÁC CỘT ĐÃ SCALE (CHỈ CHO XGBOOST)\n",
    "print(f\"\\n[PROCESSING] Step 5: Creating Lag Features FROM SCALED COLUMNS (XGBoost only)...\")\n",
    "\n",
    "# [WARNING] QUAN TRỌNG: Lag features được tạo TỪ CÁC CỘT ĐÃ SCALE\n",
    "# -> Đảm bảo lag và gốc có CÙNG SCALE PARAMETERS\n",
    "# -> Giữ đúng mối quan hệ giữa giá trị hiện tại và quá khứ\n",
    "\n",
    "LAG_STEPS = [1, 2, 3, 6, 12, 24]  # 1h, 2h, 3h, 6h, 12h, 24h trước\n",
    "\n",
    "# Columns cần tạo lag (sử dụng bản SCALED)\n",
    "# Note: Use PM2_5_log for lags as well (log transformed version)\n",
    "lag_base_columns = [\"PM2_5_log\", \"PM10\", \"NO2\", \"SO2\", \n",
    "                    \"temperature_2m\", \"relative_humidity_2m\", \n",
    "                    \"wind_speed_10m\", \"surface_pressure\", \"precipitation\"]\n",
    "\n",
    "print(f\"\\n[METADATA] Creating lag features:\")\n",
    "print(f\"   Deep Learning models: No lags needed (learn from sequences)\")\n",
    "print(f\"   XGBoost: {len(LAG_STEPS)} lags x {len(lag_base_columns)} variables = {len(LAG_STEPS) * len(lag_base_columns)} features\")\n",
    "print(f\"   [SUCCESS] Using SCALED columns as source (proper scale relationship)\")\n",
    "print(f\"   [NOTE] PM2_5 lags use log transformed version for consistency\")\n",
    "\n",
    "# Window cho từng location (sắp xếp theo thời gian)\n",
    "w_lag = Window.partitionBy(\"location_id\").orderBy(\"datetime\")\n",
    "\n",
    "# Tạo lag features cho từng split (train, val, test)\n",
    "def create_lag_features(df, lag_base_columns, lag_steps):\n",
    "    \"\"\"Create lag features from SCALED columns\"\"\"\n",
    "    df_with_lags = df\n",
    "    \n",
    "    for col_name in lag_base_columns:\n",
    "        col_scaled = f\"{col_name}_scaled\"\n",
    "        \n",
    "        if col_scaled in df.columns:\n",
    "            for lag in lag_steps:\n",
    "                lag_col_name = f\"{col_name}_lag{lag}_scaled\"\n",
    "                \n",
    "                # [SUCCESS] Tạo lag TỪ CỘT ĐÃ SCALE\n",
    "                df_with_lags = df_with_lags.withColumn(\n",
    "                    lag_col_name,\n",
    "                    F.lag(col_scaled, lag).over(w_lag)\n",
    "                )\n",
    "    \n",
    "    return df_with_lags\n",
    "\n",
    "# Apply to all splits\n",
    "print(f\"\\n[PROCESSING] Creating lag features for all splits...\")\n",
    "df_train = create_lag_features(df_train, lag_base_columns, LAG_STEPS)\n",
    "df_val = create_lag_features(df_val, lag_base_columns, LAG_STEPS)\n",
    "df_test = create_lag_features(df_test, lag_base_columns, LAG_STEPS)\n",
    "\n",
    "print(f\"  [OK] Train: {len(LAG_STEPS) * len(lag_base_columns)} lag features created\")\n",
    "print(f\"  [OK] Val:   {len(LAG_STEPS) * len(lag_base_columns)} lag features created\")\n",
    "print(f\"  [OK] Test:  {len(LAG_STEPS) * len(lag_base_columns)} lag features created\")\n",
    "\n",
    "# Trigger computation and cache\n",
    "_ = df_train.count()\n",
    "_ = df_val.count()\n",
    "_ = df_test.count()\n",
    "\n",
    "df_train = df_train.cache()\n",
    "df_val = df_val.cache()\n",
    "df_test = df_test.cache()\n",
    "\n",
    "print(f\"\\n[SUCCESS] Lag features created successfully!\")\n",
    "print(f\"   [SUCCESS] All lags created FROM SCALED columns\")\n",
    "print(f\"   [SUCCESS] Lag and base features have SAME scale parameters\")\n",
    "print(f\"   [SUCCESS] Proper temporal relationship preserved\")\n",
    "\n",
    "# ========================================\n",
    "# XỬ LÝ NULL VALUES TRONG LAG FEATURES\n",
    "# ========================================\n",
    "print(f\"\\n[PROCESSING] Handling null values in lag features...\")\n",
    "\n",
    "# Tạo list tất cả lag feature names\n",
    "lag_feature_names = [f\"{col}_lag{lag}_scaled\" for col in lag_base_columns for lag in LAG_STEPS]\n",
    "\n",
    "# Đếm nulls TRƯỚC khi xử lý\n",
    "print(f\"\\n[DATA] Null counts BEFORE handling:\")\n",
    "sample_lag_features = lag_feature_names[:3]\n",
    "for lag_col in sample_lag_features:\n",
    "    if lag_col in df_train.columns:\n",
    "        null_count = df_train.filter(F.col(lag_col).isNull()).count()\n",
    "        total_count = df_train.count()\n",
    "        print(f\"  {lag_col:35s}: {null_count:8,} nulls ({null_count/total_count*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n[WARNING]  Reason: First {max(LAG_STEPS)} hours of each location have no previous data\")\n",
    "print(f\"   Strategy: DROP records with ANY null lag feature\")\n",
    "\n",
    "# Track counts before drop\n",
    "train_before = df_train.count()\n",
    "val_before = df_val.count()\n",
    "test_before = df_test.count()\n",
    "\n",
    "# Function to drop nulls\n",
    "def drop_lag_nulls(df, lag_features):\n",
    "    \"\"\"Drop records with any null lag feature\"\"\"\n",
    "    df_clean = df\n",
    "    for col in lag_features:\n",
    "        if col in df.columns:\n",
    "            df_clean = df_clean.filter(F.col(col).isNotNull())\n",
    "    return df_clean\n",
    "\n",
    "# Apply to all splits\n",
    "print(f\"\\n[?]  Dropping records with null lag features...\")\n",
    "df_train_clean = drop_lag_nulls(df_train, lag_feature_names)\n",
    "df_val_clean = drop_lag_nulls(df_val, lag_feature_names)\n",
    "df_test_clean = drop_lag_nulls(df_test, lag_feature_names)\n",
    "\n",
    "# Count after\n",
    "train_after = df_train_clean.count()\n",
    "val_after = df_val_clean.count()\n",
    "test_after = df_test_clean.count()\n",
    "\n",
    "# Cache cleaned datasets\n",
    "df_train_clean = df_train_clean.cache()\n",
    "df_val_clean = df_val_clean.cache()\n",
    "df_test_clean = df_test_clean.cache()\n",
    "\n",
    "# Unpersist old ones\n",
    "df_train.unpersist()\n",
    "df_val.unpersist()\n",
    "df_test.unpersist()\n",
    "\n",
    "# Reassign\n",
    "df_train = df_train_clean\n",
    "df_val = df_val_clean\n",
    "df_test = df_test_clean\n",
    "\n",
    "print(f\"\\n[DATA] Records dropped (null lag features):\")\n",
    "print(f\"  [?] Train: {train_before:,} -> {train_after:,} (dropped {train_before - train_after:,}, {(train_before - train_after)/train_before*100:.2f}%)\")\n",
    "print(f\"  [?] Val:   {val_before:,} -> {val_after:,} (dropped {val_before - val_after:,}, {(val_before - val_after)/val_before*100:.2f}%)\")\n",
    "print(f\"  [?] Test:  {test_before:,} -> {test_after:,} (dropped {test_before - test_after:,}, {(test_before - test_after)/test_before*100:.2f}%)\")\n",
    "\n",
    "# Verify no nulls\n",
    "print(f\"\\n[SUCCESS] Verification - checking for remaining nulls...\")\n",
    "sample_check = lag_feature_names[:3]\n",
    "total_nulls_after = 0\n",
    "for lag_col in sample_check:\n",
    "    if lag_col in df_train.columns:\n",
    "        null_count = df_train.filter(F.col(lag_col).isNull()).count()\n",
    "        total_nulls_after += null_count\n",
    "        status = \"[SUCCESS]\" if null_count == 0 else \"[ERROR]\"\n",
    "        print(f\"  {lag_col:35s}: {null_count:8,} nulls {status}\")\n",
    "\n",
    "if total_nulls_after == 0:\n",
    "    print(f\"\\n[SUCCESS] All lag features are clean!\")\n",
    "else:\n",
    "    print(f\"\\n[WARNING]  Still {total_nulls_after} nulls found!\")\n",
    "\n",
    "print(f\"\\n[SUCCESS] Lag features + Null handling completed!\")\n",
    "print(f\"   - Created {len(lag_feature_names)} lag features FROM SCALED columns\")\n",
    "print(f\"   - Lost only first {max(LAG_STEPS)} hours per location\")\n",
    "print(f\"   - All lag features now have valid values\")\n",
    "print(f\"   - Data quality ensured for XGBoost training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869801aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:40:58.284429Z",
     "iopub.status.busy": "2025-11-20T10:40:58.283918Z",
     "iopub.status.idle": "2025-11-20T10:40:58.299506Z",
     "shell.execute_reply": "2025-11-20T10:40:58.298361Z"
    },
    "papermill": {
     "duration": 0.043725,
     "end_time": "2025-11-20T10:40:58.300881",
     "exception": false,
     "start_time": "2025-11-20T10:40:58.257156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PROCESSING] Step 6: Preparing Model-Specific Features...\n",
      "[MODEL] DEEP LEARNING Features: 17 features\n",
      "   - Current pollutants (scaled): 3\n",
      "   - Weather (scaled): 5\n",
      "   - Time (cyclic): 6\n",
      "   - Time (binary): 1\n",
      "   - NO LAG FEATURES (models learn from sequences)\n",
      "\n",
      "[DATA] XGBOOST Features: 71 features\n",
      "   - Deep Learning base features: 17\n",
      "   - Lag features (from scaled columns): 54\n",
      "   - Total: 71 features\n",
      "\n",
      "[SUCCESS] Model-specific features prepared:\n",
      "  [MODEL] CNN1D-BLSTM-Attention: 17 features\n",
      "  [MODEL] LSTM: 17 features\n",
      "  [DATA] XGBoost: 71 features\n",
      "  [TARGET] Target: PM2_5_scaled\n",
      "\n",
      "[SUCCESS] All feature columns exist in datasets!\n"
     ]
    }
   ],
   "source": [
    "# Bước 6: Chuẩn bị Features cho từng Model\n",
    "print(\"\\n[PROCESSING] Step 6: Preparing Model-Specific Features...\")\n",
    "\n",
    "# ========================================\n",
    "# FEATURES CHO DEEP LEARNING MODELS (CNN1D-BLSTM, LSTM)\n",
    "# ========================================\n",
    "# Không cần lag features vì models tự học temporal patterns từ sequences\n",
    "\n",
    "dl_input_features = []\n",
    "\n",
    "# 1. Pollutants scaled (trừ PM2_5 - đây là target)\n",
    "dl_input_features.extend([\"PM10_scaled\", \"NO2_scaled\", \"SO2_scaled\"])\n",
    "\n",
    "# 2. Weather features scaled (core features)\n",
    "dl_input_features.extend([\n",
    "    \"temperature_2m_scaled\", \"relative_humidity_2m_scaled\",\n",
    "    \"wind_speed_10m_scaled\", \"surface_pressure_scaled\", \"precipitation_scaled\"  # ✅ Added surface_pressure\n",
    "])\n",
    "\n",
    "# 3. Time features (cyclic encoding - đã ở dạng sin/cos trong [-1, 1])\n",
    "dl_input_features.extend([\n",
    "    \"hour_sin\", \"hour_cos\", \n",
    "    \"month_sin\", \"month_cos\",\n",
    "    \"day_of_week_sin\", \"day_of_week_cos\",\n",
    "    \"wind_direction_sin\", \"wind_direction_cos\"\n",
    "])\n",
    "\n",
    "# 4. Time features (binary)\n",
    "dl_input_features.extend([\"is_weekend\"])\n",
    "\n",
    "print(f\"[MODEL] DEEP LEARNING Features: {len(dl_input_features)} features\")\n",
    "print(f\"   - Current pollutants (scaled): 3\")\n",
    "print(f\"   - Weather (scaled): 5\") \n",
    "print(f\"   - Time (cyclic): 6\")\n",
    "print(f\"   - Time (binary): 1\")\n",
    "print(f\"   - NO LAG FEATURES (models learn from sequences)\")\n",
    "\n",
    "# ========================================  \n",
    "# FEATURES CHO XGBOOST\n",
    "# ========================================\n",
    "# Cần lag features vì không có khả năng xử lý sequences\n",
    "\n",
    "xgb_input_features = dl_input_features.copy()  # Start with DL features\n",
    "\n",
    "# Thêm lag features CHỈ CHO XGBOOST (đã được tạo từ scaled columns)\n",
    "for col_name in lag_base_columns:\n",
    "    for lag in LAG_STEPS:\n",
    "        lag_col_name = f\"{col_name}_lag{lag}_scaled\"\n",
    "        xgb_input_features.append(lag_col_name)\n",
    "\n",
    "print(f\"\\n[DATA] XGBOOST Features: {len(xgb_input_features)} features\")\n",
    "print(f\"   - Deep Learning base features: {len(dl_input_features)}\")\n",
    "print(f\"   - Lag features (from scaled columns): {len(lag_base_columns) * len(LAG_STEPS)}\")\n",
    "print(f\"   - Total: {len(xgb_input_features)} features\")\n",
    "\n",
    "# Target variable (log transformed and scaled for better training)\n",
    "# Using PM2_5_log_scaled instead of PM2_5_scaled to handle skewness\n",
    "target_feature = \"PM2_5_log_scaled\"\n",
    "\n",
    "print(f\"\\n[SUCCESS] Model-specific features prepared:\")\n",
    "print(f\"  [MODEL] CNN1D-BLSTM-Attention: {len(dl_input_features)} features\")\n",
    "print(f\"  [MODEL] LSTM: {len(dl_input_features)} features\")  \n",
    "print(f\"  [DATA] XGBoost: {len(xgb_input_features)} features\")\n",
    "print(f\"  [TARGET] Target: {target_feature} (log transformed)\")\n",
    "\n",
    "# [WARNING] CRITICAL: Verify ALL columns exist\n",
    "missing_dl = [col for col in dl_input_features if col not in df_train.columns]\n",
    "missing_xgb = [col for col in xgb_input_features if col not in df_train.columns]\n",
    "missing_target = target_feature not in df_train.columns\n",
    "\n",
    "if missing_dl or missing_xgb or missing_target:\n",
    "    print(f\"\\n[ERROR] MISSING COLUMNS DETECTED:\")\n",
    "    if missing_dl: \n",
    "        print(f\"  DL models: {missing_dl}\")\n",
    "    if missing_xgb: \n",
    "        print(f\"  XGBoost: {missing_xgb[:5]}...\")  # Show first 5\n",
    "    if missing_target:\n",
    "        print(f\"  Target: {target_feature}\")\n",
    "    \n",
    "    print(f\"\\n[WARNING]  Available scaled columns:\")\n",
    "    scaled_cols = [c for c in df_train.columns if c.endswith('_scaled')]\n",
    "    print(f\"  {scaled_cols[:10]}...\")\n",
    "    \n",
    "    raise ValueError(\"Missing required feature columns! Check normalization step.\")\n",
    "else:\n",
    "    print(f\"\\n[SUCCESS] All feature columns exist in datasets!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad115278",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:40:58.353094Z",
     "iopub.status.busy": "2025-11-20T10:40:58.352817Z",
     "iopub.status.idle": "2025-11-20T10:41:12.751081Z",
     "shell.execute_reply": "2025-11-20T10:41:12.749968Z"
    },
    "papermill": {
     "duration": 14.426037,
     "end_time": "2025-11-20T10:41:12.752486",
     "exception": false,
     "start_time": "2025-11-20T10:40:58.326449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PROCESSING] Step 7: Preparing Final Model-Specific Datasets...\n",
      "\n",
      "[MODEL] Deep Learning datasets (no lag features):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] Train: 187,251 records, 17 features\n",
      "  [OK] Val:   49,620 records, 17 features\n",
      "  [OK] Test:  51,278 records, 17 features\n",
      "\n",
      "[DATA] XGBoost datasets (with lag features):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7811:==========================================>             (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] Train: 187,251 records, 71 features\n",
      "  [OK] Val:   49,620 records, 71 features\n",
      "  [OK] Test:  51,278 records, 71 features\n",
      "\n",
      "[SUCCESS] Final datasets prepared!\n",
      "   [MODEL] Deep Learning: 17 features (no lags)\n",
      "   [DATA] XGBoost: 71 features (with 54 lags)\n",
      "   [TARGET] Target: PM2_5_scaled\n",
      "   [SUCCESS] All datasets cleaned and ready for training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Bước 7: Prepare Final Model Datasets\n",
    "print(\"\\n[PROCESSING] Step 7: Preparing Final Model-Specific Datasets...\")\n",
    "\n",
    "# ========================================\n",
    "# DEEP LEARNING DATASETS (CNN1D-BLSTM & LSTM)\n",
    "# ========================================\n",
    "# Không cần lag features, chỉ cần base features + time features\n",
    "\n",
    "print(f\"\\n[MODEL] Deep Learning datasets (no lag features):\")\n",
    "\n",
    "# Select only DL features + target\n",
    "dl_train = df_train.select(\"location_id\", \"datetime\", target_feature, *dl_input_features)\n",
    "dl_val = df_val.select(\"location_id\", \"datetime\", target_feature, *dl_input_features)\n",
    "dl_test = df_test.select(\"location_id\", \"datetime\", target_feature, *dl_input_features)\n",
    "\n",
    "# Cache\n",
    "dl_train = dl_train.cache()\n",
    "dl_val = dl_val.cache()\n",
    "dl_test = dl_test.cache()\n",
    "\n",
    "dl_train_count = dl_train.count()\n",
    "dl_val_count = dl_val.count()\n",
    "dl_test_count = dl_test.count()\n",
    "\n",
    "print(f\"  [OK] Train: {dl_train_count:,} records, {len(dl_input_features)} features\")\n",
    "print(f\"  [OK] Val:   {dl_val_count:,} records, {len(dl_input_features)} features\")\n",
    "print(f\"  [OK] Test:  {dl_test_count:,} records, {len(dl_input_features)} features\")\n",
    "\n",
    "# ========================================\n",
    "# XGBOOST DATASETS\n",
    "# ========================================\n",
    "# Cần cả base features + lag features\n",
    "\n",
    "print(f\"\\n[DATA] XGBoost datasets (with lag features):\")\n",
    "\n",
    "# Select XGB features + target\n",
    "xgb_train = df_train.select(\"location_id\", \"datetime\", target_feature, *xgb_input_features)\n",
    "xgb_val = df_val.select(\"location_id\", \"datetime\", target_feature, *xgb_input_features)\n",
    "xgb_test = df_test.select(\"location_id\", \"datetime\", target_feature, *xgb_input_features)\n",
    "\n",
    "# Cache\n",
    "xgb_train = xgb_train.cache()\n",
    "xgb_val = xgb_val.cache()\n",
    "xgb_test = xgb_test.cache()\n",
    "\n",
    "xgb_train_count = xgb_train.count()\n",
    "xgb_val_count = xgb_val.count()\n",
    "xgb_test_count = xgb_test.count()\n",
    "\n",
    "print(f\"  [OK] Train: {xgb_train_count:,} records, {len(xgb_input_features)} features\")\n",
    "print(f\"  [OK] Val:   {xgb_val_count:,} records, {len(xgb_input_features)} features\")\n",
    "print(f\"  [OK] Test:  {xgb_test_count:,} records, {len(xgb_input_features)} features\")\n",
    "\n",
    "print(f\"\\n[SUCCESS] Final datasets prepared!\")\n",
    "print(f\"   [MODEL] Deep Learning: {len(dl_input_features)} features (no lags)\")\n",
    "print(f\"   [DATA] XGBoost: {len(xgb_input_features)} features (with {len(lag_base_columns) * len(LAG_STEPS)} lags)\")\n",
    "print(f\"   [TARGET] Target: {target_feature}\")\n",
    "print(f\"   [SUCCESS] All datasets cleaned and ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5b567e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:41:12.806608Z",
     "iopub.status.busy": "2025-11-20T10:41:12.806308Z",
     "iopub.status.idle": "2025-11-20T10:41:13.934000Z",
     "shell.execute_reply": "2025-11-20T10:41:13.933194Z"
    },
    "papermill": {
     "duration": 1.157316,
     "end_time": "2025-11-20T10:41:13.936053",
     "exception": false,
     "start_time": "2025-11-20T10:41:12.778737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[DATA] FEATURE ENGINEERING PIPELINE SUMMARY\n",
      "================================================================================\n",
      "\n",
      "[SUCCESS] PIPELINE EXECUTION ORDER (Correct - No Data Leakage):\n",
      "   [1] Time Features -> Added cyclic (sin/cos) + is_weekend\n",
      "   [2] Temporal Split -> 70% train / 15% val / 15% test\n",
      "   [3] Normalization -> Min-Max [0,1] using TRAIN statistics ONLY\n",
      "   [4] Lag Features + Null Handling -> Created FROM SCALED columns, dropped nulls\n",
      "   [5] Scaler Params -> Saved for inference\n",
      "   [6] Model Features -> Prepared for Deep Learning & XGBoost\n",
      "   [7] Final Datasets -> Ready for training\n",
      "\n",
      "[DATA] DATASET STATISTICS:\n",
      "   Total records: 288,149\n",
      "   Total locations: 14\n",
      "   Time range: 2022-11-01 -> 2025-09-30\n",
      "\n",
      "[METADATA] FEATURE BREAKDOWN:\n",
      "   [MODEL] Deep Learning (CNN1D-BLSTM & LSTM): 17 features\n",
      "      ├─ Pollutants (scaled): 3 (PM10, NO2, SO2)\n",
      "      ├─ Weather (scaled): 5 (temp, humidity, wind, precipitation)\n",
      "      ├─ Time (cyclic): 6 (hour, month, day_of_week -> sin/cos)\n",
      "      └─ Time (binary): 1 (is_weekend)\n",
      "   \n",
      "   [DATA] XGBoost: 71 features\n",
      "      ├─ Deep Learning features: 17\n",
      "      └─ Lag features: 54 (9 vars × 6 lags)\n",
      "\n",
      "[TARGET] TARGET VARIABLE:\n",
      "   PM2_5_scaled (normalized PM2.5 in [0, 1])\n",
      "\n",
      "[SUCCESS] DATA QUALITY CHECKS:\n",
      "   [OK] No missing values in target\n",
      "   [OK] No missing values in features\n",
      "   [OK] No outliers (removed by WHO/EPA standards)\n",
      "   [OK] Proper temporal ordering\n",
      "   [OK] No data leakage (train/val/test temporally separated)\n",
      "   [OK] Correct scale relationship (lag from scaled columns)\n",
      "   [OK] No nulls in lag features (first 24h dropped)\n",
      "\n",
      "[SAVE] SAVED ARTIFACTS:\n",
      "   [FILES] scaler_params.json -> Min-Max parameters (train set only)\n",
      "   [FILES] feature_metadata.json -> Feature lists & configuration\n",
      "\n",
      "[RUN] READY FOR NEXT PHASE:\n",
      "   Variables in memory:\n",
      "   - Deep Learning: dl_train, dl_val, dl_test\n",
      "   - XGBoost: xgb_train, xgb_val, xgb_test\n",
      "   Next step: Sequence creation for Deep Learning models\n",
      "================================================================================\n",
      "\n",
      "[KAGGLE] Kaggle mode: Saving metadata to /kaggle/working/processed\n",
      "\n",
      "[SAVE] Feature metadata saved to: /kaggle/working/processed/feature_metadata.json\n",
      "   [SUCCESS] Pipeline version: 2.0 (refactored - no data leakage)\n",
      "   [SUCCESS] Contains: feature lists, lag config, split info, dataset counts\n"
     ]
    }
   ],
   "source": [
    "# Bước 8: Feature Engineering Summary + Metadata Saving\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[DATA] FEATURE ENGINEERING PIPELINE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n[SUCCESS] PIPELINE EXECUTION ORDER (Correct - No Data Leakage):\")\n",
    "print(f\"   [1] Time Features -> Added cyclic (sin/cos) + is_weekend\")\n",
    "print(f\"   [2] Temporal Split -> 70% train / 15% val / 15% test\")\n",
    "print(f\"   [3] Normalization -> Min-Max [0,1] using TRAIN statistics ONLY\")\n",
    "print(f\"   [4] Lag Features + Null Handling -> Created FROM SCALED columns, dropped nulls\")\n",
    "print(f\"   [5] Scaler Params -> Saved for inference\")\n",
    "print(f\"   [6] Model Features -> Prepared for Deep Learning & XGBoost\")\n",
    "print(f\"   [7] Final Datasets -> Ready for training\")\n",
    "\n",
    "print(f\"\\n[DATA] DATASET STATISTICS:\")\n",
    "print(f\"   Total records: {dl_train_count + dl_val_count + dl_test_count:,}\")\n",
    "print(f\"   Total locations: {df_train.select('location_id').distinct().count()}\")\n",
    "\n",
    "print(f\"\\n[METADATA] FEATURE BREAKDOWN:\")\n",
    "print(f\"   [MODEL] Deep Learning (CNN1D-BLSTM & LSTM): {len(dl_input_features)} features\")\n",
    "print(f\"      ├─ Pollutants (scaled): 3 (PM10, NO2, SO2)\")\n",
    "print(f\"      ├─ Weather (scaled): 5 (temp, humidity, wind, precipitation)\")\n",
    "print(f\"      ├─ Time (cyclic): 6 (hour, month, day_of_week -> sin/cos)\")\n",
    "print(f\"      └─ Time (binary): 1 (is_weekend)\")\n",
    "print(f\"   \")\n",
    "print(f\"   [DATA] XGBoost: {len(xgb_input_features)} features\")\n",
    "print(f\"      ├─ Deep Learning features: {len(dl_input_features)}\")\n",
    "print(f\"      └─ Lag features: {len(lag_base_columns) * len(LAG_STEPS)} ({len(lag_base_columns)} vars × {len(LAG_STEPS)} lags)\")\n",
    "\n",
    "print(f\"\\n[TARGET] TARGET VARIABLE:\")\n",
    "print(f\"   {target_feature} (normalized PM2.5 in [0, 1])\")\n",
    "\n",
    "print(f\"\\n[SUCCESS] DATA QUALITY CHECKS:\")\n",
    "print(f\"   [OK] No missing values in target\")\n",
    "print(f\"   [OK] No missing values in features\")\n",
    "print(f\"   [OK] No outliers (removed by WHO/EPA standards)\")\n",
    "print(f\"   [OK] Proper temporal ordering\")\n",
    "print(f\"   [OK] STRATIFIED split (all seasons in each split)\")\n",
    "print(f\"   [OK] Correct scale relationship (lag from scaled columns)\")\n",
    "print(f\"   [OK] No nulls in lag features (first {max(LAG_STEPS)}h dropped)\")\n",
    "print(f\"   [OK] Log transform applied to target (reduced skewness)\")\n",
    "\n",
    "print(f\"\\n[SAVE] SAVED ARTIFACTS:\")\n",
    "print(f\"   [FILES] scaler_params.json -> Min-Max parameters (train set only)\")\n",
    "print(f\"   [FILES] feature_metadata.json -> Feature lists & configuration\")\n",
    "\n",
    "print(f\"\\n[RUN] READY FOR NEXT PHASE:\")\n",
    "print(f\"   Variables in memory:\")\n",
    "print(f\"   - Deep Learning: dl_train, dl_val, dl_test\")\n",
    "print(f\"   - XGBoost: xgb_train, xgb_val, xgb_test\")\n",
    "print(f\"   Next step: Sequence creation for Deep Learning models\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========================================\n",
    "# SAVE FEATURE METADATA\n",
    "# ========================================\n",
    "# Lưu metadata về feature engineering để tham khảo trong tương lai\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Metadata cho feature engineering\n",
    "dataset_metadata = {\n",
    "    \"project\": \"PM2.5 Prediction\",\n",
    "    \"preprocessing_version\": \"3.0_stratified_split\",\n",
    "    \"pipeline_order\": [\n",
    "        \"Time Features (cyclic encoding)\",\n",
    "        \"STRATIFIED Temporal Split (70/15/15 per month)\",\n",
    "        \"Log Transform (target PM2.5)\",\n",
    "        \"Normalization (train stats only)\",\n",
    "        \"Lag Features (from scaled columns)\",\n",
    "        \"Null Handling (drop first 24h per location)\"\n",
    "    ],\n",
    "    \"split_strategy\": {\n",
    "        \"method\": \"stratified_temporal\",\n",
    "        \"description\": \"Each month split 70/15/15, ensures all seasons in each split\",\n",
    "        \"ratios\": {\"train\": 0.70, \"val\": 0.15, \"test\": 0.15}\n",
    "    },\n",
    "    \"deep_learning_features\": dl_input_features,\n",
    "    \"xgboost_features\": xgb_input_features,\n",
    "    \"target_feature\": target_feature,\n",
    "    \"target_transform\": \"log1p\",\n",
    "    \"lag_config\": {\n",
    "        \"lag_steps\": LAG_STEPS,\n",
    "        \"lag_base_columns\": lag_base_columns,\n",
    "        \"total_lag_features\": len(lag_base_columns) * len(LAG_STEPS)\n",
    "    },\n",
    "    \"dataset_counts\": {\n",
    "        \"dl_train\": dl_train_count,\n",
    "        \"dl_val\": dl_val_count,\n",
    "        \"dl_test\": dl_test_count,\n",
    "        \"xgb_train\": xgb_train_count,\n",
    "        \"xgb_val\": xgb_val_count,\n",
    "        \"xgb_test\": xgb_test_count\n",
    "    },\n",
    "    \"total_features\": {\n",
    "        \"deep_learning\": len(dl_input_features),\n",
    "        \"xgboost\": len(xgb_input_features)\n",
    "    }\n",
    "}\n",
    "\n",
    "# ========================================\n",
    "# ADAPTIVE PATH (Kaggle vs Colab vs Local)\n",
    "# ========================================\n",
    "if IN_KAGGLE:\n",
    "    # [KAGGLE] Kaggle: Write to /kaggle/working (auto-saved on commit)\n",
    "    processed_dir = Path(\"/kaggle/working/processed\")\n",
    "    print(f\"\\n[KAGGLE] Kaggle mode: Saving metadata to {processed_dir}\")\n",
    "    \n",
    "elif IN_COLAB:\n",
    "    # [COLAB] Colab: Write to Google Drive\n",
    "    processed_dir = Path(\"/content/drive/MyDrive/pm25-data/processed\")\n",
    "    print(f\"\\n[COLAB] Colab mode: Saving metadata to Google Drive\")\n",
    "    \n",
    "else:\n",
    "    # [LOCAL] Local: Write to project folder\n",
    "    processed_dir = Path(\"../data/processed\")\n",
    "    print(f\"\\n[LOCAL] Local mode: Saving metadata to {processed_dir}\")\n",
    "\n",
    "# Tạo thư mục với parents=True (tạo cả parent directories nếu chưa có)\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Lưu metadata\n",
    "metadata_path = processed_dir / \"feature_metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(dataset_metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\n[SAVE] Feature metadata saved to: {metadata_path}\")\n",
    "print(f\"   [SUCCESS] Pipeline version: 2.0 (refactored - no data leakage)\")\n",
    "print(f\"   [SUCCESS] Contains: feature lists, lag config, split info, dataset counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8d0768b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:41:14.001063Z",
     "iopub.status.busy": "2025-11-20T10:41:14.000122Z",
     "iopub.status.idle": "2025-11-20T10:50:04.110837Z",
     "shell.execute_reply": "2025-11-20T10:50:04.109792Z"
    },
    "papermill": {
     "duration": 530.139687,
     "end_time": "2025-11-20T10:50:04.112386",
     "exception": false,
     "start_time": "2025-11-20T10:41:13.972699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PROCESSING] Step 9: Creating Sequence Data for Deep Learning Models...\n",
      "[GEAR]  Sequence Configuration:\n",
      "   - CNN1D-BLSTM-Attention: 48 timesteps\n",
      "   - LSTM: 24 timesteps\n",
      "\n",
      "[DATA] Creating sequences for each model...\n",
      "\n",
      "[MODEL] CNN1D-BLSTM-Attention (48 timesteps):\n",
      "    Creating 48-step sequences...\n",
      "      [?]  Layer 1: Dropped first 48 records/location\n",
      "         Records: 187,251\n",
      "        [INSTALL] Processing 5 batches (17 features)...\n",
      "           Batch 1/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 2/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 3/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 4/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 5/5: 1 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [?] Filtering null sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [?]  Layer 2: Dropped 672 records containing NaN/Null inside sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/20 10:43:35 WARN DAGScheduler: Broadcasting large task binary with size 1600.3 KiB\n",
      "25/11/20 10:43:41 WARN DAGScheduler: Broadcasting large task binary with size 1605.7 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [SUCCESS] Final: 186,579 records (99.6% retained)\n",
      "    Creating 48-step sequences...\n",
      "      [?]  Layer 1: Dropped first 48 records/location\n",
      "         Records: 49,620\n",
      "        [INSTALL] Processing 5 batches (17 features)...\n",
      "           Batch 1/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 2/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 3/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 4/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 5/5: 1 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [?] Filtering null sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [?]  Layer 2: Dropped 672 records containing NaN/Null inside sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [SUCCESS] Final: 48,948 records (98.6% retained)\n",
      "    Creating 48-step sequences...\n",
      "      [?]  Layer 1: Dropped first 48 records/location\n",
      "         Records: 51,278\n",
      "        [INSTALL] Processing 5 batches (17 features)...\n",
      "           Batch 1/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 2/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 3/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 4/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 5/5: 1 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [?] Filtering null sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [?]  Layer 2: Dropped 672 records containing NaN/Null inside sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [SUCCESS] Final: 50,606 records (98.7% retained)\n",
      "    [SUCCESS] CNN sequences created successfully\n",
      "\n",
      "[PROCESSING] LSTM (24 timesteps):\n",
      "    Creating 24-step sequences...\n",
      "      [?]  Layer 1: Dropped first 24 records/location\n",
      "         Records: 187,251\n",
      "        [INSTALL] Processing 5 batches (17 features)...\n",
      "           Batch 1/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 2/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 3/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 4/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 5/5: 1 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [?] Filtering null sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [?]  Layer 2: Dropped 336 records containing NaN/Null inside sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/20 10:48:06 WARN DAGScheduler: Broadcasting large task binary with size 1220.9 KiB\n",
      "25/11/20 10:48:12 WARN DAGScheduler: Broadcasting large task binary with size 1226.3 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [SUCCESS] Final: 186,915 records (99.8% retained)\n",
      "    Creating 24-step sequences...\n",
      "      [?]  Layer 1: Dropped first 24 records/location\n",
      "         Records: 49,620\n",
      "        [INSTALL] Processing 5 batches (17 features)...\n",
      "           Batch 1/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 2/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 3/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 4/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 5/5: 1 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [?] Filtering null sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [?]  Layer 2: Dropped 336 records containing NaN/Null inside sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [SUCCESS] Final: 49,284 records (99.3% retained)\n",
      "    Creating 24-step sequences...\n",
      "      [?]  Layer 1: Dropped first 24 records/location\n",
      "         Records: 51,278\n",
      "        [INSTALL] Processing 5 batches (17 features)...\n",
      "           Batch 1/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 2/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 3/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 4/5: 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Batch 5/5: 1 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [?] Filtering null sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [?]  Layer 2: Dropped 336 records containing NaN/Null inside sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12643:================================================>      (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      [SUCCESS] Final: 50,942 records (99.3% retained)\n",
      "    [SUCCESS] LSTM sequences created successfully\n",
      "\n",
      "[SUCCESS] Sequence data preparation completed!\n",
      "\n",
      "[METADATA] Data Quality Guarantee:\n",
      "   [OK] Layer 1: No incomplete history (first 48/24 records dropped)\n",
      "   [OK] Layer 2: No data gaps in middle (nulls filtered out)\n",
      "   [OK] Result: 100% clean sequences with ZERO nulls\n",
      "   [OK] Ready for high-quality model training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Bước 9: Create Sequence Data for Deep Learning Models\n",
    "print(\"\\n[PROCESSING] Step 9: Creating Sequence Data for Deep Learning Models...\")\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "# Sequence configuration (optimized for Colab)\n",
    "CNN_SEQUENCE_LENGTH = 48  # Optimal for long-term patterns\n",
    "LSTM_SEQUENCE_LENGTH = 24  # Optimal for medium-term patterns\n",
    "\n",
    "print(f\"[GEAR]  Sequence Configuration:\")\n",
    "print(f\"   - CNN1D-BLSTM-Attention: {CNN_SEQUENCE_LENGTH} timesteps\")\n",
    "print(f\"   - LSTM: {LSTM_SEQUENCE_LENGTH} timesteps\")\n",
    "\n",
    "def create_sequences_optimized(df, feature_cols, target_col, sequence_length):\n",
    "    \"\"\"\n",
    "    Optimized sequence creation with checkpointing to avoid StackOverflow\n",
    "    \n",
    "    [TARGET] Key Strategy:\n",
    "    - Batch processing to avoid deep logical plans\n",
    "    - Checkpoint after each batch to reset plan depth\n",
    "    - Use broadcast joins for efficiency\n",
    "    - Single final filter for null handling\n",
    "    \n",
    "    [?] Null Handling (2-Layer Protection):\n",
    "    Layer 1: Drop first N records/location (incomplete history)\n",
    "    Layer 2: Filter ANY null in sequences (data gaps)\n",
    "    Result: 100% clean sequences with ZERO nulls\n",
    "    \"\"\"\n",
    "    print(f\"    Creating {sequence_length}-step sequences...\")\n",
    "    \n",
    "    window_spec = Window.partitionBy(\"location_id\").orderBy(\"datetime\")\n",
    "    \n",
    "    # ========================================\n",
    "    # LAYER 1: Drop first N records (incomplete history)\n",
    "    # ========================================\n",
    "    # df_base = df.select(\"location_id\", \"datetime\", target_col, *feature_cols) \\\n",
    "    #             .repartition(4, \"location_id\") \\\n",
    "    #             .withColumn(\"row_num\", F.row_number().over(window_spec)) \\\n",
    "    #             .filter(F.col(\"row_num\") > sequence_length) \\\n",
    "    #             .drop(\"row_num\") \\\n",
    "    #             .cache()\n",
    "    df_base = df\n",
    "    \n",
    "    records_after_layer1 = df_base.count()  # Materialize\n",
    "    print(f\"      [?]  Layer 1: Dropped first {sequence_length} records/location\")\n",
    "    print(f\"         Records: {records_after_layer1:,}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # BATCH PROCESSING (避免 StackOverflow)\n",
    "    # ========================================\n",
    "    # Chia features thành batches nhỏ để tránh logical plan quá sâu\n",
    "    BATCH_SIZE = 4  # Mỗi batch xử lý 4 features (4 × 48 lags = 192 ops - safe)\n",
    "    feature_batches = [feature_cols[i:i+BATCH_SIZE] for i in range(0, len(feature_cols), BATCH_SIZE)]\n",
    "    \n",
    "    print(f\"        [INSTALL] Processing {len(feature_batches)} batches ({len(feature_cols)} features)...\")\n",
    "    \n",
    "    base_cols = [\"location_id\", \"datetime\"]\n",
    "    result_df = df_base.select(*base_cols)\n",
    "    \n",
    "    for batch_idx, batch_features in enumerate(feature_batches, 1):\n",
    "        print(f\"           Batch {batch_idx}/{len(feature_batches)}: {len(batch_features)} features\")\n",
    "        \n",
    "        # Tạo batch DataFrame\n",
    "        batch_df = df_base.select(*base_cols, *batch_features)\n",
    "        \n",
    "        # Tạo sequences cho batch này\n",
    "        for col_name in batch_features:\n",
    "            # Tạo array of lags [t-1, t-2, ..., t-N]\n",
    "            lag_exprs = [F.lag(col_name, step).over(window_spec) for step in range(1, sequence_length + 1)]\n",
    "            batch_df = batch_df.withColumn(f\"{col_name}_sequence\", F.array(*lag_exprs))\n",
    "\n",
    "            lag_exprs = [F.lag(col_name, step).over(window_spec)\n",
    "             for step in range(sequence_length, 0, -1)]  # ✅ Đảo ngược: N -> 1\n",
    "            batch_df = batch_df.withColumn(f\"{col_name}_sequence\", F.array(*lag_exprs))\n",
    "        \n",
    "        # Select chỉ sequence columns\n",
    "        sequence_cols = [f\"{col}_sequence\" for col in batch_features]\n",
    "        batch_df = batch_df.select(*base_cols, *sequence_cols).cache()\n",
    "        batch_df.count()  # Materialize để reset logical plan\n",
    "        \n",
    "        # Join vào result\n",
    "        result_df = result_df.join(batch_df, base_cols, \"inner\")\n",
    "        \n",
    "        # Unpersist batch (giải phóng memory)\n",
    "        batch_df.unpersist()\n",
    "    \n",
    "    # ========================================\n",
    "    # LAYER 2: Filter nulls in sequences\n",
    "    # ========================================\n",
    "    print(f\"        [?] Filtering null sequences...\")\n",
    "    \n",
    "    all_sequence_cols = [f\"{col}_sequence\" for col in feature_cols]\n",
    "\n",
    "    # Hàm kiểm tra chặt chẽ từng cột sequence\n",
    "    def get_valid_sequence_condition(col_name):\n",
    "        col = F.col(col_name)\n",
    "        # Điều kiện 1: Bản thân mảng không được null và phải đủ kích thước\n",
    "        basic_cond = col.isNotNull() & (F.size(col) == sequence_length)\n",
    "        \n",
    "        # Điều kiện 2 (QUAN TRỌNG NHẤT): Duyệt từng phần tử x trong mảng\n",
    "        # x phải NOT NULL và x phải NOT NaN\n",
    "        # Hàm forall có từ Spark 3.1+\n",
    "        element_check = F.forall(col, lambda x: x.isNotNull() & (~F.isnan(x)))\n",
    "        \n",
    "        return basic_cond & element_check\n",
    "    \n",
    "    # Build null filter: ALL sequences must be NOT NULL\n",
    "    from functools import reduce\n",
    "    final_filter = reduce(\n",
    "        lambda acc, col_name: acc & get_valid_sequence_condition(col_name),\n",
    "        all_sequence_cols,\n",
    "        F.lit(True)\n",
    "    )\n",
    "    \n",
    "    # Áp dụng lọc\n",
    "    result_df = result_df.filter(final_filter)\n",
    "    \n",
    "    # Tính toán số lượng sau khi lọc\n",
    "    records_after_layer2 = result_df.count()\n",
    "    dropped = records_after_layer1 - records_after_layer2\n",
    "    \n",
    "    if dropped > 0:\n",
    "        print(f\"      [?]  Layer 2: Dropped {dropped:,} records containing NaN/Null inside sequences\")\n",
    "    else:\n",
    "        print(f\"      [SUCCESS] Layer 2: Data clean, no gaps found\")\n",
    "    # ========================================\n",
    "    # FINAL: Add target and clean up\n",
    "    # ========================================\n",
    "    result_df = result_df.join(\n",
    "        df_base.select(\"location_id\", \"datetime\", target_col),\n",
    "        [\"location_id\", \"datetime\"],\n",
    "        \"inner\"\n",
    "    ).filter(F.col(target_col).isNotNull()) \\\n",
    "     .withColumnRenamed(target_col, \"target_value\") \\\n",
    "     .cache()\n",
    "    \n",
    "    final_count = result_df.count()\n",
    "    retention_rate = (final_count / records_after_layer1) * 100\n",
    "    \n",
    "    print(f\"      [SUCCESS] Final: {final_count:,} records ({retention_rate:.1f}% retained)\")\n",
    "    \n",
    "    # Cleanup\n",
    "    df_base.unpersist()\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "print(\"\\n[DATA] Creating sequences for each model...\")\n",
    "\n",
    "# Create CNN1D-BLSTM sequences\n",
    "print(f\"\\n[MODEL] CNN1D-BLSTM-Attention ({CNN_SEQUENCE_LENGTH} timesteps):\")\n",
    "try:\n",
    "    cnn_train_clean = create_sequences_optimized(dl_train, dl_input_features, target_feature, CNN_SEQUENCE_LENGTH)\n",
    "    cnn_val_clean = create_sequences_optimized(dl_val, dl_input_features, target_feature, CNN_SEQUENCE_LENGTH)\n",
    "    cnn_test_clean = create_sequences_optimized(dl_test, dl_input_features, target_feature, CNN_SEQUENCE_LENGTH)\n",
    "    print(f\"    [SUCCESS] CNN sequences created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"    [ERROR] CNN sequence creation failed: {str(e)[:100]}...\")\n",
    "    cnn_train_clean = cnn_val_clean = cnn_test_clean = None\n",
    "\n",
    "# Create LSTM sequences  \n",
    "print(f\"\\n[PROCESSING] LSTM ({LSTM_SEQUENCE_LENGTH} timesteps):\")\n",
    "try:\n",
    "    lstm_train_clean = create_sequences_optimized(dl_train, dl_input_features, target_feature, LSTM_SEQUENCE_LENGTH)\n",
    "    lstm_val_clean = create_sequences_optimized(dl_val, dl_input_features, target_feature, LSTM_SEQUENCE_LENGTH)\n",
    "    lstm_test_clean = create_sequences_optimized(dl_test, dl_input_features, target_feature, LSTM_SEQUENCE_LENGTH)\n",
    "    print(f\"    [SUCCESS] LSTM sequences created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"    [ERROR] LSTM sequence creation failed: {str(e)[:100]}...\")\n",
    "    lstm_train_clean = lstm_val_clean = lstm_test_clean = None\n",
    "\n",
    "print(f\"\\n[SUCCESS] Sequence data preparation completed!\")\n",
    "print(f\"\\n[METADATA] Data Quality Guarantee:\")\n",
    "print(f\"   [OK] Layer 1: No incomplete history (first {CNN_SEQUENCE_LENGTH}/{LSTM_SEQUENCE_LENGTH} records dropped)\")\n",
    "print(f\"   [OK] Layer 2: No data gaps in middle (nulls filtered out)\")\n",
    "print(f\"   [OK] Result: 100% clean sequences with ZERO nulls\")\n",
    "print(f\"   [OK] Ready for high-quality model training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ac61b64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:50:04.205681Z",
     "iopub.status.busy": "2025-11-20T10:50:04.205294Z",
     "iopub.status.idle": "2025-11-20T10:51:19.917298Z",
     "shell.execute_reply": "2025-11-20T10:51:19.916458Z"
    },
    "papermill": {
     "duration": 75.76105,
     "end_time": "2025-11-20T10:51:19.919263",
     "exception": false,
     "start_time": "2025-11-20T10:50:04.158213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INSTALL] Step 10: Exporting Final Datasets to Disk...\n",
      "[KAGGLE] Kaggle mode: Saving to /kaggle/working/processed\n",
      "   [WARNING]  Files will be auto-saved when you commit notebook\n",
      "\n",
      "[DATA] Dataset Status:\n",
      "  CNN1D-BLSTM: [SUCCESS] Ready\n",
      "  LSTM: [SUCCESS] Ready\n",
      "  XGBoost: [SUCCESS] Ready\n",
      "\n",
      "[SAVE] Exporting datasets to Parquet format...\n",
      "\n",
      "  [MODEL] Exporting CNN1D-BLSTM datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/20 10:50:07 WARN DAGScheduler: Broadcasting large task binary with size 1808.0 KiB\n",
      "25/11/20 10:50:36 WARN DAGScheduler: Broadcasting large task binary with size 1605.7 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     [SUCCESS] Saved to: /kaggle/working/processed/cnn_sequences/\n",
      "        - train: 186,579 records\n",
      "        - val:   48,948 records\n",
      "        - test:  50,606 records\n",
      "\n",
      "  [PROCESSING] Exporting LSTM datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/20 10:50:47 WARN DAGScheduler: Broadcasting large task binary with size 1428.6 KiB\n",
      "25/11/20 10:50:56 WARN DAGScheduler: Broadcasting large task binary with size 1005.8 KiB\n",
      "25/11/20 10:51:00 WARN DAGScheduler: Broadcasting large task binary with size 1005.4 KiB\n",
      "25/11/20 10:51:04 WARN DAGScheduler: Broadcasting large task binary with size 1226.3 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     [SUCCESS] Saved to: /kaggle/working/processed/lstm_sequences/\n",
      "        - train: 186,915 records\n",
      "        - val:   49,284 records\n",
      "        - test:  50,942 records\n",
      "\n",
      "  [DATA] Exporting XGBoost datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     [SUCCESS] Saved to: /kaggle/working/processed/xgboost/\n",
      "        - train: 187,251 records\n",
      "        - val:   49,620 records\n",
      "        - test:  51,278 records\n",
      "\n",
      "[SAVE] Saving metadata...\n",
      "   [SUCCESS] Metadata saved to: /kaggle/working/processed/datasets_ready.json\n",
      "   [SUCCESS] Scaler params saved to: /kaggle/working/processed/scaler_params.json\n",
      "   [SUCCESS] Feature metadata saved to: /kaggle/working/processed/feature_metadata.json\n",
      "\n",
      "================================================================================\n",
      "[SUCCESS] DATA PREPROCESSING & EXPORT COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "[KAGGLE] KAGGLE OUTPUT:\n",
      "   [?] Location: /kaggle/working/processed/\n",
      "   [?] To save permanently:\n",
      "      1. Click 'Save Version' (top right)\n",
      "      2. Choose 'Save & Run All' (recommended)\n",
      "      3. Wait for completion (~20-30 min)\n",
      "      4. Output will appear in 'Output' tab\n",
      "      5. Use as dataset: '+ Add Data' -> Your Output\n",
      "\n",
      "[?] Exported Directory Structure:\n",
      "   /kaggle/working/processed/\n",
      "   ├── cnn_sequences/\n",
      "   │   ├── train/  (186,579 records)\n",
      "   │   ├── val/    (48,948 records)\n",
      "   │   └── test/   (50,606 records)\n",
      "   ├── lstm_sequences/\n",
      "   │   ├── train/  (186,915 records)\n",
      "   │   ├── val/    (49,284 records)\n",
      "   │   └── test/   (50,942 records)\n",
      "   ├── xgboost/\n",
      "   │   ├── train/  (187,251 records)\n",
      "   │   ├── val/    (49,620 records)\n",
      "   │   └── test/   (51,278 records)\n",
      "   ├── scaler_params.json\n",
      "   ├── feature_metadata.json\n",
      "   └── datasets_ready.json\n",
      "\n",
      "[DATA] Total Dataset Sizes:\n",
      "   - CNN1D-BLSTM: 286,133 records (48 timesteps, 17 features)\n",
      "   - LSTM:        287,141 records (24 timesteps, 17 features)\n",
      "   - XGBoost:     288,149 records (71 features)\n",
      "\n",
      "[RUN] Ready for Model Training Phase!\n",
      "   [?] Next: Create new notebook, add this output as dataset\n",
      "   [?] Load: spark.read.parquet('/kaggle/input/<output-name>/processed/...')\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Bước 10: Export Final Datasets to Disk\n",
    "print(\"\\n[INSTALL] Step 10: Exporting Final Datasets to Disk...\")\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ========================================\n",
    "# ADAPTIVE OUTPUT PATH (Kaggle vs Colab vs Local)\n",
    "# ========================================\n",
    "if IN_KAGGLE:\n",
    "    # [KAGGLE] Kaggle: Write to /kaggle/working (auto-saved on commit)\n",
    "    processed_dir = Path(\"/kaggle/working/processed\")\n",
    "    print(f\"[KAGGLE] Kaggle mode: Saving to {processed_dir}\")\n",
    "    print(f\"   [WARNING]  Files will be auto-saved when you commit notebook\")\n",
    "    \n",
    "elif IN_COLAB:\n",
    "    # [COLAB] Colab: Write to Google Drive\n",
    "    processed_dir = Path(\"/content/drive/MyDrive/pm25-data/processed\")\n",
    "    print(f\"[COLAB] Colab mode: Saving to Google Drive\")\n",
    "    \n",
    "else:\n",
    "    # [LOCAL] Local: Write to project folder\n",
    "    processed_dir = Path(\"../data/processed\")\n",
    "    print(f\"[LOCAL] Local mode: Saving to {processed_dir}\")\n",
    "\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check dataset availability\n",
    "datasets_ready = {\n",
    "    \"cnn\": cnn_train_clean is not None and cnn_val_clean is not None and cnn_test_clean is not None,\n",
    "    \"lstm\": lstm_train_clean is not None and lstm_val_clean is not None and lstm_test_clean is not None,\n",
    "    \"xgb\": xgb_train is not None and xgb_val is not None and xgb_test is not None\n",
    "}\n",
    "\n",
    "print(f\"\\n[DATA] Dataset Status:\")\n",
    "for model, ready in datasets_ready.items():\n",
    "    model_name = {\"cnn\": \"CNN1D-BLSTM\", \"lstm\": \"LSTM\", \"xgb\": \"XGBoost\"}[model]\n",
    "    status = \"[SUCCESS] Ready\" if ready else \"[ERROR] Not Ready\"\n",
    "    print(f\"  {model_name}: {status}\")\n",
    "\n",
    "# ========================================\n",
    "# EXPORT DATASETS TO PARQUET\n",
    "# ========================================\n",
    "print(f\"\\n[SAVE] Exporting datasets to Parquet format...\")\n",
    "\n",
    "export_summary = {\n",
    "    \"cnn\": {\"train\": 0, \"val\": 0, \"test\": 0},\n",
    "    \"lstm\": {\"train\": 0, \"val\": 0, \"test\": 0},\n",
    "    \"xgb\": {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "}\n",
    "\n",
    "# Export CNN1D-BLSTM datasets\n",
    "if datasets_ready[\"cnn\"]:\n",
    "    print(f\"\\n  [MODEL] Exporting CNN1D-BLSTM datasets...\")\n",
    "    cnn_dir = processed_dir / \"cnn_sequences\"\n",
    "    cnn_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    cnn_train_clean.write.mode(\"overwrite\").parquet(str(cnn_dir / \"train\"))\n",
    "    cnn_val_clean.write.mode(\"overwrite\").parquet(str(cnn_dir / \"val\"))\n",
    "    cnn_test_clean.write.mode(\"overwrite\").parquet(str(cnn_dir / \"test\"))\n",
    "    \n",
    "    export_summary[\"cnn\"][\"train\"] = cnn_train_clean.count()\n",
    "    export_summary[\"cnn\"][\"val\"] = cnn_val_clean.count()\n",
    "    export_summary[\"cnn\"][\"test\"] = cnn_test_clean.count()\n",
    "    \n",
    "    print(f\"     [SUCCESS] Saved to: {cnn_dir}/\")\n",
    "    print(f\"        - train: {export_summary['cnn']['train']:,} records\")\n",
    "    print(f\"        - val:   {export_summary['cnn']['val']:,} records\")\n",
    "    print(f\"        - test:  {export_summary['cnn']['test']:,} records\")\n",
    "\n",
    "# Export LSTM datasets\n",
    "if datasets_ready[\"lstm\"]:\n",
    "    print(f\"\\n  [PROCESSING] Exporting LSTM datasets...\")\n",
    "    lstm_dir = processed_dir / \"lstm_sequences\"\n",
    "    lstm_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    lstm_train_clean.write.mode(\"overwrite\").parquet(str(lstm_dir / \"train\"))\n",
    "    lstm_val_clean.write.mode(\"overwrite\").parquet(str(lstm_dir / \"val\"))\n",
    "    lstm_test_clean.write.mode(\"overwrite\").parquet(str(lstm_dir / \"test\"))\n",
    "    \n",
    "    export_summary[\"lstm\"][\"train\"] = lstm_train_clean.count()\n",
    "    export_summary[\"lstm\"][\"val\"] = lstm_val_clean.count()\n",
    "    export_summary[\"lstm\"][\"test\"] = lstm_test_clean.count()\n",
    "    \n",
    "    print(f\"     [SUCCESS] Saved to: {lstm_dir}/\")\n",
    "    print(f\"        - train: {export_summary['lstm']['train']:,} records\")\n",
    "    print(f\"        - val:   {export_summary['lstm']['val']:,} records\")\n",
    "    print(f\"        - test:  {export_summary['lstm']['test']:,} records\")\n",
    "\n",
    "# Export XGBoost datasets\n",
    "if datasets_ready[\"xgb\"]:\n",
    "    print(f\"\\n  [DATA] Exporting XGBoost datasets...\")\n",
    "    xgb_dir = processed_dir / \"xgboost\"\n",
    "    xgb_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    xgb_train.write.mode(\"overwrite\").parquet(str(xgb_dir / \"train\"))\n",
    "    xgb_val.write.mode(\"overwrite\").parquet(str(xgb_dir / \"val\"))\n",
    "    xgb_test.write.mode(\"overwrite\").parquet(str(xgb_dir / \"test\"))\n",
    "    \n",
    "    export_summary[\"xgb\"][\"train\"] = xgb_train.count()\n",
    "    export_summary[\"xgb\"][\"val\"] = xgb_val.count()\n",
    "    export_summary[\"xgb\"][\"test\"] = xgb_test.count()\n",
    "    \n",
    "    print(f\"     [SUCCESS] Saved to: {xgb_dir}/\")\n",
    "    print(f\"        - train: {export_summary['xgb']['train']:,} records\")\n",
    "    print(f\"        - val:   {export_summary['xgb']['val']:,} records\")\n",
    "    print(f\"        - test:  {export_summary['xgb']['test']:,} records\")\n",
    "\n",
    "# ========================================\n",
    "# SAVE METADATA\n",
    "# ========================================\n",
    "print(f\"\\n[SAVE] Saving metadata...\")\n",
    "\n",
    "# Create comprehensive metadata\n",
    "final_metadata = {\n",
    "    \"project\": \"PM2.5 Prediction\",\n",
    "    \"preprocessing_completed\": True,\n",
    "    \"export_timestamp\": str(pd.Timestamp.now()),\n",
    "    \"environment\": \"kaggle\" if IN_KAGGLE else (\"colab\" if IN_COLAB else \"local\"),\n",
    "    \"models\": {\n",
    "        \"cnn1d_blstm\": {\n",
    "            \"sequence_length\": CNN_SEQUENCE_LENGTH,\n",
    "            \"features\": len(dl_input_features),\n",
    "            \"ready\": datasets_ready[\"cnn\"],\n",
    "            \"export_path\": str(processed_dir / \"cnn_sequences\"),\n",
    "            \"record_counts\": export_summary[\"cnn\"]\n",
    "        },\n",
    "        \"lstm\": {\n",
    "            \"sequence_length\": LSTM_SEQUENCE_LENGTH, \n",
    "            \"features\": len(dl_input_features),\n",
    "            \"ready\": datasets_ready[\"lstm\"],\n",
    "            \"export_path\": str(processed_dir / \"lstm_sequences\"),\n",
    "            \"record_counts\": export_summary[\"lstm\"]\n",
    "        },\n",
    "        \"xgboost\": {\n",
    "            \"features\": len(xgb_input_features),\n",
    "            \"lag_steps\": LAG_STEPS,\n",
    "            \"ready\": datasets_ready[\"xgb\"],\n",
    "            \"export_path\": str(processed_dir / \"xgboost\"),\n",
    "            \"record_counts\": export_summary[\"xgb\"]\n",
    "        }\n",
    "    },\n",
    "    \"feature_details\": {\n",
    "        \"deep_learning_features\": dl_input_features,\n",
    "        \"xgboost_features\": xgb_input_features,\n",
    "        \"target\": target_feature\n",
    "    },\n",
    "    \"data_format\": \"parquet\",\n",
    "    \"null_handling\": {\n",
    "        \"strategy\": \"2-layer protection\",\n",
    "        \"layer1\": f\"Dropped first {CNN_SEQUENCE_LENGTH}/{LSTM_SEQUENCE_LENGTH} records per location\",\n",
    "        \"layer2\": \"Filtered records with nulls in sequence history\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = processed_dir / \"datasets_ready.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(final_metadata, f, indent=2)\n",
    "\n",
    "print(f\"   [SUCCESS] Metadata saved to: {metadata_path}\")\n",
    "\n",
    "# Save scaler params\n",
    "scaler_path = processed_dir / \"scaler_params.json\"\n",
    "scaler_json = {\n",
    "    col: {\"min\": float(params[\"min\"]), \"max\": float(params[\"max\"])} \n",
    "    for col, params in scaler_params.items()\n",
    "}\n",
    "with open(scaler_path, 'w') as f:\n",
    "    json.dump(scaler_json, f, indent=2)\n",
    "print(f\"   [SUCCESS] Scaler params saved to: {scaler_path}\")\n",
    "\n",
    "# Save feature metadata\n",
    "feature_metadata_path = processed_dir / \"feature_metadata.json\"\n",
    "feature_metadata = {\n",
    "    \"deep_learning_features\": dl_input_features,\n",
    "    \"xgboost_features\": xgb_input_features,\n",
    "    \"target\": target_feature,\n",
    "    \"lag_steps\": LAG_STEPS,\n",
    "    \"lag_base_columns\": lag_base_columns\n",
    "}\n",
    "with open(feature_metadata_path, 'w') as f:\n",
    "    json.dump(feature_metadata, f, indent=2)\n",
    "print(f\"   [SUCCESS] Feature metadata saved to: {feature_metadata_path}\")\n",
    "\n",
    "# ========================================\n",
    "# FINAL SUMMARY\n",
    "# ========================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"[SUCCESS] DATA PREPROCESSING & EXPORT COMPLETE!\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    print(f\"\\n[KAGGLE] KAGGLE OUTPUT:\")\n",
    "    print(f\"   [?] Location: /kaggle/working/processed/\")\n",
    "    print(f\"   [?] To save permanently:\")\n",
    "    print(f\"      1. Click 'Save Version' (top right)\")\n",
    "    print(f\"      2. Choose 'Save & Run All' (recommended)\")\n",
    "    print(f\"      3. Wait for completion (~20-30 min)\")\n",
    "    print(f\"      4. Output will appear in 'Output' tab\")\n",
    "    print(f\"      5. Use as dataset: '+ Add Data' -> Your Output\")\n",
    "    \n",
    "elif IN_COLAB:\n",
    "    print(f\"\\n[COLAB] COLAB OUTPUT:\")\n",
    "    print(f\"   [?] Saved to Google Drive: {processed_dir}\")\n",
    "    print(f\"   [SUCCESS] Files persist across sessions\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n[LOCAL] LOCAL OUTPUT:\")\n",
    "    print(f\"   [?] Location: {processed_dir.absolute()}\")\n",
    "\n",
    "print(f\"\\n[?] Exported Directory Structure:\")\n",
    "print(f\"   {processed_dir}/\")\n",
    "print(f\"   ├── cnn_sequences/\")\n",
    "print(f\"   │   ├── train/  ({export_summary['cnn']['train']:,} records)\")\n",
    "print(f\"   │   ├── val/    ({export_summary['cnn']['val']:,} records)\")\n",
    "print(f\"   │   └── test/   ({export_summary['cnn']['test']:,} records)\")\n",
    "print(f\"   ├── lstm_sequences/\")\n",
    "print(f\"   │   ├── train/  ({export_summary['lstm']['train']:,} records)\")\n",
    "print(f\"   │   ├── val/    ({export_summary['lstm']['val']:,} records)\")\n",
    "print(f\"   │   └── test/   ({export_summary['lstm']['test']:,} records)\")\n",
    "print(f\"   ├── xgboost/\")\n",
    "print(f\"   │   ├── train/  ({export_summary['xgb']['train']:,} records)\")\n",
    "print(f\"   │   ├── val/    ({export_summary['xgb']['val']:,} records)\")\n",
    "print(f\"   │   └── test/   ({export_summary['xgb']['test']:,} records)\")\n",
    "print(f\"   ├── scaler_params.json\")\n",
    "print(f\"   ├── feature_metadata.json\")\n",
    "print(f\"   └── datasets_ready.json\")\n",
    "\n",
    "print(f\"\\n[DATA] Total Dataset Sizes:\")\n",
    "total_cnn = sum(export_summary['cnn'].values())\n",
    "total_lstm = sum(export_summary['lstm'].values())\n",
    "total_xgb = sum(export_summary['xgb'].values())\n",
    "print(f\"   - CNN1D-BLSTM: {total_cnn:,} records ({CNN_SEQUENCE_LENGTH} timesteps, {len(dl_input_features)} features)\")\n",
    "print(f\"   - LSTM:        {total_lstm:,} records ({LSTM_SEQUENCE_LENGTH} timesteps, {len(dl_input_features)} features)\")\n",
    "print(f\"   - XGBoost:     {total_xgb:,} records ({len(xgb_input_features)} features)\")\n",
    "\n",
    "print(f\"\\n[RUN] Ready for Model Training Phase!\")\n",
    "if IN_KAGGLE:\n",
    "    print(f\"   [?] Next: Create new notebook, add this output as dataset\")\n",
    "    print(f\"   [?] Load: spark.read.parquet('/kaggle/input/<output-name>/processed/...')\")\n",
    "elif IN_COLAB:\n",
    "    print(f\"   [?] Load from Drive in next session\")\n",
    "else:\n",
    "    print(f\"   [?] Load: spark.read.parquet('{processed_dir}/...')\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a33a07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:51:20.177063Z",
     "iopub.status.busy": "2025-11-20T10:51:20.176700Z",
     "iopub.status.idle": "2025-11-20T10:51:30.608147Z",
     "shell.execute_reply": "2025-11-20T10:51:30.607069Z"
    },
    "papermill": {
     "duration": 10.483321,
     "end_time": "2025-11-20T10:51:30.609726",
     "exception": false,
     "start_time": "2025-11-20T10:51:20.126405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[?] Loading Preprocessed Data with Pandas...\n",
      "================================================================================\n",
      "[KAGGLE] Kaggle mode: Loading from /kaggle/input/\n",
      "   Replace <your-dataset-name> with actual dataset name\n",
      "\n",
      "[INSTALL] Loading datasets...\n",
      "\n",
      "[MODEL] CNN1D-BLSTM-Attention:\n",
      "   [SUCCESS] Train: (186579, 20) | Val: (48948, 20) | Test: (50606, 20)\n",
      "\n",
      "[PROCESSING] LSTM:\n",
      "   [SUCCESS] Train: (186915, 20) | Val: (49284, 20) | Test: (50942, 20)\n",
      "\n",
      "[DATA] XGBoost:\n",
      "   [SUCCESS] Train: (187251, 74) | Val: (49620, 74) | Test: (51278, 74)\n",
      "\n",
      "[SUCCESS] All datasets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# [?] EXAMPLE: Load Preprocessed Data with Pandas\n",
    "print(\"\\n[?] Loading Preprocessed Data with Pandas...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========================================\n",
    "# ADAPTIVE PATH\n",
    "# ========================================\n",
    "if IN_KAGGLE:\n",
    "    data_dir = Path(\"/kaggle/working/processed\")\n",
    "    print(f\"[KAGGLE] Kaggle mode: Loading from /kaggle/input/\")\n",
    "    print(f\"   Replace <your-dataset-name> with actual dataset name\")\n",
    "elif IN_COLAB:\n",
    "    data_dir = Path(\"/content/drive/MyDrive/pm25-data/processed\")\n",
    "    print(f\"[COLAB] Colab mode: Loading from Google Drive\")\n",
    "else:\n",
    "    data_dir = Path(\"../data/processed\")\n",
    "    print(f\"[LOCAL] Local mode: Loading from {data_dir}\")\n",
    "\n",
    "# ========================================\n",
    "# LOAD PARQUET FILES\n",
    "# ========================================\n",
    "print(\"\\n[INSTALL] Loading datasets...\")\n",
    "\n",
    "try:\n",
    "    # CNN sequences (48 timesteps)\n",
    "    print(\"\\n[MODEL] CNN1D-BLSTM-Attention:\")\n",
    "    cnn_train = pd.read_parquet(data_dir / 'cnn_sequences' / 'train')\n",
    "    cnn_val = pd.read_parquet(data_dir / 'cnn_sequences' / 'val')\n",
    "    cnn_test = pd.read_parquet(data_dir / 'cnn_sequences' / 'test')\n",
    "    print(f\"   [SUCCESS] Train: {cnn_train.shape} | Val: {cnn_val.shape} | Test: {cnn_test.shape}\")\n",
    "    \n",
    "    # LSTM sequences (24 timesteps)\n",
    "    print(\"\\n[PROCESSING] LSTM:\")\n",
    "    lstm_train = pd.read_parquet(data_dir / 'lstm_sequences' / 'train')\n",
    "    lstm_val = pd.read_parquet(data_dir / 'lstm_sequences' / 'val')\n",
    "    lstm_test = pd.read_parquet(data_dir / 'lstm_sequences' / 'test')\n",
    "    print(f\"   [SUCCESS] Train: {lstm_train.shape} | Val: {lstm_val.shape} | Test: {lstm_test.shape}\")\n",
    "    \n",
    "    # XGBoost data (flat features)\n",
    "    print(\"\\n[DATA] XGBoost:\")\n",
    "    xgb_train = pd.read_parquet(data_dir / 'xgboost' / 'train')\n",
    "    xgb_val = pd.read_parquet(data_dir / 'xgboost' / 'val')\n",
    "    xgb_test = pd.read_parquet(data_dir / 'xgboost' / 'test')\n",
    "    print(f\"   [SUCCESS] Train: {xgb_train.shape} | Val: {xgb_val.shape} | Test: {xgb_test.shape}\")\n",
    "    \n",
    "    print(f\"\\n[SUCCESS] All datasets loaded successfully!\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n[ERROR] Error: Dataset not found!\")\n",
    "    print(f\"   {e}\")\n",
    "    print(f\"\\n[INFO] Make sure to:\")\n",
    "    if IN_KAGGLE:\n",
    "        print(f\"   1. Add this notebook's output as dataset\")\n",
    "        print(f\"   2. Update <your-dataset-name> in path\")\n",
    "    else:\n",
    "        print(f\"   1. Run previous cells to generate data\")\n",
    "        print(f\"   2. Check path: {data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca511998",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:51:30.710237Z",
     "iopub.status.busy": "2025-11-20T10:51:30.709919Z",
     "iopub.status.idle": "2025-11-20T10:51:30.820433Z",
     "shell.execute_reply": "2025-11-20T10:51:30.819522Z"
    },
    "papermill": {
     "duration": 0.162933,
     "end_time": "2025-11-20T10:51:30.822005",
     "exception": false,
     "start_time": "2025-11-20T10:51:30.659072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>PM10_scaled_sequence</th>\n",
       "      <th>NO2_scaled_sequence</th>\n",
       "      <th>SO2_scaled_sequence</th>\n",
       "      <th>temperature_2m_scaled_sequence</th>\n",
       "      <th>relative_humidity_2m_scaled_sequence</th>\n",
       "      <th>wind_speed_10m_scaled_sequence</th>\n",
       "      <th>surface_pressure_scaled_sequence</th>\n",
       "      <th>precipitation_scaled_sequence</th>\n",
       "      <th>hour_sin_sequence</th>\n",
       "      <th>hour_cos_sequence</th>\n",
       "      <th>month_sin_sequence</th>\n",
       "      <th>month_cos_sequence</th>\n",
       "      <th>day_of_week_sin_sequence</th>\n",
       "      <th>day_of_week_cos_sequence</th>\n",
       "      <th>wind_direction_sin_sequence</th>\n",
       "      <th>wind_direction_cos_sequence</th>\n",
       "      <th>is_weekend_sequence</th>\n",
       "      <th>target_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-05 16:00:00</td>\n",
       "      <td>[0.01935767707875055, 0.059392872855257364, 0....</td>\n",
       "      <td>[0.1828434723171565, 0.11073137388926861, 0.08...</td>\n",
       "      <td>[0.018205461638491544, 0.014304291287386216, 0...</td>\n",
       "      <td>[0.5718475073313782, 0.5865102639296187, 0.601...</td>\n",
       "      <td>[0.9358974358974359, 0.8846153846153846, 0.846...</td>\n",
       "      <td>[0.14094775212636695, 0.20534629404617252, 0.2...</td>\n",
       "      <td>[0.5490196078431373, 0.5784313725490197, 0.570...</td>\n",
       "      <td>[0.009398496240601503, 0.013157894736842105, 0...</td>\n",
       "      <td>[0.9659258262890683, -0.2588190451025208, -0.4...</td>\n",
       "      <td>[-0.25881904510252063, -0.9659258262890683, -0...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-0.9749279121818236, -0.9749279121818236, -0....</td>\n",
       "      <td>[-0.2225209339563146, -0.2225209339563146, -0....</td>\n",
       "      <td>[0.9961946980917455, 0.9993908270190958, 0.994...</td>\n",
       "      <td>[-0.08715574274765824, -0.03489949670250073, -...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.118110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-05 21:00:00</td>\n",
       "      <td>[0.06467223933128025, 0.08007039155301363, 0.0...</td>\n",
       "      <td>[0.045112781954887216, 0.04032809295967191, 0....</td>\n",
       "      <td>[0.024707412223667097, 0.029908972691807537, 0...</td>\n",
       "      <td>[0.5777126099706745, 0.5835777126099707, 0.583...</td>\n",
       "      <td>[0.9487179487179487, 0.9487179487179487, 0.948...</td>\n",
       "      <td>[0.11421628189550426, 0.12636695018226005, 0.1...</td>\n",
       "      <td>[0.6137254901960776, 0.623529411764705, 0.6333...</td>\n",
       "      <td>[0.011278195488721804, 0.007518796992481203, 0...</td>\n",
       "      <td>[-0.8660254037844386, -0.7071067811865477, -0....</td>\n",
       "      <td>[0.5000000000000001, 0.7071067811865474, 0.866...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-0.9749279121818236, -0.9749279121818236, -0....</td>\n",
       "      <td>[-0.2225209339563146, -0.2225209339563146, -0....</td>\n",
       "      <td>[1.0, 0.9993908270190958, 0.9925461516413221, ...</td>\n",
       "      <td>[6.123233995736766e-17, 0.03489949670250108, -...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.074147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-06 20:00:00</td>\n",
       "      <td>[0.09546854377474702, 0.10822701275846898, 0.1...</td>\n",
       "      <td>[0.09501025290498974, 0.07279562542720437, 0.0...</td>\n",
       "      <td>[0.02860858257477243, 0.029908972691807537, 0....</td>\n",
       "      <td>[0.6187683284457478, 0.6187683284457478, 0.609...</td>\n",
       "      <td>[0.8333333333333334, 0.8333333333333334, 0.846...</td>\n",
       "      <td>[0.15188335358444716, 0.14823815309842042, 0.1...</td>\n",
       "      <td>[0.6313725490196087, 0.6352941176470583, 0.643...</td>\n",
       "      <td>[0.0037593984962406013, 0.0018796992481203006,...</td>\n",
       "      <td>[-0.7071067811865471, -0.8660254037844384, -0....</td>\n",
       "      <td>[-0.7071067811865479, -0.5000000000000004, -0....</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-0.7818314824680299, -0.7818314824680299, -0....</td>\n",
       "      <td>[0.6234898018587334, 0.6234898018587334, 0.623...</td>\n",
       "      <td>[0.754709580222772, 0.8290375725550417, 0.7880...</td>\n",
       "      <td>[0.6560590289905073, 0.5591929034707468, 0.615...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0.096457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-07 07:00:00</td>\n",
       "      <td>[0.05147382314122305, 0.04575450945886494, 0.0...</td>\n",
       "      <td>[0.04955570745044429, 0.07450444292549556, 0.0...</td>\n",
       "      <td>[0.011703511053315994, 0.014304291287386216, 0...</td>\n",
       "      <td>[0.5278592375366569, 0.5190615835777127, 0.513...</td>\n",
       "      <td>[0.8717948717948718, 0.8589743589743589, 0.846...</td>\n",
       "      <td>[0.15795868772782504, 0.15795868772782504, 0.1...</td>\n",
       "      <td>[0.7058823529411765, 0.7098039215686284, 0.705...</td>\n",
       "      <td>[0.0, 0.0, 0.0018796992481203006, 0.0, 0.0, 0....</td>\n",
       "      <td>[-0.5000000000000004, -0.25881904510252157, 0....</td>\n",
       "      <td>[0.8660254037844384, 0.9659258262890681, 1.0, ...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-0.7818314824680299, -0.7818314824680299, -2....</td>\n",
       "      <td>[0.6234898018587334, 0.6234898018587334, 1.0, ...</td>\n",
       "      <td>[0.6946583704589973, 0.6946583704589973, 0.694...</td>\n",
       "      <td>[0.7193398003386512, 0.7193398003386512, 0.719...</td>\n",
       "      <td>[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0.079396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-07 14:00:00</td>\n",
       "      <td>[0.08139023317201935, 0.0875494940607127, 0.09...</td>\n",
       "      <td>[0.08578263841421736, 0.11551606288448392, 0.1...</td>\n",
       "      <td>[0.016905071521456438, 0.018205461638491544, 0...</td>\n",
       "      <td>[0.48387096774193544, 0.4780058651026393, 0.47...</td>\n",
       "      <td>[0.8461538461538461, 0.8461538461538461, 0.846...</td>\n",
       "      <td>[0.17982989064398544, 0.17739975698663427, 0.1...</td>\n",
       "      <td>[0.6882352941176475, 0.7000000000000008, 0.711...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0018796992481203006, 0....</td>\n",
       "      <td>[0.9659258262890683, 1.0, 0.9659258262890683, ...</td>\n",
       "      <td>[0.25881904510252074, 6.123233995736766e-17, -...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-2.4492935982947064e-16, -2.4492935982947064e...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>[0.7771459614569708, 0.788010753606722, 0.7986...</td>\n",
       "      <td>[0.6293203910498375, 0.6156614753256583, 0.601...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0.099081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-07 15:00:00</td>\n",
       "      <td>[0.0875494940607127, 0.09766827980642322, 0.10...</td>\n",
       "      <td>[0.11551606288448392, 0.1500341763499658, 0.16...</td>\n",
       "      <td>[0.018205461638491544, 0.024707412223667097, 0...</td>\n",
       "      <td>[0.4780058651026393, 0.4750733137829913, 0.498...</td>\n",
       "      <td>[0.8461538461538461, 0.8461538461538461, 0.833...</td>\n",
       "      <td>[0.17739975698663427, 0.17496962332928312, 0.1...</td>\n",
       "      <td>[0.7000000000000008, 0.7117647058823521, 0.727...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0018796992481203006, 0.0, 0....</td>\n",
       "      <td>[1.0, 0.9659258262890683, 0.8660254037844387, ...</td>\n",
       "      <td>[6.123233995736766e-17, -0.25881904510252063, ...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-2.4492935982947064e-16, -2.4492935982947064e...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>[0.788010753606722, 0.7986355100472928, 0.4999...</td>\n",
       "      <td>[0.6156614753256583, 0.6018150231520484, 0.866...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0.090551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-07 16:00:00</td>\n",
       "      <td>[0.09766827980642322, 0.10426748790145182, 0.0...</td>\n",
       "      <td>[0.1500341763499658, 0.1650717703349282, 0.159...</td>\n",
       "      <td>[0.024707412223667097, 0.02730819245773732, 0....</td>\n",
       "      <td>[0.4750733137829913, 0.49853372434017595, 0.51...</td>\n",
       "      <td>[0.8461538461538461, 0.8333333333333334, 0.794...</td>\n",
       "      <td>[0.17496962332928312, 0.13122721749696234, 0.1...</td>\n",
       "      <td>[0.7117647058823521, 0.7274509803921573, 0.741...</td>\n",
       "      <td>[0.0, 0.0, 0.0018796992481203006, 0.0, 0.0, 0....</td>\n",
       "      <td>[0.9659258262890683, 0.8660254037844387, 0.707...</td>\n",
       "      <td>[-0.25881904510252063, -0.4999999999999998, -0...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-2.4492935982947064e-16, -2.4492935982947064e...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>[0.7986355100472928, 0.49999999999999994, 0.51...</td>\n",
       "      <td>[0.6018150231520484, 0.8660254037844387, 0.857...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0.093176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-07 17:00:00</td>\n",
       "      <td>[0.10426748790145182, 0.07919049714034315, 0.0...</td>\n",
       "      <td>[0.1650717703349282, 0.15960355434039644, 0.15...</td>\n",
       "      <td>[0.02730819245773732, 0.022106631989596878, 0....</td>\n",
       "      <td>[0.49853372434017595, 0.5102639296187683, 0.53...</td>\n",
       "      <td>[0.8333333333333334, 0.7948717948717948, 0.743...</td>\n",
       "      <td>[0.13122721749696234, 0.13730255164034022, 0.1...</td>\n",
       "      <td>[0.7274509803921573, 0.7411764705882344, 0.741...</td>\n",
       "      <td>[0.0, 0.0018796992481203006, 0.0, 0.0, 0.0, 0....</td>\n",
       "      <td>[0.8660254037844387, 0.7071067811865476, 0.499...</td>\n",
       "      <td>[-0.4999999999999998, -0.7071067811865475, -0....</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-2.4492935982947064e-16, -2.4492935982947064e...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>[0.49999999999999994, 0.5150380749100542, 0.57...</td>\n",
       "      <td>[0.8660254037844387, 0.8571673007021123, 0.819...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0.107612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-08 00:00:00</td>\n",
       "      <td>[0.09766827980642322, 0.10162780466344039, 0.1...</td>\n",
       "      <td>[0.12371838687628162, 0.101161995898838, 0.078...</td>\n",
       "      <td>[0.03120936280884265, 0.024707412223667097, 0....</td>\n",
       "      <td>[0.5953079178885631, 0.5777126099706745, 0.560...</td>\n",
       "      <td>[0.7051282051282052, 0.717948717948718, 0.7564...</td>\n",
       "      <td>[0.1822600243013366, 0.13001215066828675, 0.11...</td>\n",
       "      <td>[0.6764705882352942, 0.6823529411764697, 0.686...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.8660254037844384, -0.9659258262890683, -1....</td>\n",
       "      <td>[-0.5000000000000004, -0.25881904510252063, -1...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-2.4492935982947064e-16, -2.4492935982947064e...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.623...</td>\n",
       "      <td>[0.6691306063588582, 0.5446390350150271, 0.469...</td>\n",
       "      <td>[0.7431448254773942, 0.838670567945424, 0.8829...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0.069554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-08 02:00:00</td>\n",
       "      <td>[0.10558732952045755, 0.10118785745710514, 0.0...</td>\n",
       "      <td>[0.07860560492139439, 0.10389610389610389, 0.0...</td>\n",
       "      <td>[0.022106631989596878, 0.022106631989596878, 0...</td>\n",
       "      <td>[0.560117302052786, 0.5425219941348973, 0.5249...</td>\n",
       "      <td>[0.7564102564102564, 0.7564102564102564, 0.820...</td>\n",
       "      <td>[0.11907654921020658, 0.13973268529769137, 0.1...</td>\n",
       "      <td>[0.6862745098039216, 0.696078431372549, 0.7196...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00375939...</td>\n",
       "      <td>[-1.0, -0.9659258262890684, -0.866025403784438...</td>\n",
       "      <td>[-1.8369701987210297e-16, 0.2588190451025203, ...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-2.4492935982947064e-16, -2.4492935982947064e...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6234898018587...</td>\n",
       "      <td>[0.4694715627858908, 0.5299192642332049, 0.358...</td>\n",
       "      <td>[0.882947592858927, 0.848048096156426, 0.93358...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0.088583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  location_id            datetime  \\\n",
       "0      233335 2022-11-05 16:00:00   \n",
       "1      233335 2022-11-05 21:00:00   \n",
       "2      233335 2022-11-06 20:00:00   \n",
       "3      233335 2022-11-07 07:00:00   \n",
       "4      233335 2022-11-07 14:00:00   \n",
       "5      233335 2022-11-07 15:00:00   \n",
       "6      233335 2022-11-07 16:00:00   \n",
       "7      233335 2022-11-07 17:00:00   \n",
       "8      233335 2022-11-08 00:00:00   \n",
       "9      233335 2022-11-08 02:00:00   \n",
       "\n",
       "                                PM10_scaled_sequence  \\\n",
       "0  [0.01935767707875055, 0.059392872855257364, 0....   \n",
       "1  [0.06467223933128025, 0.08007039155301363, 0.0...   \n",
       "2  [0.09546854377474702, 0.10822701275846898, 0.1...   \n",
       "3  [0.05147382314122305, 0.04575450945886494, 0.0...   \n",
       "4  [0.08139023317201935, 0.0875494940607127, 0.09...   \n",
       "5  [0.0875494940607127, 0.09766827980642322, 0.10...   \n",
       "6  [0.09766827980642322, 0.10426748790145182, 0.0...   \n",
       "7  [0.10426748790145182, 0.07919049714034315, 0.0...   \n",
       "8  [0.09766827980642322, 0.10162780466344039, 0.1...   \n",
       "9  [0.10558732952045755, 0.10118785745710514, 0.0...   \n",
       "\n",
       "                                 NO2_scaled_sequence  \\\n",
       "0  [0.1828434723171565, 0.11073137388926861, 0.08...   \n",
       "1  [0.045112781954887216, 0.04032809295967191, 0....   \n",
       "2  [0.09501025290498974, 0.07279562542720437, 0.0...   \n",
       "3  [0.04955570745044429, 0.07450444292549556, 0.0...   \n",
       "4  [0.08578263841421736, 0.11551606288448392, 0.1...   \n",
       "5  [0.11551606288448392, 0.1500341763499658, 0.16...   \n",
       "6  [0.1500341763499658, 0.1650717703349282, 0.159...   \n",
       "7  [0.1650717703349282, 0.15960355434039644, 0.15...   \n",
       "8  [0.12371838687628162, 0.101161995898838, 0.078...   \n",
       "9  [0.07860560492139439, 0.10389610389610389, 0.0...   \n",
       "\n",
       "                                 SO2_scaled_sequence  \\\n",
       "0  [0.018205461638491544, 0.014304291287386216, 0...   \n",
       "1  [0.024707412223667097, 0.029908972691807537, 0...   \n",
       "2  [0.02860858257477243, 0.029908972691807537, 0....   \n",
       "3  [0.011703511053315994, 0.014304291287386216, 0...   \n",
       "4  [0.016905071521456438, 0.018205461638491544, 0...   \n",
       "5  [0.018205461638491544, 0.024707412223667097, 0...   \n",
       "6  [0.024707412223667097, 0.02730819245773732, 0....   \n",
       "7  [0.02730819245773732, 0.022106631989596878, 0....   \n",
       "8  [0.03120936280884265, 0.024707412223667097, 0....   \n",
       "9  [0.022106631989596878, 0.022106631989596878, 0...   \n",
       "\n",
       "                      temperature_2m_scaled_sequence  \\\n",
       "0  [0.5718475073313782, 0.5865102639296187, 0.601...   \n",
       "1  [0.5777126099706745, 0.5835777126099707, 0.583...   \n",
       "2  [0.6187683284457478, 0.6187683284457478, 0.609...   \n",
       "3  [0.5278592375366569, 0.5190615835777127, 0.513...   \n",
       "4  [0.48387096774193544, 0.4780058651026393, 0.47...   \n",
       "5  [0.4780058651026393, 0.4750733137829913, 0.498...   \n",
       "6  [0.4750733137829913, 0.49853372434017595, 0.51...   \n",
       "7  [0.49853372434017595, 0.5102639296187683, 0.53...   \n",
       "8  [0.5953079178885631, 0.5777126099706745, 0.560...   \n",
       "9  [0.560117302052786, 0.5425219941348973, 0.5249...   \n",
       "\n",
       "                relative_humidity_2m_scaled_sequence  \\\n",
       "0  [0.9358974358974359, 0.8846153846153846, 0.846...   \n",
       "1  [0.9487179487179487, 0.9487179487179487, 0.948...   \n",
       "2  [0.8333333333333334, 0.8333333333333334, 0.846...   \n",
       "3  [0.8717948717948718, 0.8589743589743589, 0.846...   \n",
       "4  [0.8461538461538461, 0.8461538461538461, 0.846...   \n",
       "5  [0.8461538461538461, 0.8461538461538461, 0.833...   \n",
       "6  [0.8461538461538461, 0.8333333333333334, 0.794...   \n",
       "7  [0.8333333333333334, 0.7948717948717948, 0.743...   \n",
       "8  [0.7051282051282052, 0.717948717948718, 0.7564...   \n",
       "9  [0.7564102564102564, 0.7564102564102564, 0.820...   \n",
       "\n",
       "                      wind_speed_10m_scaled_sequence  \\\n",
       "0  [0.14094775212636695, 0.20534629404617252, 0.2...   \n",
       "1  [0.11421628189550426, 0.12636695018226005, 0.1...   \n",
       "2  [0.15188335358444716, 0.14823815309842042, 0.1...   \n",
       "3  [0.15795868772782504, 0.15795868772782504, 0.1...   \n",
       "4  [0.17982989064398544, 0.17739975698663427, 0.1...   \n",
       "5  [0.17739975698663427, 0.17496962332928312, 0.1...   \n",
       "6  [0.17496962332928312, 0.13122721749696234, 0.1...   \n",
       "7  [0.13122721749696234, 0.13730255164034022, 0.1...   \n",
       "8  [0.1822600243013366, 0.13001215066828675, 0.11...   \n",
       "9  [0.11907654921020658, 0.13973268529769137, 0.1...   \n",
       "\n",
       "                    surface_pressure_scaled_sequence  \\\n",
       "0  [0.5490196078431373, 0.5784313725490197, 0.570...   \n",
       "1  [0.6137254901960776, 0.623529411764705, 0.6333...   \n",
       "2  [0.6313725490196087, 0.6352941176470583, 0.643...   \n",
       "3  [0.7058823529411765, 0.7098039215686284, 0.705...   \n",
       "4  [0.6882352941176475, 0.7000000000000008, 0.711...   \n",
       "5  [0.7000000000000008, 0.7117647058823521, 0.727...   \n",
       "6  [0.7117647058823521, 0.7274509803921573, 0.741...   \n",
       "7  [0.7274509803921573, 0.7411764705882344, 0.741...   \n",
       "8  [0.6764705882352942, 0.6823529411764697, 0.686...   \n",
       "9  [0.6862745098039216, 0.696078431372549, 0.7196...   \n",
       "\n",
       "                       precipitation_scaled_sequence  \\\n",
       "0  [0.009398496240601503, 0.013157894736842105, 0...   \n",
       "1  [0.011278195488721804, 0.007518796992481203, 0...   \n",
       "2  [0.0037593984962406013, 0.0018796992481203006,...   \n",
       "3  [0.0, 0.0, 0.0018796992481203006, 0.0, 0.0, 0....   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0018796992481203006, 0....   \n",
       "5  [0.0, 0.0, 0.0, 0.0018796992481203006, 0.0, 0....   \n",
       "6  [0.0, 0.0, 0.0018796992481203006, 0.0, 0.0, 0....   \n",
       "7  [0.0, 0.0018796992481203006, 0.0, 0.0, 0.0, 0....   \n",
       "8  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "9  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00375939...   \n",
       "\n",
       "                                   hour_sin_sequence  \\\n",
       "0  [0.9659258262890683, -0.2588190451025208, -0.4...   \n",
       "1  [-0.8660254037844386, -0.7071067811865477, -0....   \n",
       "2  [-0.7071067811865471, -0.8660254037844384, -0....   \n",
       "3  [-0.5000000000000004, -0.25881904510252157, 0....   \n",
       "4  [0.9659258262890683, 1.0, 0.9659258262890683, ...   \n",
       "5  [1.0, 0.9659258262890683, 0.8660254037844387, ...   \n",
       "6  [0.9659258262890683, 0.8660254037844387, 0.707...   \n",
       "7  [0.8660254037844387, 0.7071067811865476, 0.499...   \n",
       "8  [-0.8660254037844384, -0.9659258262890683, -1....   \n",
       "9  [-1.0, -0.9659258262890684, -0.866025403784438...   \n",
       "\n",
       "                                   hour_cos_sequence  \\\n",
       "0  [-0.25881904510252063, -0.9659258262890683, -0...   \n",
       "1  [0.5000000000000001, 0.7071067811865474, 0.866...   \n",
       "2  [-0.7071067811865479, -0.5000000000000004, -0....   \n",
       "3  [0.8660254037844384, 0.9659258262890681, 1.0, ...   \n",
       "4  [0.25881904510252074, 6.123233995736766e-17, -...   \n",
       "5  [6.123233995736766e-17, -0.25881904510252063, ...   \n",
       "6  [-0.25881904510252063, -0.4999999999999998, -0...   \n",
       "7  [-0.4999999999999998, -0.7071067811865475, -0....   \n",
       "8  [-0.5000000000000004, -0.25881904510252063, -1...   \n",
       "9  [-1.8369701987210297e-16, 0.2588190451025203, ...   \n",
       "\n",
       "                                  month_sin_sequence  \\\n",
       "0  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "1  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "2  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "3  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "4  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "5  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "6  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "7  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "8  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "9  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "\n",
       "                                  month_cos_sequence  \\\n",
       "0  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "1  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "2  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "3  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "4  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "5  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "6  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "7  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "8  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "9  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "\n",
       "                            day_of_week_sin_sequence  \\\n",
       "0  [-0.9749279121818236, -0.9749279121818236, -0....   \n",
       "1  [-0.9749279121818236, -0.9749279121818236, -0....   \n",
       "2  [-0.7818314824680299, -0.7818314824680299, -0....   \n",
       "3  [-0.7818314824680299, -0.7818314824680299, -2....   \n",
       "4  [-2.4492935982947064e-16, -2.4492935982947064e...   \n",
       "5  [-2.4492935982947064e-16, -2.4492935982947064e...   \n",
       "6  [-2.4492935982947064e-16, -2.4492935982947064e...   \n",
       "7  [-2.4492935982947064e-16, -2.4492935982947064e...   \n",
       "8  [-2.4492935982947064e-16, -2.4492935982947064e...   \n",
       "9  [-2.4492935982947064e-16, -2.4492935982947064e...   \n",
       "\n",
       "                            day_of_week_cos_sequence  \\\n",
       "0  [-0.2225209339563146, -0.2225209339563146, -0....   \n",
       "1  [-0.2225209339563146, -0.2225209339563146, -0....   \n",
       "2  [0.6234898018587334, 0.6234898018587334, 0.623...   \n",
       "3  [0.6234898018587334, 0.6234898018587334, 1.0, ...   \n",
       "4  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n",
       "5  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n",
       "6  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n",
       "7  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n",
       "8  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.623...   \n",
       "9  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6234898018587...   \n",
       "\n",
       "                         wind_direction_sin_sequence  \\\n",
       "0  [0.9961946980917455, 0.9993908270190958, 0.994...   \n",
       "1  [1.0, 0.9993908270190958, 0.9925461516413221, ...   \n",
       "2  [0.754709580222772, 0.8290375725550417, 0.7880...   \n",
       "3  [0.6946583704589973, 0.6946583704589973, 0.694...   \n",
       "4  [0.7771459614569708, 0.788010753606722, 0.7986...   \n",
       "5  [0.788010753606722, 0.7986355100472928, 0.4999...   \n",
       "6  [0.7986355100472928, 0.49999999999999994, 0.51...   \n",
       "7  [0.49999999999999994, 0.5150380749100542, 0.57...   \n",
       "8  [0.6691306063588582, 0.5446390350150271, 0.469...   \n",
       "9  [0.4694715627858908, 0.5299192642332049, 0.358...   \n",
       "\n",
       "                         wind_direction_cos_sequence  \\\n",
       "0  [-0.08715574274765824, -0.03489949670250073, -...   \n",
       "1  [6.123233995736766e-17, 0.03489949670250108, -...   \n",
       "2  [0.6560590289905073, 0.5591929034707468, 0.615...   \n",
       "3  [0.7193398003386512, 0.7193398003386512, 0.719...   \n",
       "4  [0.6293203910498375, 0.6156614753256583, 0.601...   \n",
       "5  [0.6156614753256583, 0.6018150231520484, 0.866...   \n",
       "6  [0.6018150231520484, 0.8660254037844387, 0.857...   \n",
       "7  [0.8660254037844387, 0.8571673007021123, 0.819...   \n",
       "8  [0.7431448254773942, 0.838670567945424, 0.8829...   \n",
       "9  [0.882947592858927, 0.848048096156426, 0.93358...   \n",
       "\n",
       "                                 is_weekend_sequence  target_value  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      0.118110  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      0.074147  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...      0.096457  \n",
       "3  [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      0.079396  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      0.099081  \n",
       "5  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      0.090551  \n",
       "6  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      0.093176  \n",
       "7  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      0.107612  \n",
       "8  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      0.069554  \n",
       "9  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      0.088583  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3d7ddcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:51:30.922653Z",
     "iopub.status.busy": "2025-11-20T10:51:30.922324Z",
     "iopub.status.idle": "2025-11-20T10:51:30.992387Z",
     "shell.execute_reply": "2025-11-20T10:51:30.991585Z"
    },
    "papermill": {
     "duration": 0.121219,
     "end_time": "2025-11-20T10:51:30.993810",
     "exception": false,
     "start_time": "2025-11-20T10:51:30.872591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>PM10_scaled_sequence</th>\n",
       "      <th>NO2_scaled_sequence</th>\n",
       "      <th>SO2_scaled_sequence</th>\n",
       "      <th>temperature_2m_scaled_sequence</th>\n",
       "      <th>relative_humidity_2m_scaled_sequence</th>\n",
       "      <th>wind_speed_10m_scaled_sequence</th>\n",
       "      <th>surface_pressure_scaled_sequence</th>\n",
       "      <th>precipitation_scaled_sequence</th>\n",
       "      <th>hour_sin_sequence</th>\n",
       "      <th>hour_cos_sequence</th>\n",
       "      <th>month_sin_sequence</th>\n",
       "      <th>month_cos_sequence</th>\n",
       "      <th>day_of_week_sin_sequence</th>\n",
       "      <th>day_of_week_cos_sequence</th>\n",
       "      <th>wind_direction_sin_sequence</th>\n",
       "      <th>wind_direction_cos_sequence</th>\n",
       "      <th>is_weekend_sequence</th>\n",
       "      <th>target_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-04 15:00:00</td>\n",
       "      <td>[0.01935767707875055, 0.059392872855257364, 0....</td>\n",
       "      <td>[0.1828434723171565, 0.11073137388926861, 0.08...</td>\n",
       "      <td>[0.018205461638491544, 0.014304291287386216, 0...</td>\n",
       "      <td>[0.5718475073313782, 0.5865102639296187, 0.601...</td>\n",
       "      <td>[0.9358974358974359, 0.8846153846153846, 0.846...</td>\n",
       "      <td>[0.14094775212636695, 0.20534629404617252, 0.2...</td>\n",
       "      <td>[0.5490196078431373, 0.5784313725490197, 0.570...</td>\n",
       "      <td>[0.009398496240601503, 0.013157894736842105, 0...</td>\n",
       "      <td>[0.9659258262890683, -0.2588190451025208, -0.4...</td>\n",
       "      <td>[-0.25881904510252063, -0.9659258262890683, -0...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-0.9749279121818236, -0.9749279121818236, -0....</td>\n",
       "      <td>[-0.2225209339563146, -0.2225209339563146, -0....</td>\n",
       "      <td>[0.9961946980917455, 0.9993908270190958, 0.994...</td>\n",
       "      <td>[-0.08715574274765824, -0.03489949670250073, -...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.098425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-04 18:00:00</td>\n",
       "      <td>[0.04663440387153541, 0.042674879014518254, 0....</td>\n",
       "      <td>[0.06971975393028024, 0.062200956937799035, 0....</td>\n",
       "      <td>[0.011703511053315994, 0.018205461638491544, 0...</td>\n",
       "      <td>[0.5953079178885631, 0.5659824046920822, 0.577...</td>\n",
       "      <td>[0.8846153846153846, 0.9615384615384616, 0.948...</td>\n",
       "      <td>[0.1968408262454435, 0.1057108140947752, 0.114...</td>\n",
       "      <td>[0.5686274509803921, 0.5960784313725486, 0.613...</td>\n",
       "      <td>[0.06203007518796992, 0.015037593984962405, 0....</td>\n",
       "      <td>[-0.7071067811865471, -0.9659258262890684, -0....</td>\n",
       "      <td>[-0.7071067811865479, 0.2588190451025203, 0.50...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-0.9749279121818236, -0.9749279121818236, -0....</td>\n",
       "      <td>[-0.2225209339563146, -0.2225209339563146, -0....</td>\n",
       "      <td>[0.9781476007338057, 0.7071067811865475, 1.0, ...</td>\n",
       "      <td>[-0.20791169081775912, 0.7071067811865476, 6.1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.106299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-04 20:00:00</td>\n",
       "      <td>[0.06467223933128025, 0.08007039155301363, 0.0...</td>\n",
       "      <td>[0.045112781954887216, 0.04032809295967191, 0....</td>\n",
       "      <td>[0.024707412223667097, 0.029908972691807537, 0...</td>\n",
       "      <td>[0.5777126099706745, 0.5835777126099707, 0.583...</td>\n",
       "      <td>[0.9487179487179487, 0.9487179487179487, 0.948...</td>\n",
       "      <td>[0.11421628189550426, 0.12636695018226005, 0.1...</td>\n",
       "      <td>[0.6137254901960776, 0.623529411764705, 0.6333...</td>\n",
       "      <td>[0.011278195488721804, 0.007518796992481203, 0...</td>\n",
       "      <td>[-0.8660254037844386, -0.7071067811865477, -0....</td>\n",
       "      <td>[0.5000000000000001, 0.7071067811865474, 0.866...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-0.9749279121818236, -0.9749279121818236, -0....</td>\n",
       "      <td>[-0.2225209339563146, -0.2225209339563146, -0....</td>\n",
       "      <td>[1.0, 0.9993908270190958, 0.9925461516413221, ...</td>\n",
       "      <td>[6.123233995736766e-17, 0.03489949670250108, -...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.078084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-04 23:00:00</td>\n",
       "      <td>[0.0796304443466784, 0.07347118345798503, 0.08...</td>\n",
       "      <td>[0.10457963089542037, 0.1548188653451811, 0.15...</td>\n",
       "      <td>[0.02860858257477243, 0.03250975292587776, 0.0...</td>\n",
       "      <td>[0.5865102639296187, 0.5806451612903226, 0.571...</td>\n",
       "      <td>[0.9487179487179487, 0.9615384615384616, 0.961...</td>\n",
       "      <td>[0.14094775212636695, 0.11057108140947752, 0.1...</td>\n",
       "      <td>[0.6313725490196087, 0.6254901960784309, 0.619...</td>\n",
       "      <td>[0.015037593984962405, 0.03759398496240601, 0....</td>\n",
       "      <td>[-0.25881904510252157, 0.0, 0.2588190451025207...</td>\n",
       "      <td>[0.9659258262890681, 1.0, 0.9659258262890683, ...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-0.9749279121818236, -0.7818314824680299, -0....</td>\n",
       "      <td>[-0.2225209339563146, 0.6234898018587334, 0.62...</td>\n",
       "      <td>[0.9925461516413221, 0.9876883405951378, 0.819...</td>\n",
       "      <td>[-0.12186934340514737, 0.15643446504023092, 0....</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.040682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-05 02:00:00</td>\n",
       "      <td>[0.1029476462824461, 0.10426748790145182, 0.09...</td>\n",
       "      <td>[0.12235133287764864, 0.11346548188653452, 0.1...</td>\n",
       "      <td>[0.03641092327698309, 0.03641092327698309, 0.0...</td>\n",
       "      <td>[0.5718475073313782, 0.5747800586510264, 0.574...</td>\n",
       "      <td>[0.9615384615384616, 0.9487179487179487, 0.923...</td>\n",
       "      <td>[0.13001215066828675, 0.16403402187120292, 0.1...</td>\n",
       "      <td>[0.6156862745098035, 0.605882352941176, 0.6078...</td>\n",
       "      <td>[0.013157894736842105, 0.005639097744360902, 0...</td>\n",
       "      <td>[0.49999999999999994, 0.7071067811865475, 0.86...</td>\n",
       "      <td>[0.8660254037844387, 0.7071067811865476, 0.500...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-0.7818314824680299, -0.7818314824680299, -0....</td>\n",
       "      <td>[0.6234898018587334, 0.6234898018587334, 0.623...</td>\n",
       "      <td>[0.9396926207859083, 0.9876883405951378, 0.970...</td>\n",
       "      <td>[0.3420201433256688, 0.15643446504023092, 0.24...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.062992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-05 16:00:00</td>\n",
       "      <td>[0.09546854377474702, 0.10822701275846898, 0.1...</td>\n",
       "      <td>[0.09501025290498974, 0.07279562542720437, 0.0...</td>\n",
       "      <td>[0.02860858257477243, 0.029908972691807537, 0....</td>\n",
       "      <td>[0.6187683284457478, 0.6187683284457478, 0.609...</td>\n",
       "      <td>[0.8333333333333334, 0.8333333333333334, 0.846...</td>\n",
       "      <td>[0.15188335358444716, 0.14823815309842042, 0.1...</td>\n",
       "      <td>[0.6313725490196087, 0.6352941176470583, 0.643...</td>\n",
       "      <td>[0.0037593984962406013, 0.0018796992481203006,...</td>\n",
       "      <td>[-0.7071067811865471, -0.8660254037844384, -0....</td>\n",
       "      <td>[-0.7071067811865479, -0.5000000000000004, -0....</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-0.7818314824680299, -0.7818314824680299, -0....</td>\n",
       "      <td>[0.6234898018587334, 0.6234898018587334, 0.623...</td>\n",
       "      <td>[0.754709580222772, 0.8290375725550417, 0.7880...</td>\n",
       "      <td>[0.6560590289905073, 0.5591929034707468, 0.615...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0.118110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-05 21:00:00</td>\n",
       "      <td>[0.08227012758468984, 0.060272767267927845, 0....</td>\n",
       "      <td>[0.053998632946001365, 0.060833902939166094, 0...</td>\n",
       "      <td>[0.018205461638491544, 0.02080624187256177, 0....</td>\n",
       "      <td>[0.5366568914956011, 0.5337243401759532, 0.527...</td>\n",
       "      <td>[0.8717948717948718, 0.8846153846153846, 0.871...</td>\n",
       "      <td>[0.1543134872417983, 0.1543134872417983, 0.157...</td>\n",
       "      <td>[0.6843137254901956, 0.696078431372549, 0.7058...</td>\n",
       "      <td>[0.0018796992481203006, 0.0, 0.0, 0.0, 0.00187...</td>\n",
       "      <td>[-0.8660254037844386, -0.7071067811865477, -0....</td>\n",
       "      <td>[0.5000000000000001, 0.7071067811865474, 0.866...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-0.7818314824680299, -0.7818314824680299, -0....</td>\n",
       "      <td>[0.6234898018587334, 0.6234898018587334, 0.623...</td>\n",
       "      <td>[0.6819983600624985, 0.6819983600624985, 0.694...</td>\n",
       "      <td>[0.7313537016191706, 0.7313537016191706, 0.719...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0.074147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-06 20:00:00</td>\n",
       "      <td>[0.09766827980642322, 0.10162780466344039, 0.1...</td>\n",
       "      <td>[0.12371838687628162, 0.101161995898838, 0.078...</td>\n",
       "      <td>[0.03120936280884265, 0.024707412223667097, 0....</td>\n",
       "      <td>[0.5953079178885631, 0.5777126099706745, 0.560...</td>\n",
       "      <td>[0.7051282051282052, 0.717948717948718, 0.7564...</td>\n",
       "      <td>[0.1822600243013366, 0.13001215066828675, 0.11...</td>\n",
       "      <td>[0.6764705882352942, 0.6823529411764697, 0.686...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[-0.8660254037844384, -0.9659258262890683, -1....</td>\n",
       "      <td>[-0.5000000000000004, -0.25881904510252063, -1...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-2.4492935982947064e-16, -2.4492935982947064e...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.623...</td>\n",
       "      <td>[0.6691306063588582, 0.5446390350150271, 0.469...</td>\n",
       "      <td>[0.7431448254773942, 0.838670567945424, 0.8829...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0.096457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-07 07:00:00</td>\n",
       "      <td>[0.09766827980642322, 0.09898812142542894, 0.0...</td>\n",
       "      <td>[0.101161995898838, 0.09706083390293915, 0.101...</td>\n",
       "      <td>[0.016905071521456438, 0.02080624187256177, 0....</td>\n",
       "      <td>[0.5161290322580645, 0.5102639296187683, 0.498...</td>\n",
       "      <td>[0.8076923076923077, 0.8076923076923077, 0.820...</td>\n",
       "      <td>[0.1275820170109356, 0.10814094775212638, 0.11...</td>\n",
       "      <td>[0.7294117647058832, 0.7274509803921573, 0.723...</td>\n",
       "      <td>[0.0, 0.0, 0.0037593984962406013, 0.0018796992...</td>\n",
       "      <td>[-0.25881904510252157, 0.0, 0.2588190451025207...</td>\n",
       "      <td>[0.9659258262890681, 1.0, 0.9659258262890683, ...</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[-2.4492935982947064e-16, 0.7818314824680298, ...</td>\n",
       "      <td>[1.0, 0.6234898018587336, 0.6234898018587336, ...</td>\n",
       "      <td>[0.6560590289905072, 0.5299192642332049, 0.629...</td>\n",
       "      <td>[0.7547095802227721, 0.848048096156426, 0.7771...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>0.079396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>233335</td>\n",
       "      <td>2022-11-07 14:00:00</td>\n",
       "      <td>[0.10338759348878133, 0.08271007479102507, 0.0...</td>\n",
       "      <td>[0.11756664388243335, 0.1640464798359535, 0.15...</td>\n",
       "      <td>[0.014304291287386216, 0.018205461638491544, 0...</td>\n",
       "      <td>[0.5073313782991202, 0.5219941348973607, 0.539...</td>\n",
       "      <td>[0.8589743589743589, 0.8461538461538461, 0.782...</td>\n",
       "      <td>[0.1069258809234508, 0.11907654921020658, 0.13...</td>\n",
       "      <td>[0.7490196078431381, 0.7411764705882344, 0.717...</td>\n",
       "      <td>[0.015037593984962405, 0.06954887218045112, 0....</td>\n",
       "      <td>[0.49999999999999994, 0.258819045102521, 1.224...</td>\n",
       "      <td>[-0.8660254037844387, -0.9659258262890682, -1....</td>\n",
       "      <td>[-0.5000000000000004, -0.5000000000000004, -0....</td>\n",
       "      <td>[0.8660254037844384, 0.8660254037844384, 0.866...</td>\n",
       "      <td>[0.7818314824680298, 0.7818314824680298, 0.781...</td>\n",
       "      <td>[0.6234898018587336, 0.6234898018587336, 0.623...</td>\n",
       "      <td>[0.8191520442889918, 0.8090169943749475, 0.719...</td>\n",
       "      <td>[0.5735764363510462, 0.5877852522924731, 0.694...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>0.099081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  location_id            datetime  \\\n",
       "0      233335 2022-11-04 15:00:00   \n",
       "1      233335 2022-11-04 18:00:00   \n",
       "2      233335 2022-11-04 20:00:00   \n",
       "3      233335 2022-11-04 23:00:00   \n",
       "4      233335 2022-11-05 02:00:00   \n",
       "5      233335 2022-11-05 16:00:00   \n",
       "6      233335 2022-11-05 21:00:00   \n",
       "7      233335 2022-11-06 20:00:00   \n",
       "8      233335 2022-11-07 07:00:00   \n",
       "9      233335 2022-11-07 14:00:00   \n",
       "\n",
       "                                PM10_scaled_sequence  \\\n",
       "0  [0.01935767707875055, 0.059392872855257364, 0....   \n",
       "1  [0.04663440387153541, 0.042674879014518254, 0....   \n",
       "2  [0.06467223933128025, 0.08007039155301363, 0.0...   \n",
       "3  [0.0796304443466784, 0.07347118345798503, 0.08...   \n",
       "4  [0.1029476462824461, 0.10426748790145182, 0.09...   \n",
       "5  [0.09546854377474702, 0.10822701275846898, 0.1...   \n",
       "6  [0.08227012758468984, 0.060272767267927845, 0....   \n",
       "7  [0.09766827980642322, 0.10162780466344039, 0.1...   \n",
       "8  [0.09766827980642322, 0.09898812142542894, 0.0...   \n",
       "9  [0.10338759348878133, 0.08271007479102507, 0.0...   \n",
       "\n",
       "                                 NO2_scaled_sequence  \\\n",
       "0  [0.1828434723171565, 0.11073137388926861, 0.08...   \n",
       "1  [0.06971975393028024, 0.062200956937799035, 0....   \n",
       "2  [0.045112781954887216, 0.04032809295967191, 0....   \n",
       "3  [0.10457963089542037, 0.1548188653451811, 0.15...   \n",
       "4  [0.12235133287764864, 0.11346548188653452, 0.1...   \n",
       "5  [0.09501025290498974, 0.07279562542720437, 0.0...   \n",
       "6  [0.053998632946001365, 0.060833902939166094, 0...   \n",
       "7  [0.12371838687628162, 0.101161995898838, 0.078...   \n",
       "8  [0.101161995898838, 0.09706083390293915, 0.101...   \n",
       "9  [0.11756664388243335, 0.1640464798359535, 0.15...   \n",
       "\n",
       "                                 SO2_scaled_sequence  \\\n",
       "0  [0.018205461638491544, 0.014304291287386216, 0...   \n",
       "1  [0.011703511053315994, 0.018205461638491544, 0...   \n",
       "2  [0.024707412223667097, 0.029908972691807537, 0...   \n",
       "3  [0.02860858257477243, 0.03250975292587776, 0.0...   \n",
       "4  [0.03641092327698309, 0.03641092327698309, 0.0...   \n",
       "5  [0.02860858257477243, 0.029908972691807537, 0....   \n",
       "6  [0.018205461638491544, 0.02080624187256177, 0....   \n",
       "7  [0.03120936280884265, 0.024707412223667097, 0....   \n",
       "8  [0.016905071521456438, 0.02080624187256177, 0....   \n",
       "9  [0.014304291287386216, 0.018205461638491544, 0...   \n",
       "\n",
       "                      temperature_2m_scaled_sequence  \\\n",
       "0  [0.5718475073313782, 0.5865102639296187, 0.601...   \n",
       "1  [0.5953079178885631, 0.5659824046920822, 0.577...   \n",
       "2  [0.5777126099706745, 0.5835777126099707, 0.583...   \n",
       "3  [0.5865102639296187, 0.5806451612903226, 0.571...   \n",
       "4  [0.5718475073313782, 0.5747800586510264, 0.574...   \n",
       "5  [0.6187683284457478, 0.6187683284457478, 0.609...   \n",
       "6  [0.5366568914956011, 0.5337243401759532, 0.527...   \n",
       "7  [0.5953079178885631, 0.5777126099706745, 0.560...   \n",
       "8  [0.5161290322580645, 0.5102639296187683, 0.498...   \n",
       "9  [0.5073313782991202, 0.5219941348973607, 0.539...   \n",
       "\n",
       "                relative_humidity_2m_scaled_sequence  \\\n",
       "0  [0.9358974358974359, 0.8846153846153846, 0.846...   \n",
       "1  [0.8846153846153846, 0.9615384615384616, 0.948...   \n",
       "2  [0.9487179487179487, 0.9487179487179487, 0.948...   \n",
       "3  [0.9487179487179487, 0.9615384615384616, 0.961...   \n",
       "4  [0.9615384615384616, 0.9487179487179487, 0.923...   \n",
       "5  [0.8333333333333334, 0.8333333333333334, 0.846...   \n",
       "6  [0.8717948717948718, 0.8846153846153846, 0.871...   \n",
       "7  [0.7051282051282052, 0.717948717948718, 0.7564...   \n",
       "8  [0.8076923076923077, 0.8076923076923077, 0.820...   \n",
       "9  [0.8589743589743589, 0.8461538461538461, 0.782...   \n",
       "\n",
       "                      wind_speed_10m_scaled_sequence  \\\n",
       "0  [0.14094775212636695, 0.20534629404617252, 0.2...   \n",
       "1  [0.1968408262454435, 0.1057108140947752, 0.114...   \n",
       "2  [0.11421628189550426, 0.12636695018226005, 0.1...   \n",
       "3  [0.14094775212636695, 0.11057108140947752, 0.1...   \n",
       "4  [0.13001215066828675, 0.16403402187120292, 0.1...   \n",
       "5  [0.15188335358444716, 0.14823815309842042, 0.1...   \n",
       "6  [0.1543134872417983, 0.1543134872417983, 0.157...   \n",
       "7  [0.1822600243013366, 0.13001215066828675, 0.11...   \n",
       "8  [0.1275820170109356, 0.10814094775212638, 0.11...   \n",
       "9  [0.1069258809234508, 0.11907654921020658, 0.13...   \n",
       "\n",
       "                    surface_pressure_scaled_sequence  \\\n",
       "0  [0.5490196078431373, 0.5784313725490197, 0.570...   \n",
       "1  [0.5686274509803921, 0.5960784313725486, 0.613...   \n",
       "2  [0.6137254901960776, 0.623529411764705, 0.6333...   \n",
       "3  [0.6313725490196087, 0.6254901960784309, 0.619...   \n",
       "4  [0.6156862745098035, 0.605882352941176, 0.6078...   \n",
       "5  [0.6313725490196087, 0.6352941176470583, 0.643...   \n",
       "6  [0.6843137254901956, 0.696078431372549, 0.7058...   \n",
       "7  [0.6764705882352942, 0.6823529411764697, 0.686...   \n",
       "8  [0.7294117647058832, 0.7274509803921573, 0.723...   \n",
       "9  [0.7490196078431381, 0.7411764705882344, 0.717...   \n",
       "\n",
       "                       precipitation_scaled_sequence  \\\n",
       "0  [0.009398496240601503, 0.013157894736842105, 0...   \n",
       "1  [0.06203007518796992, 0.015037593984962405, 0....   \n",
       "2  [0.011278195488721804, 0.007518796992481203, 0...   \n",
       "3  [0.015037593984962405, 0.03759398496240601, 0....   \n",
       "4  [0.013157894736842105, 0.005639097744360902, 0...   \n",
       "5  [0.0037593984962406013, 0.0018796992481203006,...   \n",
       "6  [0.0018796992481203006, 0.0, 0.0, 0.0, 0.00187...   \n",
       "7  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "8  [0.0, 0.0, 0.0037593984962406013, 0.0018796992...   \n",
       "9  [0.015037593984962405, 0.06954887218045112, 0....   \n",
       "\n",
       "                                   hour_sin_sequence  \\\n",
       "0  [0.9659258262890683, -0.2588190451025208, -0.4...   \n",
       "1  [-0.7071067811865471, -0.9659258262890684, -0....   \n",
       "2  [-0.8660254037844386, -0.7071067811865477, -0....   \n",
       "3  [-0.25881904510252157, 0.0, 0.2588190451025207...   \n",
       "4  [0.49999999999999994, 0.7071067811865475, 0.86...   \n",
       "5  [-0.7071067811865471, -0.8660254037844384, -0....   \n",
       "6  [-0.8660254037844386, -0.7071067811865477, -0....   \n",
       "7  [-0.8660254037844384, -0.9659258262890683, -1....   \n",
       "8  [-0.25881904510252157, 0.0, 0.2588190451025207...   \n",
       "9  [0.49999999999999994, 0.258819045102521, 1.224...   \n",
       "\n",
       "                                   hour_cos_sequence  \\\n",
       "0  [-0.25881904510252063, -0.9659258262890683, -0...   \n",
       "1  [-0.7071067811865479, 0.2588190451025203, 0.50...   \n",
       "2  [0.5000000000000001, 0.7071067811865474, 0.866...   \n",
       "3  [0.9659258262890681, 1.0, 0.9659258262890683, ...   \n",
       "4  [0.8660254037844387, 0.7071067811865476, 0.500...   \n",
       "5  [-0.7071067811865479, -0.5000000000000004, -0....   \n",
       "6  [0.5000000000000001, 0.7071067811865474, 0.866...   \n",
       "7  [-0.5000000000000004, -0.25881904510252063, -1...   \n",
       "8  [0.9659258262890681, 1.0, 0.9659258262890683, ...   \n",
       "9  [-0.8660254037844387, -0.9659258262890682, -1....   \n",
       "\n",
       "                                  month_sin_sequence  \\\n",
       "0  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "1  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "2  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "3  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "4  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "5  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "6  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "7  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "8  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "9  [-0.5000000000000004, -0.5000000000000004, -0....   \n",
       "\n",
       "                                  month_cos_sequence  \\\n",
       "0  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "1  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "2  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "3  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "4  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "5  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "6  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "7  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "8  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "9  [0.8660254037844384, 0.8660254037844384, 0.866...   \n",
       "\n",
       "                            day_of_week_sin_sequence  \\\n",
       "0  [-0.9749279121818236, -0.9749279121818236, -0....   \n",
       "1  [-0.9749279121818236, -0.9749279121818236, -0....   \n",
       "2  [-0.9749279121818236, -0.9749279121818236, -0....   \n",
       "3  [-0.9749279121818236, -0.7818314824680299, -0....   \n",
       "4  [-0.7818314824680299, -0.7818314824680299, -0....   \n",
       "5  [-0.7818314824680299, -0.7818314824680299, -0....   \n",
       "6  [-0.7818314824680299, -0.7818314824680299, -0....   \n",
       "7  [-2.4492935982947064e-16, -2.4492935982947064e...   \n",
       "8  [-2.4492935982947064e-16, 0.7818314824680298, ...   \n",
       "9  [0.7818314824680298, 0.7818314824680298, 0.781...   \n",
       "\n",
       "                            day_of_week_cos_sequence  \\\n",
       "0  [-0.2225209339563146, -0.2225209339563146, -0....   \n",
       "1  [-0.2225209339563146, -0.2225209339563146, -0....   \n",
       "2  [-0.2225209339563146, -0.2225209339563146, -0....   \n",
       "3  [-0.2225209339563146, 0.6234898018587334, 0.62...   \n",
       "4  [0.6234898018587334, 0.6234898018587334, 0.623...   \n",
       "5  [0.6234898018587334, 0.6234898018587334, 0.623...   \n",
       "6  [0.6234898018587334, 0.6234898018587334, 0.623...   \n",
       "7  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.623...   \n",
       "8  [1.0, 0.6234898018587336, 0.6234898018587336, ...   \n",
       "9  [0.6234898018587336, 0.6234898018587336, 0.623...   \n",
       "\n",
       "                         wind_direction_sin_sequence  \\\n",
       "0  [0.9961946980917455, 0.9993908270190958, 0.994...   \n",
       "1  [0.9781476007338057, 0.7071067811865475, 1.0, ...   \n",
       "2  [1.0, 0.9993908270190958, 0.9925461516413221, ...   \n",
       "3  [0.9925461516413221, 0.9876883405951378, 0.819...   \n",
       "4  [0.9396926207859083, 0.9876883405951378, 0.970...   \n",
       "5  [0.754709580222772, 0.8290375725550417, 0.7880...   \n",
       "6  [0.6819983600624985, 0.6819983600624985, 0.694...   \n",
       "7  [0.6691306063588582, 0.5446390350150271, 0.469...   \n",
       "8  [0.6560590289905072, 0.5299192642332049, 0.629...   \n",
       "9  [0.8191520442889918, 0.8090169943749475, 0.719...   \n",
       "\n",
       "                         wind_direction_cos_sequence  \\\n",
       "0  [-0.08715574274765824, -0.03489949670250073, -...   \n",
       "1  [-0.20791169081775912, 0.7071067811865476, 6.1...   \n",
       "2  [6.123233995736766e-17, 0.03489949670250108, -...   \n",
       "3  [-0.12186934340514737, 0.15643446504023092, 0....   \n",
       "4  [0.3420201433256688, 0.15643446504023092, 0.24...   \n",
       "5  [0.6560590289905073, 0.5591929034707468, 0.615...   \n",
       "6  [0.7313537016191706, 0.7313537016191706, 0.719...   \n",
       "7  [0.7431448254773942, 0.838670567945424, 0.8829...   \n",
       "8  [0.7547095802227721, 0.848048096156426, 0.7771...   \n",
       "9  [0.5735764363510462, 0.5877852522924731, 0.694...   \n",
       "\n",
       "                                 is_weekend_sequence  target_value  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      0.098425  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      0.106299  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      0.078084  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      0.040682  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      0.062992  \n",
       "5  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...      0.118110  \n",
       "6  [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      0.074147  \n",
       "7  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      0.096457  \n",
       "8  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      0.079396  \n",
       "9  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...      0.099081  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f26617b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:51:31.097321Z",
     "iopub.status.busy": "2025-11-20T10:51:31.096678Z",
     "iopub.status.idle": "2025-11-20T10:51:31.125317Z",
     "shell.execute_reply": "2025-11-20T10:51:31.124482Z"
    },
    "papermill": {
     "duration": 0.081881,
     "end_time": "2025-11-20T10:51:31.126678",
     "exception": false,
     "start_time": "2025-11-20T10:51:31.044797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>PM2_5_scaled</th>\n",
       "      <th>PM10_scaled</th>\n",
       "      <th>NO2_scaled</th>\n",
       "      <th>SO2_scaled</th>\n",
       "      <th>temperature_2m_scaled</th>\n",
       "      <th>relative_humidity_2m_scaled</th>\n",
       "      <th>wind_speed_10m_scaled</th>\n",
       "      <th>surface_pressure_scaled</th>\n",
       "      <th>...</th>\n",
       "      <th>surface_pressure_lag3_scaled</th>\n",
       "      <th>surface_pressure_lag6_scaled</th>\n",
       "      <th>surface_pressure_lag12_scaled</th>\n",
       "      <th>surface_pressure_lag24_scaled</th>\n",
       "      <th>precipitation_lag1_scaled</th>\n",
       "      <th>precipitation_lag2_scaled</th>\n",
       "      <th>precipitation_lag3_scaled</th>\n",
       "      <th>precipitation_lag6_scaled</th>\n",
       "      <th>precipitation_lag12_scaled</th>\n",
       "      <th>precipitation_lag24_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7727</td>\n",
       "      <td>2022-11-02 02:00:00</td>\n",
       "      <td>0.020997</td>\n",
       "      <td>0.034316</td>\n",
       "      <td>0.158920</td>\n",
       "      <td>0.065020</td>\n",
       "      <td>0.542522</td>\n",
       "      <td>0.641026</td>\n",
       "      <td>0.397327</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>...</td>\n",
       "      <td>0.498039</td>\n",
       "      <td>0.496078</td>\n",
       "      <td>0.519608</td>\n",
       "      <td>0.515686</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.016917</td>\n",
       "      <td>0.033835</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7727</td>\n",
       "      <td>2022-11-02 04:00:00</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>0.021557</td>\n",
       "      <td>0.158237</td>\n",
       "      <td>0.066320</td>\n",
       "      <td>0.521994</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.391252</td>\n",
       "      <td>0.458824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.494118</td>\n",
       "      <td>0.503922</td>\n",
       "      <td>0.503922</td>\n",
       "      <td>0.507843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.016917</td>\n",
       "      <td>0.067669</td>\n",
       "      <td>0.005639</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7727</td>\n",
       "      <td>2022-11-02 06:00:00</td>\n",
       "      <td>0.098425</td>\n",
       "      <td>0.069952</td>\n",
       "      <td>0.147300</td>\n",
       "      <td>0.067620</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.679487</td>\n",
       "      <td>0.385176</td>\n",
       "      <td>0.462745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.486275</td>\n",
       "      <td>0.501961</td>\n",
       "      <td>0.474510</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.009398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.016917</td>\n",
       "      <td>0.001880</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7727</td>\n",
       "      <td>2022-11-02 07:00:00</td>\n",
       "      <td>0.086614</td>\n",
       "      <td>0.071711</td>\n",
       "      <td>0.137731</td>\n",
       "      <td>0.070221</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.679487</td>\n",
       "      <td>0.381531</td>\n",
       "      <td>0.476471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.498039</td>\n",
       "      <td>0.472549</td>\n",
       "      <td>0.478431</td>\n",
       "      <td>0.005639</td>\n",
       "      <td>0.009398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7727</td>\n",
       "      <td>2022-11-02 10:00:00</td>\n",
       "      <td>0.034121</td>\n",
       "      <td>0.037835</td>\n",
       "      <td>0.082365</td>\n",
       "      <td>0.065020</td>\n",
       "      <td>0.530792</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.393682</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.458824</td>\n",
       "      <td>0.494118</td>\n",
       "      <td>0.474510</td>\n",
       "      <td>0.474510</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.005639</td>\n",
       "      <td>0.009398</td>\n",
       "      <td>0.016917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7727</td>\n",
       "      <td>2022-11-02 11:00:00</td>\n",
       "      <td>0.035433</td>\n",
       "      <td>0.043115</td>\n",
       "      <td>0.072796</td>\n",
       "      <td>0.070221</td>\n",
       "      <td>0.542522</td>\n",
       "      <td>0.756410</td>\n",
       "      <td>0.398542</td>\n",
       "      <td>0.525490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.462745</td>\n",
       "      <td>0.486275</td>\n",
       "      <td>0.480392</td>\n",
       "      <td>0.472549</td>\n",
       "      <td>0.016917</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.005639</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7727</td>\n",
       "      <td>2022-11-02 12:00:00</td>\n",
       "      <td>0.051181</td>\n",
       "      <td>0.050154</td>\n",
       "      <td>0.068694</td>\n",
       "      <td>0.066320</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.407047</td>\n",
       "      <td>0.513725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.476471</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.496078</td>\n",
       "      <td>0.484314</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.016917</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7727</td>\n",
       "      <td>2022-11-02 13:00:00</td>\n",
       "      <td>0.045276</td>\n",
       "      <td>0.040035</td>\n",
       "      <td>0.078264</td>\n",
       "      <td>0.065020</td>\n",
       "      <td>0.536657</td>\n",
       "      <td>0.756410</td>\n",
       "      <td>0.403402</td>\n",
       "      <td>0.498039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.458824</td>\n",
       "      <td>0.503922</td>\n",
       "      <td>0.496078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.016917</td>\n",
       "      <td>0.009398</td>\n",
       "      <td>0.067669</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7727</td>\n",
       "      <td>2022-11-02 14:00:00</td>\n",
       "      <td>0.036745</td>\n",
       "      <td>0.040475</td>\n",
       "      <td>0.050239</td>\n",
       "      <td>0.067620</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.400972</td>\n",
       "      <td>0.484314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525490</td>\n",
       "      <td>0.462745</td>\n",
       "      <td>0.501961</td>\n",
       "      <td>0.525490</td>\n",
       "      <td>0.001880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.005639</td>\n",
       "      <td>0.016917</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7727</td>\n",
       "      <td>2022-11-02 15:00:00</td>\n",
       "      <td>0.032808</td>\n",
       "      <td>0.038715</td>\n",
       "      <td>0.050239</td>\n",
       "      <td>0.067620</td>\n",
       "      <td>0.551320</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.386391</td>\n",
       "      <td>0.478431</td>\n",
       "      <td>...</td>\n",
       "      <td>0.513725</td>\n",
       "      <td>0.476471</td>\n",
       "      <td>0.498039</td>\n",
       "      <td>0.537255</td>\n",
       "      <td>0.001880</td>\n",
       "      <td>0.001880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.033835</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  location_id            datetime  PM2_5_scaled  PM10_scaled  NO2_scaled  \\\n",
       "0        7727 2022-11-02 02:00:00      0.020997     0.034316    0.158920   \n",
       "1        7727 2022-11-02 04:00:00      0.001969     0.021557    0.158237   \n",
       "2        7727 2022-11-02 06:00:00      0.098425     0.069952    0.147300   \n",
       "3        7727 2022-11-02 07:00:00      0.086614     0.071711    0.137731   \n",
       "4        7727 2022-11-02 10:00:00      0.034121     0.037835    0.082365   \n",
       "5        7727 2022-11-02 11:00:00      0.035433     0.043115    0.072796   \n",
       "6        7727 2022-11-02 12:00:00      0.051181     0.050154    0.068694   \n",
       "7        7727 2022-11-02 13:00:00      0.045276     0.040035    0.078264   \n",
       "8        7727 2022-11-02 14:00:00      0.036745     0.040475    0.050239   \n",
       "9        7727 2022-11-02 15:00:00      0.032808     0.038715    0.050239   \n",
       "\n",
       "   SO2_scaled  temperature_2m_scaled  relative_humidity_2m_scaled  \\\n",
       "0    0.065020               0.542522                     0.641026   \n",
       "1    0.066320               0.521994                     0.653846   \n",
       "2    0.067620               0.516129                     0.679487   \n",
       "3    0.070221               0.516129                     0.679487   \n",
       "4    0.065020               0.530792                     0.769231   \n",
       "5    0.070221               0.542522                     0.756410   \n",
       "6    0.066320               0.545455                     0.743590   \n",
       "7    0.065020               0.536657                     0.756410   \n",
       "8    0.067620               0.545455                     0.743590   \n",
       "9    0.067620               0.551320                     0.730769   \n",
       "\n",
       "   wind_speed_10m_scaled  surface_pressure_scaled  ...  \\\n",
       "0               0.397327                 0.470588  ...   \n",
       "1               0.391252                 0.458824  ...   \n",
       "2               0.385176                 0.462745  ...   \n",
       "3               0.381531                 0.476471  ...   \n",
       "4               0.393682                 0.529412  ...   \n",
       "5               0.398542                 0.525490  ...   \n",
       "6               0.407047                 0.513725  ...   \n",
       "7               0.403402                 0.498039  ...   \n",
       "8               0.400972                 0.484314  ...   \n",
       "9               0.386391                 0.478431  ...   \n",
       "\n",
       "   surface_pressure_lag3_scaled  surface_pressure_lag6_scaled  \\\n",
       "0                      0.498039                      0.496078   \n",
       "1                      0.494118                      0.503922   \n",
       "2                      0.486275                      0.501961   \n",
       "3                      0.470588                      0.498039   \n",
       "4                      0.458824                      0.494118   \n",
       "5                      0.462745                      0.486275   \n",
       "6                      0.476471                      0.470588   \n",
       "7                      0.529412                      0.458824   \n",
       "8                      0.525490                      0.462745   \n",
       "9                      0.513725                      0.476471   \n",
       "\n",
       "   surface_pressure_lag12_scaled  surface_pressure_lag24_scaled  \\\n",
       "0                       0.519608                       0.515686   \n",
       "1                       0.503922                       0.507843   \n",
       "2                       0.474510                       0.490196   \n",
       "3                       0.472549                       0.478431   \n",
       "4                       0.474510                       0.474510   \n",
       "5                       0.480392                       0.472549   \n",
       "6                       0.496078                       0.484314   \n",
       "7                       0.503922                       0.496078   \n",
       "8                       0.501961                       0.525490   \n",
       "9                       0.498039                       0.537255   \n",
       "\n",
       "   precipitation_lag1_scaled  precipitation_lag2_scaled  \\\n",
       "0                   0.003759                   0.016917   \n",
       "1                   0.000000                   0.003759   \n",
       "2                   0.009398                   0.000000   \n",
       "3                   0.005639                   0.009398   \n",
       "4                   0.007519                   0.005639   \n",
       "5                   0.016917                   0.007519   \n",
       "6                   0.003759                   0.016917   \n",
       "7                   0.000000                   0.003759   \n",
       "8                   0.001880                   0.000000   \n",
       "9                   0.001880                   0.001880   \n",
       "\n",
       "   precipitation_lag3_scaled  precipitation_lag6_scaled  \\\n",
       "0                   0.033835                   0.013158   \n",
       "1                   0.016917                   0.067669   \n",
       "2                   0.003759                   0.016917   \n",
       "3                   0.000000                   0.033835   \n",
       "4                   0.009398                   0.016917   \n",
       "5                   0.005639                   0.003759   \n",
       "6                   0.007519                   0.000000   \n",
       "7                   0.016917                   0.009398   \n",
       "8                   0.003759                   0.005639   \n",
       "9                   0.000000                   0.007519   \n",
       "\n",
       "   precipitation_lag12_scaled  precipitation_lag24_scaled  \n",
       "0                    0.000000                         0.0  \n",
       "1                    0.005639                         0.0  \n",
       "2                    0.001880                         0.0  \n",
       "3                    0.000000                         0.0  \n",
       "4                    0.000000                         0.0  \n",
       "5                    0.000000                         0.0  \n",
       "6                    0.013158                         0.0  \n",
       "7                    0.067669                         0.0  \n",
       "8                    0.016917                         0.0  \n",
       "9                    0.033835                         0.0  \n",
       "\n",
       "[10 rows x 74 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_train.head(10)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8787588,
     "sourceId": 13801744,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1402.87466,
   "end_time": "2025-11-20T10:51:34.096961",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-20T10:28:11.222301",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

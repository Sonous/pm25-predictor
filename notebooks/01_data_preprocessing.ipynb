{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b24733ca",
   "metadata": {},
   "source": [
    "# PM2.5 Prediction - Data Preprocessing\n",
    "\n",
    "Notebook nÃ y thá»±c hiá»‡n tiá»n xá»­ lÃ½ dá»¯ liá»‡u vá»›i PySpark:\n",
    "1. Káº¿t ná»‘i Spark cluster\n",
    "2. Äá»c vÃ  khÃ¡m phÃ¡ dá»¯ liá»‡u\n",
    "3. Tá»•ng quan vá» dataset\n",
    "4. **LÃ m sáº¡ch dá»¯ liá»‡u** (Outlier Removal â†’ Missing Value Imputation)\n",
    "5. Feature engineering\n",
    "6. Data summary & statistics\n",
    "7. LÆ°u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6420c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’» Running on Local Machine\n",
      "âœ“ Using Java: C:\\Program Files\\Java\\jdk-21\n",
      "âœ“ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Detect environment (Kaggle vs Colab vs Local)\n",
    "IN_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    print(\"ğŸ† Running on Kaggle\")\n",
    "    \n",
    "    # Kaggle has Java pre-installed, just set JAVA_HOME\n",
    "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "    os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ.get(\"PATH\", \"\")\n",
    "    \n",
    "    # Install PySpark\n",
    "    print(\"ğŸ“¦ Installing PySpark...\")\n",
    "    !pip install -q pyspark\n",
    "    print(\"âœ“ PySpark installed\")\n",
    "    print(f\"âœ“ Java: {os.environ['JAVA_HOME']}\")\n",
    "    \n",
    "elif IN_COLAB:\n",
    "    print(\"ğŸŒ Running on Google Colab\")\n",
    "    \n",
    "    # Install Java 11 (required for PySpark)\n",
    "    print(\"ğŸ“¦ Installing Java 11...\")\n",
    "    !apt-get install -y openjdk-11-jdk-headless -qq > /dev/null\n",
    "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "    os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ.get(\"PATH\", \"\")\n",
    "    print(f\"âœ“ Java installed: {os.environ['JAVA_HOME']}\")\n",
    "    \n",
    "    # Install PySpark\n",
    "    print(\"ğŸ“¦ Installing PySpark...\")\n",
    "    !pip install -q pyspark\n",
    "    print(\"âœ“ PySpark installed\")\n",
    "    \n",
    "    # Mount Google Drive (optional - if data is in Drive)\n",
    "    # from google.colab import drive\n",
    "    # drive.mount('/content/drive')\n",
    "    \n",
    "else:\n",
    "    print(\"ğŸ’» Running on Local Machine\")\n",
    "    \n",
    "    # Set Java 21 for PySpark (local only)\n",
    "    os.environ['JAVA_HOME'] = r'C:\\Program Files\\Java\\jdk-21'\n",
    "    os.environ['PATH'] = os.environ['JAVA_HOME'] + r'\\bin;' + os.environ.get('PATH', '')\n",
    "    print(f\"âœ“ Using Java: {os.environ['JAVA_HOME']}\")\n",
    "\n",
    "# Common imports\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"âœ“ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18df14d2",
   "metadata": {},
   "source": [
    "## 1. Káº¿t ná»‘i Spark Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057cbfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’» Spark running on Local (4GB memory)\n",
      "âœ“ Spark version: 4.0.1\n",
      "âœ“ Spark mode: local[*]\n",
      "âœ“ Application ID: local-1762619080056\n",
      "âœ“ Cores: 12\n"
     ]
    }
   ],
   "source": [
    "# Táº¡o Spark Session vá»›i cáº¥u hÃ¬nh tÃ¹y theo mÃ´i trÆ°á»ng\n",
    "if IN_KAGGLE:\n",
    "    # Kaggle configuration - Balanced (4 cores, 16GB RAM)\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PM25-Preprocessing\") \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"6g\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "        .config(\"spark.default.parallelism\", \"8\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(\"ğŸ† Spark running on Kaggle (4 cores, 12GB total)\")\n",
    "    \n",
    "elif IN_COLAB:\n",
    "    # Colab configuration - Lighter settings (2 cores, 12GB RAM)\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PM25-Preprocessing\") \\\n",
    "        .master(\"local[2]\") \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .config(\"spark.executor.memory\", \"2g\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(\"ğŸŒ Spark running on Colab (2 cores, 4GB total)\")\n",
    "    \n",
    "else:\n",
    "    # Local configuration - OPTIMIZED for 8-core AMD Ryzen + 15.7GB RAM\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PM25-Preprocessing\") \\\n",
    "        .master(\"local[8]\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"16\") \\\n",
    "        .config(\"spark.default.parallelism\", \"16\") \\\n",
    "        .config(\"spark.python.worker.timeout\", \"600\") \\\n",
    "        .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "        .config(\"spark.network.timeout\", \"600s\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(\"ğŸ’» Spark running on Local (8 cores, 12GB total - OPTIMIZED)\")\n",
    "\n",
    "print(f\"âœ“ Spark version: {spark.version}\")\n",
    "print(f\"âœ“ Spark mode: {spark.sparkContext.master}\")\n",
    "print(f\"âœ“ Application ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"âœ“ Cores: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"âœ“ Parallelism: {spark.conf.get('spark.default.parallelism', 'default')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cdf19f",
   "metadata": {},
   "source": [
    "## 2. Äá»‹nh nghÄ©a Schema vÃ  Scan Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f6e4012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Schemas defined\n"
     ]
    }
   ],
   "source": [
    "# Schema cho dá»¯ liá»‡u OpenAQ\n",
    "openaq_schema = StructType([\n",
    "    StructField(\"location_id\", StringType(), True),\n",
    "    StructField(\"sensors_id\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"datetime\", TimestampType(), True),\n",
    "    StructField(\"lat\", DoubleType(), True),\n",
    "    StructField(\"lon\", DoubleType(), True),\n",
    "    StructField(\"parameter\", StringType(), True),\n",
    "    StructField(\"units\", StringType(), True),\n",
    "    StructField(\"value\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Schema cho dá»¯ liá»‡u Weather\n",
    "weather_schema = StructType([\n",
    "    StructField(\"time\", TimestampType(), True),\n",
    "    StructField(\"temperature_2m\", DoubleType(), True),\n",
    "    StructField(\"relative_humidity_2m\", DoubleType(), True),\n",
    "    StructField(\"wind_speed_10m\", DoubleType(), True),\n",
    "    StructField(\"wind_direction_10m\", DoubleType(), True),\n",
    "    StructField(\"surface_pressure\", DoubleType(), True),\n",
    "    StructField(\"precipitation\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "print(\"âœ“ Schemas defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb7210a",
   "metadata": {},
   "source": [
    "### 2.1 Scan vÃ  Map Files theo Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b984b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Found 14 pollutant files:\n",
      "  âœ“ Location 233335: pollutant_location_233335.csv + weather_location_233335.csv\n",
      "  âœ“ Location 233336: pollutant_location_233336.csv + weather_location_233336.csv\n",
      "  âœ“ Location 7727: pollutant_location_7727.csv + weather_location_7727.csv\n",
      "  âœ“ Location 7728: pollutant_location_7728.csv + weather_location_7728.csv\n",
      "  âœ“ Location 7730: pollutant_location_7730.csv + weather_location_7730.csv\n",
      "  âœ“ Location 7732: pollutant_location_7732.csv + weather_location_7732.csv\n",
      "  âœ“ Location 7733: pollutant_location_7733.csv + weather_location_7733.csv\n",
      "  âœ“ Location 7734: pollutant_location_7734.csv + weather_location_7734.csv\n",
      "  âœ“ Location 7735: pollutant_location_7735.csv + weather_location_7735.csv\n",
      "  âœ“ Location 7736: pollutant_location_7736.csv + weather_location_7736.csv\n",
      "  âœ“ Location 7737: pollutant_location_7737.csv + weather_location_7737.csv\n",
      "  âœ“ Location 7739: pollutant_location_7739.csv + weather_location_7739.csv\n",
      "  âœ“ Location 7740: pollutant_location_7740.csv + weather_location_7740.csv\n",
      "  âœ“ Location 7742: pollutant_location_7742.csv + weather_location_7742.csv\n",
      "\n",
      "âœ… Total locations to process: 14\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# ========================================\n",
    "# KAGGLE: Sá»­ dá»¥ng Ä‘Æ°á»ng dáº«n Kaggle dataset\n",
    "# ========================================\n",
    "if IN_KAGGLE:\n",
    "    # ğŸ† Kaggle paths\n",
    "    # Format: /kaggle/input/{dataset-name}/\n",
    "    raw_data_path = Path(\"/kaggle/input/pm25-hongkong-raw\")  # â† Thay tÃªn dataset cá»§a báº¡n\n",
    "    print(f\"ğŸ† Using Kaggle dataset: {raw_data_path}\")\n",
    "    \n",
    "elif IN_COLAB:\n",
    "    # ğŸŒ Colab: Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    raw_data_path = Path(\"/content/drive/MyDrive/pm25-data/raw\")  # â† Thay Ä‘Æ°á»ng dáº«n Drive cá»§a báº¡n\n",
    "    print(f\"ğŸŒ Using Google Drive: {raw_data_path}\")\n",
    "    \n",
    "else:\n",
    "    # ğŸ’» Local path (giá»¯ nguyÃªn)\n",
    "    raw_data_path = Path(\"../data/raw\")\n",
    "    print(f\"ğŸ’» Using local path: {raw_data_path}\")\n",
    "\n",
    "# TÃ¬m táº¥t cáº£ cÃ¡c file pollutant\n",
    "pollutant_files = list(raw_data_path.glob(\"pollutant_location_*.csv\"))\n",
    "\n",
    "print(f\"ğŸ“ Found {len(pollutant_files)} pollutant files:\")\n",
    "\n",
    "# Táº¡o mapping giá»¯a pollutant vÃ  weather files\n",
    "location_mapping = {}\n",
    "\n",
    "for pollutant_file in pollutant_files:\n",
    "    # Extract location_id tá»« tÃªn file: pollutant_location_7727.csv â†’ 7727\n",
    "    match = re.search(r'pollutant_location_(\\d+)\\.csv', pollutant_file.name)\n",
    "    \n",
    "    if match:\n",
    "        location_id = match.group(1)\n",
    "        weather_file = raw_data_path / f\"weather_location_{location_id}.csv\"\n",
    "        \n",
    "        # Kiá»ƒm tra file weather tÆ°Æ¡ng á»©ng cÃ³ tá»“n táº¡i khÃ´ng\n",
    "        if weather_file.exists():\n",
    "            location_mapping[location_id] = {\n",
    "                'pollutant': str(pollutant_file),\n",
    "                'weather': str(weather_file)\n",
    "            }\n",
    "            print(f\"  âœ“ Location {location_id}: {pollutant_file.name} + {weather_file.name}\")\n",
    "        else:\n",
    "            print(f\"  âš ï¸  Location {location_id}: Missing weather file!\")\n",
    "\n",
    "print(f\"\\nâœ… Total locations to process: {len(location_mapping)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bd57cd",
   "metadata": {},
   "source": [
    "### 2.2 Xá»­ lÃ½ tá»«ng Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62c0476b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ Processing Location 233335...\n",
      "  ğŸ“Š Air quality (PM2.5, PM10, SO2, NO2): 83,970 records\n",
      "  ğŸŒ¤ï¸  Weather: 25,560 records\n",
      "  âœ“ After join: 21,686 records\n",
      "\n",
      "ğŸ”„ Processing Location 233336...\n",
      "  ğŸ“Š Air quality (PM2.5, PM10, SO2, NO2): 83,818 records\n",
      "  ğŸŒ¤ï¸  Weather: 25,560 records\n",
      "  âœ“ After join: 21,262 records\n",
      "\n",
      "ğŸ”„ Processing Location 7727...\n",
      "  ğŸ“Š Air quality (PM2.5, PM10, SO2, NO2): 86,684 records\n",
      "  ğŸŒ¤ï¸  Weather: 25,560 records\n",
      "  âœ“ After join: 22,367 records\n",
      "\n",
      "ğŸ”„ Processing Location 7728...\n",
      "  ğŸ“Š Air quality (PM2.5, PM10, SO2, NO2): 84,638 records\n",
      "  ğŸŒ¤ï¸  Weather: 25,560 records\n",
      "  âœ“ After join: 21,683 records\n",
      "\n",
      "ğŸ”„ Processing Location 7730...\n",
      "  ğŸ“Š Air quality (PM2.5, PM10, SO2, NO2): 82,776 records\n",
      "  ğŸŒ¤ï¸  Weather: 25,560 records\n",
      "  âœ“ After join: 21,274 records\n",
      "\n",
      "ğŸ”„ Processing Location 7732...\n",
      "  ğŸ“Š Air quality (PM2.5, PM10, SO2, NO2): 83,386 records\n",
      "  ğŸŒ¤ï¸  Weather: 25,560 records\n",
      "  âœ“ After join: 21,306 records\n",
      "\n",
      "ğŸ”„ Processing Location 7733...\n",
      "  ğŸ“Š Air quality (PM2.5, PM10, SO2, NO2): 83,047 records\n",
      "  ğŸŒ¤ï¸  Weather: 25,560 records\n",
      "  âœ“ After join: 21,235 records\n",
      "\n",
      "ğŸ”„ Processing Location 7734...\n",
      "  ğŸ“Š Air quality (PM2.5, PM10, SO2, NO2): 82,570 records\n",
      "  ğŸŒ¤ï¸  Weather: 25,560 records\n",
      "  âœ“ After join: 21,254 records\n",
      "\n",
      "ğŸ”„ Processing Location 7735...\n",
      "  ğŸ“Š Air quality (PM2.5, PM10, SO2, NO2): 83,373 records\n",
      "  ğŸŒ¤ï¸  Weather: 25,560 records\n",
      "  âœ“ After join: 21,277 records\n",
      "\n",
      "ğŸ”„ Processing Location 7736...\n",
      "  ğŸ“Š Air quality (PM2.5, PM10, SO2, NO2): 83,295 records\n",
      "  ğŸŒ¤ï¸  Weather: 25,560 records\n",
      "  âœ“ After join: 21,281 records\n",
      "\n",
      "ğŸ”„ Processing Location 7737...\n",
      "  ğŸ“Š Air quality (PM2.5, PM10, SO2, NO2): 83,079 records\n",
      "  ğŸŒ¤ï¸  Weather: 25,560 records\n",
      "  âœ“ After join: 21,275 records\n",
      "\n",
      "ğŸ”„ Processing Location 7739...\n",
      "  ğŸ“Š Air quality (PM2.5, PM10, SO2, NO2): 84,686 records\n",
      "  ğŸŒ¤ï¸  Weather: 25,560 records\n",
      "  âœ“ After join: 21,728 records\n",
      "\n",
      "ğŸ”„ Processing Location 7740...\n",
      "  ğŸ“Š Air quality (PM2.5, PM10, SO2, NO2): 84,504 records\n",
      "  ğŸŒ¤ï¸  Weather: 25,560 records\n",
      "  âœ“ After join: 21,694 records\n",
      "\n",
      "ğŸ”„ Processing Location 7742...\n",
      "  ğŸ“Š Air quality (PM2.5, PM10, SO2, NO2): 82,234 records\n",
      "  ğŸŒ¤ï¸  Weather: 25,560 records\n",
      "  âœ“ After join: 21,094 records\n",
      "\n",
      "âœ… Processed 14 locations successfully!\n"
     ]
    }
   ],
   "source": [
    "# List Ä‘á»ƒ chá»©a dataframes cá»§a tá»«ng location\n",
    "all_locations_data = []\n",
    "\n",
    "for location_id, files in location_mapping.items():\n",
    "    print(f\"\\nğŸ”„ Processing Location {location_id}...\")\n",
    "    \n",
    "    # Äá»c pollutant data\n",
    "    df_air = spark.read.csv(\n",
    "        files['pollutant'],\n",
    "        header=True,\n",
    "        schema=openaq_schema\n",
    "    )\n",
    "    \n",
    "    # ğŸ” Lá»ŒC CHá»ˆ Láº¤Y CÃC CHá»ˆ Sá» QUAN TÃ‚M: PM2.5, PM10, SO2, NO2\n",
    "    df_air = df_air.filter(\n",
    "        F.col(\"parameter\").isin([\"pm25\", \"pm10\", \"so2\", \"no2\"])\n",
    "    )\n",
    "    \n",
    "    # Äá»c weather data\n",
    "    df_weather = spark.read.csv(\n",
    "        files['weather'],\n",
    "        header=True,\n",
    "        schema=weather_schema\n",
    "    )\n",
    "    \n",
    "    print(f\"  ğŸ“Š Air quality (PM2.5, PM10, SO2, NO2): {df_air.count():,} records\")\n",
    "    print(f\"  ğŸŒ¤ï¸  Weather: {df_weather.count():,} records\")\n",
    "    \n",
    "    # Weather data - drop missing (Ã­t missing)\n",
    "    df_weather_clean = df_weather.na.drop()\n",
    "    \n",
    "    # Pivot pollutant data\n",
    "    df_air_pivot = df_air.groupBy(\n",
    "        \"location_id\", \"location\", \"datetime\", \"lat\", \"lon\"\n",
    "    ).pivot(\"parameter\").agg(F.first(\"value\"))\n",
    "    \n",
    "    # Rename columns\n",
    "    column_mapping = {\n",
    "        \"pm25\": \"PM2_5\",\n",
    "        \"pm10\": \"PM10\",\n",
    "        \"no2\": \"NO2\",\n",
    "        \"so2\": \"SO2\"\n",
    "    }\n",
    "    \n",
    "    for old_name, new_name in column_mapping.items():\n",
    "        if old_name in df_air_pivot.columns:\n",
    "            df_air_pivot = df_air_pivot.withColumnRenamed(old_name, new_name)\n",
    "    \n",
    "    # Join vá»›i weather data (theo datetime)\n",
    "    df_location = df_air_pivot.join(\n",
    "        df_weather_clean,\n",
    "        df_air_pivot.datetime == df_weather_clean.time,\n",
    "        \"inner\"\n",
    "    ).drop(\"time\")\n",
    "    \n",
    "    print(f\"  âœ“ After join: {df_location.count():,} records\")\n",
    "    \n",
    "    # ThÃªm vÃ o list\n",
    "    all_locations_data.append(df_location)\n",
    "\n",
    "print(f\"\\nâœ… Processed {len(all_locations_data)} locations successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b156b356",
   "metadata": {},
   "source": [
    "### 2.3 Gá»™p táº¥t cáº£ Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "409a2995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Combining 14 locations...\n",
      "â³ Computing combined dataset (this may take a moment)...\n",
      "âœ… Combined dataset: 300,416 total records\n",
      "âœ… Number of locations: 14\n",
      "\n",
      "ğŸ“‹ Sample records (unsorted):\n",
      "+-----------+------------+-------------------+------------------+---------+----+----+-----+---+--------------+--------------------+--------------+------------------+----------------+-------------+\n",
      "|location_id|location    |datetime           |lat               |lon      |NO2 |PM10|PM2_5|SO2|temperature_2m|relative_humidity_2m|wind_speed_10m|wind_direction_10m|surface_pressure|precipitation|\n",
      "+-----------+------------+-------------------+------------------+---------+----+----+-----+---+--------------+--------------------+--------------+------------------+----------------+-------------+\n",
      "|233335     |North-245631|2023-10-01 05:00:00|22.496710000000004|114.12824|25.3|26.4|14.1 |1.6|24.6          |99.0                |2.8           |220.0             |1007.5          |0.0          |\n",
      "|233335     |North-245631|2023-12-03 08:00:00|22.496710000000004|114.12824|29.4|42.1|21.6 |3.7|16.7          |65.0                |11.6          |54.0              |1020.5          |0.0          |\n",
      "|233335     |North-245631|2023-11-09 20:00:00|22.496710000000004|114.12824|23.8|33.1|10.3 |2.0|24.3          |88.0                |10.9          |98.0              |1014.1          |0.0          |\n",
      "|233335     |North-245631|2022-11-11 13:00:00|22.49671          |114.12824|23.3|15.3|13.1 |2.4|27.2          |68.0                |10.2          |58.0              |1014.9          |0.0          |\n",
      "|233335     |North-245631|2023-02-20 00:00:00|22.49671          |114.12824|19.5|37.6|16.5 |2.1|19.3          |55.0                |18.5          |37.0              |1017.6          |0.0          |\n",
      "|233335     |North-245631|2023-09-04 07:00:00|22.496710000000004|114.12824|40.3|22.5|15.5 |2.7|26.3          |93.0                |9.8           |336.0             |1000.8          |0.0          |\n",
      "|233335     |North-245631|2023-02-14 07:00:00|22.49671          |114.12824|26.4|12.7|5.5  |2.0|12.2          |69.0                |20.4          |36.0              |1018.1          |0.0          |\n",
      "|233335     |North-245631|2023-02-19 06:00:00|22.49671          |114.12824|50.2|24.7|16.0 |1.4|17.0          |90.0                |5.1           |39.0              |1016.5          |0.0          |\n",
      "|233335     |North-245631|2023-01-20 17:00:00|22.49671          |114.12824|47.3|30.0|17.8 |2.1|18.0          |57.0                |9.5           |29.0              |1018.1          |0.0          |\n",
      "|233335     |North-245631|2023-07-28 02:00:00|22.496710000000004|114.12824|18.8|18.9|12.6 |1.1|27.6          |95.0                |6.3           |257.0             |994.6           |0.0          |\n",
      "+-----------+------------+-------------------+------------------+---------+----+----+-----+---+--------------+--------------------+--------------+------------------+----------------+-------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Gá»™p táº¥t cáº£ locations láº¡i\n",
    "print(f\"ğŸ”„ Combining {len(all_locations_data)} locations...\")\n",
    "\n",
    "df_combined = all_locations_data[0]\n",
    "for df in all_locations_data[1:]:\n",
    "    df_combined = df_combined.union(df)\n",
    "\n",
    "# OPTIMIZE: Cache Ä‘á»ƒ trÃ¡nh recompute nhiá»u láº§n\n",
    "df_combined = df_combined.cache()\n",
    "\n",
    "# OPTIMIZE: Trigger action 1 láº§n, trÃ¡nh count() nhiá»u láº§n\n",
    "print(\"â³ Computing combined dataset (this may take a moment)...\")\n",
    "total_records = df_combined.count()\n",
    "num_locations = df_combined.select('location_id').distinct().count()\n",
    "\n",
    "print(f\"âœ… Combined dataset: {total_records:,} total records\")\n",
    "print(f\"âœ… Number of locations: {num_locations}\")\n",
    "\n",
    "# OPTIMIZE: Chá»‰ show sample, khÃ´ng orderBy toÃ n bá»™ dataset (ráº¥t cháº­m!)\n",
    "print(\"\\nğŸ“‹ Sample records (unsorted):\")\n",
    "df_combined.show(10, truncate=False)\n",
    "\n",
    "# OPTIONAL: Náº¿u cáº§n sort, chá»‰ sort 1 partition nhá» Ä‘á»ƒ xem\n",
    "# df_combined.orderBy(\"location_id\", \"datetime\").limit(50).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3751add",
   "metadata": {},
   "source": [
    "## 3. Tá»•ng quan Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ae5670b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Dataset Overview by Location:\n",
      "+-----------+--------------------+-----+\n",
      "|location_id|location            |count|\n",
      "+-----------+--------------------+-----+\n",
      "|233335     |North-245631        |21686|\n",
      "|233336     |Southern-245632     |21262|\n",
      "|7727       |Tung Chung-7727     |22367|\n",
      "|7728       |Mong Kok-7728       |21683|\n",
      "|7730       |Central/Western-7730|21274|\n",
      "|7732       |Causeway Bay-7732   |21306|\n",
      "|7733       |Sha Tin-7733        |21235|\n",
      "|7734       |Sham Shui Po-7734   |21254|\n",
      "|7735       |Kwun Tong-7735      |21277|\n",
      "|7736       |Kwai Chung-7736     |21281|\n",
      "|7737       |Tai Po-7737         |21275|\n",
      "|7739       |Yuen Long-7739      |21728|\n",
      "|7740       |Tsuen Wan-7740      |21694|\n",
      "|7742       |Tuen Mun-7742       |2375 |\n",
      "|7742       |Tuen Mun-932161     |18719|\n",
      "+-----------+--------------------+-----+\n",
      "\n",
      "\n",
      "ğŸ“… Time Range by Location:\n",
      "+-----------+-------------------+-------------------+-------+\n",
      "|location_id|start_date         |end_date           |records|\n",
      "+-----------+-------------------+-------------------+-------+\n",
      "|233335     |2022-11-01 00:00:00|2025-09-30 23:00:00|21686  |\n",
      "|233336     |2022-11-01 00:00:00|2025-09-30 23:00:00|21262  |\n",
      "|7727       |2022-11-01 00:00:00|2025-09-30 23:00:00|22367  |\n",
      "|7728       |2022-11-01 00:00:00|2025-09-30 23:00:00|21683  |\n",
      "|7730       |2022-11-01 00:00:00|2025-09-30 23:00:00|21274  |\n",
      "|7732       |2022-11-01 00:00:00|2025-09-30 23:00:00|21306  |\n",
      "|7733       |2022-11-01 00:00:00|2025-09-30 23:00:00|21235  |\n",
      "|7734       |2022-11-01 00:00:00|2025-09-30 23:00:00|21254  |\n",
      "|7735       |2022-11-01 00:00:00|2025-09-30 23:00:00|21277  |\n",
      "|7736       |2022-11-01 00:00:00|2025-09-30 23:00:00|21281  |\n",
      "|7737       |2022-11-01 00:00:00|2025-09-30 23:00:00|21275  |\n",
      "|7739       |2022-11-01 00:00:00|2025-09-30 23:00:00|21728  |\n",
      "|7740       |2022-11-01 00:00:00|2025-09-30 23:00:00|21694  |\n",
      "|7742       |2022-11-01 00:00:00|2025-09-30 23:00:00|21094  |\n",
      "+-----------+-------------------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Thá»‘ng kÃª theo location\n",
    "print(\"ğŸ“Š Dataset Overview by Location:\")\n",
    "df_combined.groupBy(\"location_id\", \"location\").count().orderBy(\"location_id\").show(truncate=False)\n",
    "\n",
    "# Time range cá»§a tá»«ng location\n",
    "print(\"\\nğŸ“… Time Range by Location:\")\n",
    "df_combined.groupBy(\"location_id\").agg(\n",
    "    F.min(\"datetime\").alias(\"start_date\"),\n",
    "    F.max(\"datetime\").alias(\"end_date\"),\n",
    "    F.count(\"*\").alias(\"records\")\n",
    ").orderBy(\"location_id\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "368fb50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  Missing Values Summary:\n",
      "  NO2                      :    7,620 (  2.54%)\n",
      "  PM10                     :    3,391 (  1.13%)\n",
      "  PM2_5                    :   11,161 (  3.72%)\n",
      "  SO2                      :    7,432 (  2.47%)\n"
     ]
    }
   ],
   "source": [
    "# Kiá»ƒm tra missing values\n",
    "print(\"âš ï¸  Missing Values Summary:\")\n",
    "for col_name in df_combined.columns:\n",
    "    null_count = df_combined.filter(F.col(col_name).isNull()).count()\n",
    "    total = df_combined.count()\n",
    "    pct = (null_count / total) * 100\n",
    "    if null_count > 0:  # Chá»‰ hiá»ƒn thá»‹ cá»™t cÃ³ missing\n",
    "        print(f\"  {col_name:25s}: {null_count:8,} ({pct:6.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f767ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ˆ Overall Statistics:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+------------------+--------------------+------------------+------------------+\n",
      "|summary|             PM2_5|              PM10|               NO2|               SO2|    temperature_2m|relative_humidity_2m|    wind_speed_10m|     precipitation|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+--------------------+------------------+------------------+\n",
      "|  count|            289255|            297025|            292796|            292984|            300416|              300416|            300416|            300416|\n",
      "|   mean|15.684557224594208|25.423372443397003|39.196086012103926| 3.720443778499844|23.179505752023825|   79.53433239241585|12.479650884107365|0.2698651203664245|\n",
      "| stddev|10.895508720844566|19.754274618263263| 26.13457654994168|2.4376132869062395| 5.546174423257824|  15.460363021119056| 6.232915473285468|1.1481021462125374|\n",
      "|    min|               0.0|               0.0|               0.0|               0.0|               1.9|                16.0|               0.0|               0.0|\n",
      "|    max|             182.5|             401.7|             292.6|              76.9|              36.5|               100.0|              86.6|              53.2|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+--------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Statistics tá»•ng quan\n",
    "print(\"ğŸ“ˆ Overall Statistics:\")\n",
    "df_combined.select(\n",
    "    \"PM2_5\", \"PM10\", \"NO2\", \"SO2\",\n",
    "    \"temperature_2m\", \"relative_humidity_2m\", \"wind_speed_10m\", \"precipitation\"\n",
    ").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02439644",
   "metadata": {},
   "source": [
    "## 4. LÃ m sáº¡ch Dá»¯ liá»‡u\n",
    "\n",
    "**Quy trÃ¬nh lÃ m sáº¡ch:**\n",
    "1. **Loáº¡i bá» Outliers trÆ°á»›c** - Äá»ƒ trÃ¡nh giÃ¡ trá»‹ cá»±c Ä‘oan áº£nh hÆ°á»Ÿng Ä‘áº¿n tÃ­nh toÃ¡n statistics\n",
    "2. **Fill Missing Values sau** - Imputation dá»±a trÃªn dá»¯ liá»‡u Ä‘Ã£ loáº¡i bá» outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e126b6",
   "metadata": {},
   "source": [
    "### 4.1. Loáº¡i bá» Outliers\n",
    "\n",
    "Loáº¡i bá» cÃ¡c giÃ¡ trá»‹ cá»±c Ä‘oan trÆ°á»›c khi imputation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a8a917f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Outlier Removal:\n",
      "  Before: 300,416 records\n",
      "  After:  289,255 records\n",
      "  Removed: 11,161 records (3.72%)\n",
      "\n",
      "  âš ï¸  Note: Records with PM2.5 = null are REMOVED (target variable must have real values)\n",
      "\n",
      "âš ï¸  Missing values after outlier removal:\n",
      "  PM2_5     :        0 (  0.00%) âœ… (Target - must be 0%)\n",
      "  PM2_5     :        0 (  0.00%) âœ… (Target - must be 0%)\n",
      "  PM10      :      296 (  0.10%)\n",
      "  PM10      :      296 (  0.10%)\n",
      "  NO2       :    7,369 (  2.55%)\n",
      "  NO2       :    7,369 (  2.55%)\n",
      "  SO2       :    7,186 (  2.48%)\n",
      "  SO2       :    7,186 (  2.48%)\n"
     ]
    }
   ],
   "source": [
    "# Loáº¡i bá» outliers theo WHO/EPA International Standards (cho dá»¯ liá»‡u Hong Kong)\n",
    "# âš ï¸  QUAN TRá»ŒNG: PM2.5 lÃ  TARGET variable - PHáº¢I cÃ³ giÃ¡ trá»‹ tháº­t!\n",
    "#     â†’ Records cÃ³ PM2.5 = null sáº½ Bá»Š LOáº I Bá»\n",
    "#     â†’ Chá»‰ cÃ¡c features khÃ¡c (PM10, NO2, SO2) má»›i Ä‘Æ°á»£c phÃ©p null vÃ  impute sau\n",
    "\n",
    "df_no_outliers = df_combined.filter(\n",
    "    # ğŸ¯ TARGET: PM2.5 theo WHO Emergency threshold (khÃ´ng cho phÃ©p null)\n",
    "    (F.col(\"PM2_5\").isNotNull()) & \n",
    "    (F.col(\"PM2_5\") >= 0) & (F.col(\"PM2_5\") < 250) &  # WHO Emergency: 250 Î¼g/mÂ³\n",
    "    \n",
    "    # ğŸ“Š FEATURES: WHO/EPA International Standards - Cho phÃ©p null, chá»‰ loáº¡i outliers\n",
    "    ((F.col(\"PM10\").isNull()) | ((F.col(\"PM10\") >= 0) & (F.col(\"PM10\") < 430))) &  # WHO Emergency: 430 Î¼g/mÂ³\n",
    "    ((F.col(\"NO2\").isNull()) | ((F.col(\"NO2\") >= 0) & (F.col(\"NO2\") < 400))) &     # WHO/EU: 400 Î¼g/mÂ³ (1-hour)\n",
    "    ((F.col(\"SO2\").isNull()) | ((F.col(\"SO2\") >= 0) & (F.col(\"SO2\") < 500))) &     # WHO/EU: 500 Î¼g/mÂ³ (10-min)\n",
    "    \n",
    "    # ğŸŒ¤ï¸ WEATHER: WMO standards cho Hong Kong\n",
    "    (F.col(\"precipitation\") >= 0) & (F.col(\"precipitation\") < 100)  # WMO: 100mm/h extreme rain\n",
    ")\n",
    "\n",
    "records_before = df_combined.count()\n",
    "records_after = df_no_outliers.count()\n",
    "removed = records_before - records_after\n",
    "\n",
    "print(f\"ğŸ“Š Outlier Removal:\")\n",
    "print(f\"  Before: {records_before:,} records\")\n",
    "print(f\"  After:  {records_after:,} records\")\n",
    "print(f\"  Removed: {removed:,} records ({removed/records_before*100:.2f}%)\")\n",
    "print(f\"\\n  âš ï¸  Note: Records with PM2.5 = null are REMOVED (target variable must have real values)\")\n",
    "\n",
    "# Kiá»ƒm tra missing values sau khi loáº¡i outliers\n",
    "print(\"\\nâš ï¸  Missing values after outlier removal:\")\n",
    "for col_name in [\"PM2_5\", \"PM10\", \"NO2\", \"SO2\"]:\n",
    "    if col_name in df_no_outliers.columns:\n",
    "        null_count = df_no_outliers.filter(F.col(col_name).isNull()).count()\n",
    "        total = df_no_outliers.count()\n",
    "        pct = (null_count / total) * 100\n",
    "        if null_count > 0:\n",
    "            print(f\"  {col_name:10s}: {null_count:8,} ({pct:6.2f}%)\")\n",
    "        elif col_name == \"PM2_5\":\n",
    "            print(f\"  {col_name:10s}: {null_count:8,} ({pct:6.2f}%) âœ… (Target - must be 0%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0994f2",
   "metadata": {},
   "source": [
    "### 4.2. Xá»­ lÃ½ Missing Values (Interpolation)\n",
    "\n",
    "**Chiáº¿n lÆ°á»£c Imputation cho Time Series:**\n",
    "- **PM2.5**: ÄÃ£ loáº¡i bá» táº¥t cáº£ records cÃ³ null (target variable)\n",
    "- **PM10, NO2, SO2**: Sá»­ dá»¥ng **Linear Interpolation** (tá»‘t nháº¥t cho time series)\n",
    "  - BÆ°á»›c 1: **Linear Interpolation** - Ná»™i suy tuyáº¿n tÃ­nh dá»±a trÃªn giÃ¡ trá»‹ trÆ°á»›c & sau\n",
    "  - BÆ°á»›c 2: **Forward Fill** - Xá»­ lÃ½ missing á»Ÿ cuá»‘i chuá»—i (khÃ´ng cÃ³ giÃ¡ trá»‹ sau)\n",
    "  - BÆ°á»›c 3: **Backward Fill** - Xá»­ lÃ½ missing á»Ÿ Ä‘áº§u chuá»—i (khÃ´ng cÃ³ giÃ¡ trá»‹ trÆ°á»›c)\n",
    "  - BÆ°á»›c 4: **Mean** - Backup cuá»‘i cÃ¹ng (náº¿u cÃ²n missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de82f80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Time Series Imputation Strategy (PySpark Native):\n",
      "   1. True Linear Interpolation - y = yâ‚ + (yâ‚‚-yâ‚) Ã— (t-tâ‚)/(tâ‚‚-tâ‚)\n",
      "   2. Forward Fill - If only prev value available\n",
      "   3. Backward Fill - If only next value available\n",
      "   4. Null - If no surrounding values (rare)\n",
      "\n",
      "   Columns to impute: ['PM10', 'NO2', 'SO2']\n",
      "   PM2.5 NOT imputed (target variable - already removed nulls)\n",
      "   ğŸ”’ Safe: Window partitioned by location_id (no cross-location interpolation)\n",
      "\n",
      "âš ï¸  Missing values BEFORE interpolation:\n",
      "  PM10      :      296 (  0.10%)\n",
      "  PM10      :      296 (  0.10%)\n",
      "  NO2       :    7,369 (  2.55%)\n",
      "  NO2       :    7,369 (  2.55%)\n",
      "  SO2       :    7,186 (  2.48%)\n",
      "  SO2       :    7,186 (  2.48%)\n"
     ]
    }
   ],
   "source": [
    "# Chiáº¿n lÆ°á»£c Imputation cho Time Series Data\n",
    "# Sá»­ dá»¥ng PySpark Window Functions - Ná»™i suy tuyáº¿n tÃ­nh dá»±a trÃªn khoáº£ng cÃ¡ch thá»i gian\n",
    "\n",
    "# List cÃ¡c cá»™t FEATURES cáº§n impute (KHÃ”NG bao gá»“m PM2.5 - target variable)\n",
    "pollutant_cols = [\"PM10\", \"NO2\", \"SO2\"]  # âš ï¸ KhÃ´ng cÃ³ PM2.5!\n",
    "\n",
    "print(f\"ğŸ”„ Time Series Imputation Strategy (PySpark Native):\")\n",
    "print(f\"   1. True Linear Interpolation - y = yâ‚ + (yâ‚‚-yâ‚) Ã— (t-tâ‚)/(tâ‚‚-tâ‚)\")\n",
    "print(f\"   2. Forward Fill - If only prev value available\")\n",
    "print(f\"   3. Backward Fill - If only next value available\")\n",
    "print(f\"   4. Null - If no surrounding values (rare)\")\n",
    "print(f\"\\n   Columns to impute: {pollutant_cols}\")\n",
    "print(f\"   PM2.5 NOT imputed (target variable - already removed nulls)\")\n",
    "print(f\"   ğŸ”’ Safe: Window partitioned by location_id (no cross-location interpolation)\\n\")\n",
    "\n",
    "# Cache Ä‘á»ƒ tÄƒng performance\n",
    "df_filled = df_no_outliers.cache()\n",
    "\n",
    "# Kiá»ƒm tra missing TRÆ¯á»šC khi interpolate\n",
    "print(\"âš ï¸  Missing values BEFORE interpolation:\")\n",
    "for col_name in pollutant_cols:\n",
    "    if col_name in df_filled.columns:\n",
    "        null_count = df_filled.filter(F.col(col_name).isNull()).count()\n",
    "        total = df_filled.count()\n",
    "        pct = (null_count / total) * 100\n",
    "        if null_count > 0:\n",
    "            print(f\"  {col_name:10s}: {null_count:8,} ({pct:6.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a622c600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Applying true linear interpolation per location (PySpark native)...\n",
      "  â–¶ Interpolating PM10... âœ“\n",
      "  â–¶ Interpolating NO2... âœ“\n",
      "  â–¶ Interpolating NO2... âœ“\n",
      "  â–¶ Interpolating SO2... âœ“\n",
      "  â–¶ Interpolating SO2... âœ“\n",
      "âœ“\n",
      "\n",
      "âœ… Linear interpolation completed! Total records: 289,255\n",
      "   âš™ï¸  Method: True linear interpolation based on time distance (epoch)\n",
      "   ğŸ”’ Safe: No cross-location interpolation (partitioned by location_id)\n",
      "   ğŸš€ Optimized: Native PySpark (no Pandas conversion)\n",
      "\n",
      "âœ… Linear interpolation completed! Total records: 289,255\n",
      "   âš™ï¸  Method: True linear interpolation based on time distance (epoch)\n",
      "   ğŸ”’ Safe: No cross-location interpolation (partitioned by location_id)\n",
      "   ğŸš€ Optimized: Native PySpark (no Pandas conversion)\n"
     ]
    }
   ],
   "source": [
    "# Ãp dá»¥ng True Linear Interpolation vá»›i PySpark (khÃ´ng dÃ¹ng Pandas)\n",
    "# Ná»™i suy tuyáº¿n tÃ­nh dá»±a trÃªn khoáº£ng cÃ¡ch thá»i gian THá»°C (epoch)\n",
    "# Window function Ä‘áº£m báº£o KHÃ”NG ná»™i suy chÃ©o giá»¯a cÃ¡c locations\n",
    "\n",
    "print(\"ğŸ”„ Applying true linear interpolation per location (PySpark native)...\")\n",
    "\n",
    "# Táº¡o cá»™t epoch (timestamp dáº¡ng sá»‘) Ä‘á»ƒ tÃ­nh toÃ¡n khoáº£ng cÃ¡ch thá»i gian\n",
    "df_filled = df_filled.withColumn(\"epoch\", F.col(\"datetime\").cast(\"long\"))\n",
    "\n",
    "# Äá»‹nh nghÄ©a Window cho tá»«ng location\n",
    "w_forward = (\n",
    "    Window.partitionBy(\"location_id\")\n",
    "    .orderBy(\"epoch\")\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    ")\n",
    "\n",
    "w_backward = (\n",
    "    Window.partitionBy(\"location_id\")\n",
    "    .orderBy(\"epoch\")\n",
    "    .rowsBetween(Window.currentRow, Window.unboundedFollowing)\n",
    ")\n",
    "\n",
    "# Xá»­ lÃ½ tá»«ng pollutant column\n",
    "for col_name in pollutant_cols:\n",
    "    if col_name not in df_filled.columns:\n",
    "        continue\n",
    "    \n",
    "    print(f\"  â–¶ Interpolating {col_name}...\", end=\" \", flush=True)\n",
    "    \n",
    "    # BÆ°á»›c 1: TÃ¬m giÃ¡ trá»‹ & timestamp TRÆ¯á»šC vÃ  SAU gáº§n nháº¥t (cÃ³ giÃ¡ trá»‹ non-null)\n",
    "    df_filled = (\n",
    "        df_filled\n",
    "        .withColumn(f\"{col_name}_prev_value\", F.last(col_name, True).over(w_forward))\n",
    "        .withColumn(f\"{col_name}_next_value\", F.first(col_name, True).over(w_backward))\n",
    "        .withColumn(f\"{col_name}_prev_time\", F.last(F.when(F.col(col_name).isNotNull(), F.col(\"epoch\")), True).over(w_forward))\n",
    "        .withColumn(f\"{col_name}_next_time\", F.first(F.when(F.col(col_name).isNotNull(), F.col(\"epoch\")), True).over(w_backward))\n",
    "    )\n",
    "    \n",
    "    # BÆ°á»›c 2: TÃ­nh toÃ¡n Linear Interpolation theo cÃ´ng thá»©c:\n",
    "    # y = yâ‚ + (yâ‚‚ - yâ‚) * (t - tâ‚) / (tâ‚‚ - tâ‚)\n",
    "    interpolated_value = (\n",
    "        F.col(f\"{col_name}_prev_value\") +\n",
    "        (F.col(f\"{col_name}_next_value\") - F.col(f\"{col_name}_prev_value\")) *\n",
    "        ((F.col(\"epoch\") - F.col(f\"{col_name}_prev_time\")) /\n",
    "         (F.col(f\"{col_name}_next_time\") - F.col(f\"{col_name}_prev_time\")))\n",
    "    )\n",
    "    \n",
    "    # BÆ°á»›c 3: Logic chá»n giÃ¡ trá»‹ cuá»‘i cÃ¹ng vá»›i fallback\n",
    "    df_filled = df_filled.withColumn(\n",
    "        col_name,\n",
    "        F.when(F.col(col_name).isNotNull(), F.col(col_name))  # Giá»¯ nguyÃªn náº¿u cÃ³ giÃ¡ trá»‹\n",
    "         .when(\n",
    "             # Linear interpolation náº¿u cÃ³ cáº£ prev & next vÃ  khÃ´ng chia 0\n",
    "             (F.col(f\"{col_name}_prev_value\").isNotNull()) &\n",
    "             (F.col(f\"{col_name}_next_value\").isNotNull()) &\n",
    "             ((F.col(f\"{col_name}_next_time\") - F.col(f\"{col_name}_prev_time\")) != 0),\n",
    "             interpolated_value\n",
    "         )\n",
    "         .when(F.col(f\"{col_name}_prev_value\").isNotNull(), F.col(f\"{col_name}_prev_value\"))  # Forward fill\n",
    "         .when(F.col(f\"{col_name}_next_value\").isNotNull(), F.col(f\"{col_name}_next_value\"))  # Backward fill\n",
    "         .otherwise(None)  # Váº«n null náº¿u khÃ´ng cÃ³ data nÃ o\n",
    "    )\n",
    "    \n",
    "    # BÆ°á»›c 4: XÃ³a cÃ¡c cá»™t phá»¥ Ä‘á»ƒ giáº£m memory\n",
    "    df_filled = df_filled.drop(\n",
    "        f\"{col_name}_prev_value\", f\"{col_name}_next_value\",\n",
    "        f\"{col_name}_prev_time\", f\"{col_name}_next_time\"\n",
    "    )\n",
    "    \n",
    "    print(\"âœ“\")\n",
    "\n",
    "# Cache káº¿t quáº£ sau khi interpolation\n",
    "df_filled = df_filled.cache()\n",
    "\n",
    "# Trigger computation vÃ  Ä‘áº¿m records\n",
    "count = df_filled.count()\n",
    "print(f\"\\nâœ… Linear interpolation completed! Total records: {count:,}\")\n",
    "print(f\"   âš™ï¸  Method: True linear interpolation based on time distance (epoch)\")\n",
    "print(f\"   ğŸ”’ Safe: No cross-location interpolation (partitioned by location_id)\")\n",
    "print(f\"   ğŸš€ Optimized: Native PySpark (no Pandas conversion)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9001c1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‹ Final Missing Values Check (After Interpolation):\n",
      "  PM2_5 (Target): 0 nulls âœ… (Must be 0)\n",
      "  PM2_5 (Target): 0 nulls âœ… (Must be 0)\n",
      "  PM10      : 0 nulls âœ…\n",
      "  PM10      : 0 nulls âœ…\n",
      "  NO2       : 0 nulls âœ…\n",
      "  NO2       : 0 nulls âœ…\n",
      "  SO2       : 0 nulls âœ…\n",
      "\n",
      "  âœ… No missing values in any feature columns!\n",
      "\n",
      "ğŸ“Š Final Verification:\n",
      "  SO2       : 0 nulls âœ…\n",
      "\n",
      "  âœ… No missing values in any feature columns!\n",
      "\n",
      "ğŸ“Š Final Verification:\n",
      "  PM2_5     : 0 nulls âœ…\n",
      "  PM2_5     : 0 nulls âœ…\n",
      "  PM10      : 0 nulls âœ…\n",
      "  PM10      : 0 nulls âœ…\n",
      "  NO2       : 0 nulls âœ…\n",
      "  NO2       : 0 nulls âœ…\n",
      "  SO2       : 0 nulls âœ…\n",
      "\n",
      "âœ… Data cleaning completed with True Linear Interpolation!\n",
      "  SO2       : 0 nulls âœ…\n",
      "\n",
      "âœ… Data cleaning completed with True Linear Interpolation!\n",
      "   Final dataset: 289,255 records\n",
      "   âš ï¸  All records have REAL PM2.5 values (target variable)\n",
      "   âœ… Features interpolated smoothly (time-based linear interpolation)\n",
      "   âœ… Edge cases (no surrounding data) removed\n",
      "   ğŸš€ Performance: Native PySpark (no Pandas conversion)\n",
      "   Final dataset: 289,255 records\n",
      "   âš ï¸  All records have REAL PM2.5 values (target variable)\n",
      "   âœ… Features interpolated smoothly (time-based linear interpolation)\n",
      "   âœ… Edge cases (no surrounding data) removed\n",
      "   ğŸš€ Performance: Native PySpark (no Pandas conversion)\n"
     ]
    }
   ],
   "source": [
    "# Verify: PM2.5 khÃ´ng cÃ³ null, cÃ¡c features khÃ¡c khÃ´ng cÃ³ null\n",
    "print(\"\\nğŸ“‹ Final Missing Values Check (After Interpolation):\")\n",
    "\n",
    "# Kiá»ƒm tra PM2.5 (target)\n",
    "pm25_nulls = df_filled.filter(F.col(\"PM2_5\").isNull()).count()\n",
    "print(f\"  PM2_5 (Target): {pm25_nulls:,} nulls âœ… (Must be 0)\")\n",
    "\n",
    "# Kiá»ƒm tra features\n",
    "total_nulls = 0\n",
    "for col_name in pollutant_cols:\n",
    "    if col_name in df_filled.columns:\n",
    "        null_count = df_filled.filter(F.col(col_name).isNull()).count()\n",
    "        total_nulls += null_count\n",
    "        if null_count > 0:\n",
    "            print(f\"  {col_name:10s}: {null_count:,} nulls âš ï¸\")\n",
    "        else:\n",
    "            print(f\"  {col_name:10s}: {null_count:,} nulls âœ…\")\n",
    "\n",
    "# Xá»­ lÃ½ edge case: Drop records cÃ²n null (khÃ´ng cÃ³ giÃ¡ trá»‹ xung quanh Ä‘á»ƒ interpolate)\n",
    "if total_nulls > 0:\n",
    "    print(f\"\\nâš ï¸  Found {total_nulls} remaining nulls (edge cases with no surrounding data)\")\n",
    "    print(f\"   â†’ Dropping these records to ensure data quality...\")\n",
    "    \n",
    "    records_before_drop = df_filled.count()\n",
    "    \n",
    "    # Drop records cÃ³ báº¥t ká»³ feature nÃ o cÃ²n null\n",
    "    for col_name in pollutant_cols:\n",
    "        df_filled = df_filled.filter(F.col(col_name).isNotNull())\n",
    "    \n",
    "    records_after_drop = df_filled.count()\n",
    "    dropped = records_before_drop - records_after_drop\n",
    "    \n",
    "    print(f\"   Before drop: {records_before_drop:,} records\")\n",
    "    print(f\"   After drop:  {records_after_drop:,} records\")\n",
    "    print(f\"   Dropped:     {dropped:,} records ({dropped/records_before_drop*100:.2f}%)\")\n",
    "    print(f\"\\n   âœ… All feature columns now have 0 nulls!\")\n",
    "else:\n",
    "    print(\"\\n  âœ… No missing values in any feature columns!\")\n",
    "\n",
    "# XÃ³a cá»™t epoch (Ä‘Ã£ dÃ¹ng xong)\n",
    "df_filled = df_filled.drop(\"epoch\")\n",
    "\n",
    "# Verify láº§n cuá»‘i\n",
    "print(f\"\\nğŸ“Š Final Verification:\")\n",
    "for col_name in [\"PM2_5\"] + pollutant_cols:\n",
    "    if col_name in df_filled.columns:\n",
    "        null_count = df_filled.filter(F.col(col_name).isNull()).count()\n",
    "        print(f\"  {col_name:10s}: {null_count:,} nulls âœ…\")\n",
    "\n",
    "print(f\"\\nâœ… Data cleaning completed with True Linear Interpolation!\")\n",
    "print(f\"   Final dataset: {df_filled.count():,} records\")\n",
    "print(f\"   âš ï¸  All records have REAL PM2.5 values (target variable)\")\n",
    "print(f\"   âœ… Features interpolated smoothly (time-based linear interpolation)\")\n",
    "print(f\"   âœ… Edge cases (no surrounding data) removed\")\n",
    "print(f\"   ğŸš€ Performance: Native PySpark (no Pandas conversion)\")\n",
    "\n",
    "# Cáº­p nháº­t df_combined vá»›i dá»¯ liá»‡u Ä‘Ã£ clean vÃ  sáº¯p xáº¿p\n",
    "df_combined = df_filled.orderBy(\"location_id\", \"datetime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7365dd0b",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering & Normalization\n",
    "\n",
    "**Quy trÃ¬nh ÄÃšNG Ä‘á»ƒ trÃ¡nh Data Leakage:**\n",
    "1. **Time Features** - ThÃªm cyclic encoding (sin/cos) vÃ  is_weekend (khÃ´ng cáº§n normalize)\n",
    "2. **Temporal Split** - Chia train/validation/test theo thá»i gian (70/15/15)\n",
    "3. **Normalization** - Chuáº©n hÃ³a **CHá»ˆ numerical Gá»C** báº±ng Min-Max tá»« train set\n",
    "4. **Lag Features** - Táº¡o lag Tá»ª CÃC Cá»˜T ÄÃƒ SCALE (giá»¯ Ä‘Ãºng scale relationship)\n",
    "5. **Model-Specific Datasets** - Chuáº©n bá»‹ riÃªng cho Deep Learning vÃ  XGBoost\n",
    "6. **Null Handling** - Xá»­ lÃ½ nulls trong lag features cuá»‘i cÃ¹ng\n",
    "\n",
    "**âš ï¸ QUAN TRá»ŒNG:** Lag features pháº£i táº¡o SAU khi normalize Ä‘á»ƒ giá»¯ Ä‘Ãºng má»‘i quan há»‡ scale!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03014c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Step 1: Adding Time Features (No normalization needed)...\n",
      "âœ“ Time features added successfully!\n",
      "âœ“ Time features added successfully!\n",
      "âœ“ Total records: 289,255\n",
      "âœ“ Total columns: 22\n",
      "\n",
      "ğŸ“‹ Time Features Created:\n",
      "  Cyclic (sin/cos): hour, month, day_of_week â†’ Already in [-1, 1]\n",
      "  Binary: is_weekend â†’ Already in [0, 1]\n",
      "  âœ… No normalization needed for time features!\n",
      "âœ“ Total records: 289,255\n",
      "âœ“ Total columns: 22\n",
      "\n",
      "ğŸ“‹ Time Features Created:\n",
      "  Cyclic (sin/cos): hour, month, day_of_week â†’ Already in [-1, 1]\n",
      "  Binary: is_weekend â†’ Already in [0, 1]\n",
      "  âœ… No normalization needed for time features!\n"
     ]
    }
   ],
   "source": [
    "# BÆ°á»›c 1: ThÃªm Time Features tá»« dá»¯ liá»‡u Ä‘Ã£ clean\n",
    "print(\"ğŸ”„ Step 1: Adding Time Features (No normalization needed)...\")\n",
    "\n",
    "import math\n",
    "\n",
    "df_features = df_combined \\\n",
    "    .withColumn(\"hour\", F.hour(\"datetime\")) \\\n",
    "    .withColumn(\"month\", F.month(\"datetime\")) \\\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(\"datetime\"))\n",
    "\n",
    "# Cyclic encoding cho hour (24h cycle)\n",
    "df_features = df_features \\\n",
    "    .withColumn(\"hour_sin\", F.sin(2 * math.pi * F.col(\"hour\") / 24)) \\\n",
    "    .withColumn(\"hour_cos\", F.cos(2 * math.pi * F.col(\"hour\") / 24))\n",
    "\n",
    "# Cyclic encoding cho month (12 month cycle)\n",
    "df_features = df_features \\\n",
    "    .withColumn(\"month_sin\", F.sin(2 * math.pi * F.col(\"month\") / 12)) \\\n",
    "    .withColumn(\"month_cos\", F.cos(2 * math.pi * F.col(\"month\") / 12))\n",
    "\n",
    "# Cyclic encoding cho day_of_week (7 day cycle)\n",
    "df_features = df_features \\\n",
    "    .withColumn(\"day_of_week_sin\", F.sin(2 * math.pi * F.col(\"day_of_week\") / 7)) \\\n",
    "    .withColumn(\"day_of_week_cos\", F.cos(2 * math.pi * F.col(\"day_of_week\") / 7))\n",
    "\n",
    "# Binary feature: is_weekend\n",
    "df_features = df_features \\\n",
    "    .withColumn(\"is_weekend\", F.when(F.col(\"day_of_week\").isin([1, 7]), 1).otherwise(0))\n",
    "\n",
    "# XÃ³a cÃ¡c cá»™t trung gian\n",
    "df_features = df_features.drop(\"hour\", \"month\", \"day_of_week\")\n",
    "\n",
    "print(\"âœ“ Time features added successfully!\")\n",
    "print(f\"âœ“ Total records: {df_features.count():,}\")\n",
    "print(f\"âœ“ Total columns: {len(df_features.columns)}\")\n",
    "\n",
    "print(\"\\nğŸ“‹ Time Features Created:\")\n",
    "print(\"  Cyclic (sin/cos): hour, month, day_of_week â†’ Already in [-1, 1]\")\n",
    "print(\"  Binary: is_weekend â†’ Already in [0, 1]\")\n",
    "print(\"  âœ… No normalization needed for time features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d159476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ Step 2: Temporal Train/Val/Test Split BEFORE Normalization...\n",
      "ğŸ“… Temporal Split (Avoiding Data Leakage):\n",
      "  ğŸŸ¢ Train:      2022-11-01 â†’ 2024-11-14 (744 days)\n",
      "  ğŸŸ¡ Validation: 2024-11-14 â†’ 2025-04-22 (159 days)\n",
      "  ğŸ”´ Test:       2025-04-22 â†’ 2025-09-30 (161 days)\n",
      "ğŸ“… Temporal Split (Avoiding Data Leakage):\n",
      "  ğŸŸ¢ Train:      2022-11-01 â†’ 2024-11-14 (744 days)\n",
      "  ğŸŸ¡ Validation: 2024-11-14 â†’ 2025-04-22 (159 days)\n",
      "  ğŸ”´ Test:       2025-04-22 â†’ 2025-09-30 (161 days)\n",
      "\n",
      "ğŸ“Š Split Results:\n",
      "  ğŸŸ¢ Train:  187,636 (64.9%)\n",
      "  ğŸŸ¡ Val:     49,925 (17.3%)\n",
      "  ğŸ”´ Test:    51,694 (17.9%)\n",
      "\n",
      "âœ… Temporal split completed!\n",
      "   âš ï¸  Next: Normalize using TRAIN SET statistics ONLY\n",
      "\n",
      "ğŸ“Š Split Results:\n",
      "  ğŸŸ¢ Train:  187,636 (64.9%)\n",
      "  ğŸŸ¡ Val:     49,925 (17.3%)\n",
      "  ğŸ”´ Test:    51,694 (17.9%)\n",
      "\n",
      "âœ… Temporal split completed!\n",
      "   âš ï¸  Next: Normalize using TRAIN SET statistics ONLY\n"
     ]
    }
   ],
   "source": [
    "# BÆ°á»›c 2: TEMPORAL SPLIT TRÆ¯á»šC KHI NORMALIZE (TrÃ¡nh Data Leakage)\n",
    "print(\"\\nğŸ”„ Step 2: Temporal Train/Val/Test Split BEFORE Normalization...\")\n",
    "\n",
    "# TÃ­nh toÃ¡n ngÃ y chia dá»±a trÃªn percentile thá»i gian  \n",
    "time_stats = df_features.select(\n",
    "    F.min(\"datetime\").alias(\"min_time\"),\n",
    "    F.max(\"datetime\").alias(\"max_time\")\n",
    ").collect()[0]\n",
    "\n",
    "min_time = time_stats[\"min_time\"]\n",
    "max_time = time_stats[\"max_time\"]\n",
    "total_days = (max_time - min_time).days\n",
    "\n",
    "# 70% train, 15% validation, 15% test\n",
    "train_days = int(total_days * 0.70)\n",
    "val_days = int(total_days * 0.15)\n",
    "\n",
    "train_end = min_time + pd.Timedelta(days=train_days)\n",
    "val_end = train_end + pd.Timedelta(days=val_days)\n",
    "\n",
    "print(f\"ğŸ“… Temporal Split (Avoiding Data Leakage):\")\n",
    "print(f\"  ğŸŸ¢ Train:      {min_time.strftime('%Y-%m-%d')} â†’ {train_end.strftime('%Y-%m-%d')} ({(train_end - min_time).days} days)\")\n",
    "print(f\"  ğŸŸ¡ Validation: {train_end.strftime('%Y-%m-%d')} â†’ {val_end.strftime('%Y-%m-%d')} ({(val_end - train_end).days} days)\")\n",
    "print(f\"  ğŸ”´ Test:       {val_end.strftime('%Y-%m-%d')} â†’ {max_time.strftime('%Y-%m-%d')} ({(max_time - val_end).days} days)\")\n",
    "\n",
    "# Split data - Count BEFORE caching to trigger evaluation\n",
    "df_train_raw = df_features.filter(F.col(\"datetime\") < train_end)\n",
    "df_val_raw = df_features.filter((F.col(\"datetime\") >= train_end) & (F.col(\"datetime\") < val_end))\n",
    "df_test_raw = df_features.filter(F.col(\"datetime\") >= val_end)\n",
    "\n",
    "train_count = df_train_raw.count()\n",
    "val_count = df_val_raw.count() \n",
    "test_count = df_test_raw.count()\n",
    "total_count = train_count + val_count + test_count\n",
    "\n",
    "print(f\"\\nğŸ“Š Split Results:\")\n",
    "print(f\"  ğŸŸ¢ Train: {train_count:8,} ({train_count/total_count*100:.1f}%)\")\n",
    "print(f\"  ğŸŸ¡ Val:   {val_count:8,} ({val_count/total_count*100:.1f}%)\")\n",
    "print(f\"  ğŸ”´ Test:  {test_count:8,} ({test_count/total_count*100:.1f}%)\")\n",
    "\n",
    "# Now cache after counting (avoids double computation)\n",
    "df_train_raw = df_train_raw.cache()\n",
    "df_val_raw = df_val_raw.cache()\n",
    "df_test_raw = df_test_raw.cache()\n",
    "\n",
    "print(f\"\\nâœ… Temporal split completed!\")\n",
    "print(f\"   âš ï¸  Next: Normalize using TRAIN SET statistics ONLY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816ecee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BÆ°á»›c 3: Normalize NUMERICAL Gá»C (CHá»ˆ gá»‘c, KHÃ”NG cÃ³ lag features)\n",
    "print(f\"\\nğŸ”„ Step 3: Normalize NUMERICAL BASE FEATURES using TRAIN SET ONLY...\")\n",
    "\n",
    "# âš ï¸ QUAN TRá»ŒNG: CHá»ˆ normalize cÃ¡c cá»™t Gá»C, KHÃ”NG bao gá»“m lag features\n",
    "# Lag features sáº½ táº¡o SAU tá»« cÃ¡c cá»™t Ä‘Ã£ scale\n",
    "numerical_base_cols = [\n",
    "    # Pollutants (current values only)\n",
    "    \"PM2_5\", \"PM10\", \"NO2\", \"SO2\",\n",
    "    # Weather features (current values only)\n",
    "    \"temperature_2m\", \"relative_humidity_2m\", \"wind_speed_10m\", \n",
    "    \"wind_direction_10m\", \"precipitation\"\n",
    "]\n",
    "\n",
    "print(f\"ğŸ“Š Normalizing {len(numerical_base_cols)} BASE features (NO lag features yet)...\")\n",
    "print(f\"   Features to normalize: {numerical_base_cols}\")\n",
    "print(f\"   âš ï¸  Computing min/max from TRAIN SET ONLY (preventing data leakage)\")\n",
    "\n",
    "# TÃ­nh min/max CHá»ˆ Tá»ª TRAIN SET\n",
    "scaler_params = {}\n",
    "\n",
    "for col_name in numerical_base_cols:\n",
    "    if col_name in df_train_raw.columns:\n",
    "        # CHá»ˆ DÃ™NG TRAIN SET Äá»‚ TÃNH MIN/MAX  \n",
    "        stats = df_train_raw.select(\n",
    "            F.min(col_name).alias(\"min\"),\n",
    "            F.max(col_name).alias(\"max\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        min_val = stats[\"min\"]\n",
    "        max_val = stats[\"max\"]\n",
    "        \n",
    "        # âš ï¸ CRITICAL: Handle None values from null columns\n",
    "        if min_val is None or max_val is None:\n",
    "            print(f\"  âš ï¸  Skipping {col_name}: All values are null\")\n",
    "            continue\n",
    "        \n",
    "        # âš ï¸ CRITICAL: TrÃ¡nh chia 0 khi min = max\n",
    "        if max_val == min_val:\n",
    "            max_val = min_val + 1\n",
    "        \n",
    "        scaler_params[col_name] = {\"min\": min_val, \"max\": max_val}\n",
    "        print(f\"  âœ“ {col_name:30s}: [{min_val:8.2f}, {max_val:8.2f}] â†’ [0, 1]\")\n",
    "\n",
    "print(f\"\\nâœ… Scaler parameters computed from TRAIN SET only!\")\n",
    "\n",
    "# Ãp dá»¥ng normalization cho táº¥t cáº£ splits\n",
    "def apply_scaling(df, scaler_params):\n",
    "    \"\"\"Apply Min-Max scaling using precomputed parameters\"\"\"\n",
    "    df_scaled = df\n",
    "    for col_name, params in scaler_params.items():\n",
    "        if col_name in df.columns:\n",
    "            min_val = params[\"min\"]\n",
    "            max_val = params[\"max\"]\n",
    "            df_scaled = df_scaled.withColumn(\n",
    "                f\"{col_name}_scaled\",\n",
    "                (F.col(col_name) - min_val) / (max_val - min_val)\n",
    "            )\n",
    "    return df_scaled\n",
    "\n",
    "print(f\"\\n\udd04 Applying Min-Max scaling [0, 1] to all splits...\")\n",
    "\n",
    "# Apply scaling and trigger computation\n",
    "df_train = apply_scaling(df_train_raw, scaler_params)\n",
    "df_val = apply_scaling(df_val_raw, scaler_params)\n",
    "df_test = apply_scaling(df_test_raw, scaler_params)\n",
    "\n",
    "# Trigger computation and cache\n",
    "_ = df_train.count()\n",
    "_ = df_val.count()\n",
    "_ = df_test.count()\n",
    "\n",
    "df_train = df_train.cache()\n",
    "df_val = df_val.cache()\n",
    "df_test = df_test.cache()\n",
    "\n",
    "# Unpersist raw versions to free memory\n",
    "df_train_raw.unpersist()\n",
    "df_val_raw.unpersist()\n",
    "df_test_raw.unpersist()\n",
    "\n",
    "print(f\"âœ… Base feature normalization completed!\")\n",
    "print(f\"   ğŸ“Š All splits normalized using train statistics only\")\n",
    "print(f\"   âš ï¸  Next: Create lag features FROM SCALED COLUMNS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1c6a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ Scaler parameters saved to: ..\\data\\processed\\scaler_params.json\n",
      "   (Computed from TRAIN SET only - no data leakage)\n",
      "   (DÃ¹ng Ä‘á»ƒ denormalize predictions khi inference)\n",
      "\n",
      "ğŸ“‹ Example scaler params (from train set):\n",
      "  PM2_5               : min=0.00, max=152.40\n",
      "  temperature_2m      : min=1.90, max=36.00\n",
      "  wind_speed_10m      : min=0.00, max=82.30\n"
     ]
    }
   ],
   "source": [
    "# BÆ°á»›c 4: LÆ°u Scaler Parameters\n",
    "print(f\"\\nğŸ’¾ Step 4: Saving Scaler Parameters...\")\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "scaler_json = {\n",
    "    col: {\"min\": float(params[\"min\"]), \"max\": float(params[\"max\"])} \n",
    "    for col, params in scaler_params.items()\n",
    "}\n",
    "\n",
    "# ========================================\n",
    "# ADAPTIVE PATH (Kaggle vs Colab vs Local)\n",
    "# ========================================\n",
    "if IN_KAGGLE:\n",
    "    # ğŸ† Kaggle: Write to /kaggle/working (auto-saved on commit)\n",
    "    processed_dir = Path(\"/kaggle/working/processed\")\n",
    "    print(f\"ğŸ† Kaggle mode: Saving to {processed_dir}\")\n",
    "    \n",
    "elif IN_COLAB:\n",
    "    # ğŸŒ Colab: Write to Google Drive\n",
    "    processed_dir = Path(\"/content/drive/MyDrive/pm25-data/processed\")\n",
    "    print(f\"ğŸŒ Colab mode: Saving to Google Drive\")\n",
    "    \n",
    "else:\n",
    "    # ğŸ’» Local: Write to project folder\n",
    "    processed_dir = Path(\"../data/processed\")\n",
    "    print(f\"ğŸ’» Local mode: Saving to {processed_dir}\")\n",
    "\n",
    "# Táº¡o thÆ° má»¥c vá»›i parents=True (táº¡o cáº£ parent directories náº¿u chÆ°a cÃ³)\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# LÆ°u ra file JSON\n",
    "scaler_path = processed_dir / \"scaler_params.json\"\n",
    "with open(scaler_path, 'w') as f:\n",
    "    json.dump(scaler_json, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Scaler parameters saved to: {scaler_path}\")\n",
    "print(f\"   - Computed from TRAIN SET only (no data leakage)\")\n",
    "print(f\"   - Used for denormalizing predictions during inference\")\n",
    "print(f\"   - Contains {len(scaler_params)} base features\")\n",
    "\n",
    "# Hiá»ƒn thá»‹ vÃ­ dá»¥\n",
    "print(f\"\\nğŸ“‹ Example scaler params (from train set):\")\n",
    "example_cols = [\"PM2_5\", \"temperature_2m\", \"wind_speed_10m\"]\n",
    "for col in example_cols:\n",
    "    if col in scaler_params:\n",
    "        params = scaler_params[col]\n",
    "        print(f\"  {col:20s}: min={params['min']:.2f}, max={params['max']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e754fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BÆ°á»›c 5: Táº¡o Lag Features Tá»ª CÃC Cá»˜T ÄÃƒ SCALE (CHá»ˆ CHO XGBOOST)\n",
    "print(f\"\\nğŸ”„ Step 5: Creating Lag Features FROM SCALED COLUMNS (XGBoost only)...\")\n",
    "\n",
    "# âš ï¸ QUAN TRá»ŒNG: Lag features Ä‘Æ°á»£c táº¡o Tá»ª CÃC Cá»˜T ÄÃƒ SCALE\n",
    "# â†’ Äáº£m báº£o lag vÃ  gá»‘c cÃ³ CÃ™NG SCALE PARAMETERS\n",
    "# â†’ Giá»¯ Ä‘Ãºng má»‘i quan há»‡ giá»¯a giÃ¡ trá»‹ hiá»‡n táº¡i vÃ  quÃ¡ khá»©\n",
    "\n",
    "LAG_STEPS = [1, 2, 3, 6, 12, 24]  # 1h, 2h, 3h, 6h, 12h, 24h trÆ°á»›c\n",
    "\n",
    "# Columns cáº§n táº¡o lag (sá»­ dá»¥ng báº£n SCALED)\n",
    "lag_base_columns = [\"PM2_5\", \"PM10\", \"NO2\", \"SO2\", \n",
    "                    \"temperature_2m\", \"relative_humidity_2m\", \n",
    "                    \"wind_speed_10m\", \"precipitation\"]\n",
    "\n",
    "print(f\"\\nğŸ“‹ Creating lag features:\")\n",
    "print(f\"   Deep Learning models: No lags needed (learn from sequences)\")\n",
    "print(f\"   XGBoost: {len(LAG_STEPS)} lags Ã— {len(lag_base_columns)} variables = {len(LAG_STEPS) * len(lag_base_columns)} features\")\n",
    "print(f\"   âœ… Using SCALED columns as source (proper scale relationship)\")\n",
    "\n",
    "# Window cho tá»«ng location (sáº¯p xáº¿p theo thá»i gian)\n",
    "w_lag = Window.partitionBy(\"location_id\").orderBy(\"datetime\")\n",
    "\n",
    "# Táº¡o lag features cho tá»«ng split (train, val, test)\n",
    "def create_lag_features(df, lag_base_columns, lag_steps):\n",
    "    \"\"\"Create lag features from SCALED columns\"\"\"\n",
    "    df_with_lags = df\n",
    "    \n",
    "    for col_name in lag_base_columns:\n",
    "        col_scaled = f\"{col_name}_scaled\"\n",
    "        \n",
    "        if col_scaled in df.columns:\n",
    "            for lag in lag_steps:\n",
    "                lag_col_name = f\"{col_name}_lag{lag}_scaled\"\n",
    "                \n",
    "                # âœ… Táº¡o lag Tá»ª Cá»˜T ÄÃƒ SCALE\n",
    "                df_with_lags = df_with_lags.withColumn(\n",
    "                    lag_col_name,\n",
    "                    F.lag(col_scaled, lag).over(w_lag)\n",
    "                )\n",
    "    \n",
    "    return df_with_lags\n",
    "\n",
    "# Apply to all splits\n",
    "print(f\"\\nğŸ”„ Creating lag features for all splits...\")\n",
    "df_train = create_lag_features(df_train, lag_base_columns, LAG_STEPS)\n",
    "df_val = create_lag_features(df_val, lag_base_columns, LAG_STEPS)\n",
    "df_test = create_lag_features(df_test, lag_base_columns, LAG_STEPS)\n",
    "\n",
    "print(f\"  âœ“ Train: {len(LAG_STEPS) * len(lag_base_columns)} lag features created\")\n",
    "print(f\"  âœ“ Val:   {len(LAG_STEPS) * len(lag_base_columns)} lag features created\")\n",
    "print(f\"  âœ“ Test:  {len(LAG_STEPS) * len(lag_base_columns)} lag features created\")\n",
    "\n",
    "# Trigger computation and cache\n",
    "_ = df_train.count()\n",
    "_ = df_val.count()\n",
    "_ = df_test.count()\n",
    "\n",
    "df_train = df_train.cache()\n",
    "df_val = df_val.cache()\n",
    "df_test = df_test.cache()\n",
    "\n",
    "print(f\"\\nâœ… Lag features created successfully!\")\n",
    "print(f\"   âœ… All lags created FROM SCALED columns\")\n",
    "print(f\"   âœ… Lag and base features have SAME scale parameters\")\n",
    "print(f\"   âœ… Proper temporal relationship preserved\")\n",
    "\n",
    "# ========================================\n",
    "# Xá»¬ LÃ NULL VALUES TRONG LAG FEATURES\n",
    "# ========================================\n",
    "print(f\"\\nğŸ”„ Handling null values in lag features...\")\n",
    "\n",
    "# Táº¡o list táº¥t cáº£ lag feature names\n",
    "lag_feature_names = [f\"{col}_lag{lag}_scaled\" for col in lag_base_columns for lag in LAG_STEPS]\n",
    "\n",
    "# Äáº¿m nulls TRÆ¯á»šC khi xá»­ lÃ½\n",
    "print(f\"\\nğŸ“Š Null counts BEFORE handling:\")\n",
    "sample_lag_features = lag_feature_names[:3]\n",
    "for lag_col in sample_lag_features:\n",
    "    if lag_col in df_train.columns:\n",
    "        null_count = df_train.filter(F.col(lag_col).isNull()).count()\n",
    "        total_count = df_train.count()\n",
    "        print(f\"  {lag_col:35s}: {null_count:8,} nulls ({null_count/total_count*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nâš ï¸  Reason: First {max(LAG_STEPS)} hours of each location have no previous data\")\n",
    "print(f\"   Strategy: DROP records with ANY null lag feature\")\n",
    "\n",
    "# Track counts before drop\n",
    "train_before = df_train.count()\n",
    "val_before = df_val.count()\n",
    "test_before = df_test.count()\n",
    "\n",
    "# Function to drop nulls\n",
    "def drop_lag_nulls(df, lag_features):\n",
    "    \"\"\"Drop records with any null lag feature\"\"\"\n",
    "    df_clean = df\n",
    "    for col in lag_features:\n",
    "        if col in df.columns:\n",
    "            df_clean = df_clean.filter(F.col(col).isNotNull())\n",
    "    return df_clean\n",
    "\n",
    "# Apply to all splits\n",
    "print(f\"\\nğŸ—‘ï¸  Dropping records with null lag features...\")\n",
    "df_train_clean = drop_lag_nulls(df_train, lag_feature_names)\n",
    "df_val_clean = drop_lag_nulls(df_val, lag_feature_names)\n",
    "df_test_clean = drop_lag_nulls(df_test, lag_feature_names)\n",
    "\n",
    "# Count after\n",
    "train_after = df_train_clean.count()\n",
    "val_after = df_val_clean.count()\n",
    "test_after = df_test_clean.count()\n",
    "\n",
    "# Cache cleaned datasets\n",
    "df_train_clean = df_train_clean.cache()\n",
    "df_val_clean = df_val_clean.cache()\n",
    "df_test_clean = df_test_clean.cache()\n",
    "\n",
    "# Unpersist old ones\n",
    "df_train.unpersist()\n",
    "df_val.unpersist()\n",
    "df_test.unpersist()\n",
    "\n",
    "# Reassign\n",
    "df_train = df_train_clean\n",
    "df_val = df_val_clean\n",
    "df_test = df_test_clean\n",
    "\n",
    "print(f\"\\nğŸ“Š Records dropped (null lag features):\")\n",
    "print(f\"  ğŸŸ¢ Train: {train_before:,} â†’ {train_after:,} (dropped {train_before - train_after:,}, {(train_before - train_after)/train_before*100:.2f}%)\")\n",
    "print(f\"  ğŸŸ¡ Val:   {val_before:,} â†’ {val_after:,} (dropped {val_before - val_after:,}, {(val_before - val_after)/val_before*100:.2f}%)\")\n",
    "print(f\"  ğŸ”´ Test:  {test_before:,} â†’ {test_after:,} (dropped {test_before - test_after:,}, {(test_before - test_after)/test_before*100:.2f}%)\")\n",
    "\n",
    "# Verify no nulls\n",
    "print(f\"\\nâœ… Verification - checking for remaining nulls...\")\n",
    "sample_check = lag_feature_names[:3]\n",
    "total_nulls_after = 0\n",
    "for lag_col in sample_check:\n",
    "    if lag_col in df_train.columns:\n",
    "        null_count = df_train.filter(F.col(lag_col).isNull()).count()\n",
    "        total_nulls_after += null_count\n",
    "        status = \"âœ…\" if null_count == 0 else \"âŒ\"\n",
    "        print(f\"  {lag_col:35s}: {null_count:8,} nulls {status}\")\n",
    "\n",
    "if total_nulls_after == 0:\n",
    "    print(f\"\\nâœ… All lag features are clean!\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  Still {total_nulls_after} nulls found!\")\n",
    "\n",
    "print(f\"\\nâœ… Lag features + Null handling completed!\")\n",
    "print(f\"   - Created {len(lag_feature_names)} lag features FROM SCALED columns\")\n",
    "print(f\"   - Lost only first {max(LAG_STEPS)} hours per location\")\n",
    "print(f\"   - All lag features now have valid values\")\n",
    "print(f\"   - Data quality ensured for XGBoost training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45295509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ Step 5: Preparing Model-Specific Features...\n",
      "ğŸ§  DEEP LEARNING Features: 15 features\n",
      "   - Current pollutants: 3\n",
      "   - Weather: 5\n",
      "   - Time (cyclic): 6\n",
      "   - Time (linear): 1\n",
      "   - NO LAG FEATURES (models learn from sequences)\n",
      "\n",
      "ğŸ“Š XGBOOST Features: 63 features\n",
      "   - Deep Learning base features: 15\n",
      "   - Lag features: 48\n",
      "   - Total: 63 features\n",
      "\n",
      "âœ… Model-specific features prepared:\n",
      "  ğŸ§  CNN1D-BLSTM-Attention: 15 features\n",
      "  ğŸ§  LSTM: 15 features\n",
      "  ğŸ“Š XGBoost: 63 features\n",
      "  ğŸ¯ Target: PM2_5_scaled\n",
      "\n",
      "âœ… All feature columns exist in datasets!\n"
     ]
    }
   ],
   "source": [
    "# BÆ°á»›c 6: Chuáº©n bá»‹ Features cho tá»«ng Model\n",
    "print(\"\\nğŸ”„ Step 6: Preparing Model-Specific Features...\")\n",
    "\n",
    "# ========================================\n",
    "# FEATURES CHO DEEP LEARNING MODELS (CNN1D-BLSTM, LSTM)\n",
    "# ========================================\n",
    "# KhÃ´ng cáº§n lag features vÃ¬ models tá»± há»c temporal patterns tá»« sequences\n",
    "\n",
    "dl_input_features = []\n",
    "\n",
    "# 1. Pollutants scaled (trá»« PM2_5 - Ä‘Ã¢y lÃ  target)\n",
    "dl_input_features.extend([\"PM10_scaled\", \"NO2_scaled\", \"SO2_scaled\"])\n",
    "\n",
    "# 2. Weather features scaled (core features)\n",
    "dl_input_features.extend([\n",
    "    \"temperature_2m_scaled\", \"relative_humidity_2m_scaled\", \n",
    "    \"wind_speed_10m_scaled\", \"wind_direction_10m_scaled\", \"precipitation_scaled\"\n",
    "])\n",
    "\n",
    "# 3. Time features (cyclic encoding - Ä‘Ã£ á»Ÿ dáº¡ng sin/cos trong [-1, 1])\n",
    "dl_input_features.extend([\n",
    "    \"hour_sin\", \"hour_cos\", \n",
    "    \"month_sin\", \"month_cos\",\n",
    "    \"day_of_week_sin\", \"day_of_week_cos\"\n",
    "])\n",
    "\n",
    "# 4. Time features (binary)\n",
    "dl_input_features.extend([\"is_weekend\"])\n",
    "\n",
    "print(f\"ğŸ§  DEEP LEARNING Features: {len(dl_input_features)} features\")\n",
    "print(f\"   - Current pollutants (scaled): 3\")\n",
    "print(f\"   - Weather (scaled): 5\") \n",
    "print(f\"   - Time (cyclic): 6\")\n",
    "print(f\"   - Time (binary): 1\")\n",
    "print(f\"   - NO LAG FEATURES (models learn from sequences)\")\n",
    "\n",
    "# ========================================  \n",
    "# FEATURES CHO XGBOOST\n",
    "# ========================================\n",
    "# Cáº§n lag features vÃ¬ khÃ´ng cÃ³ kháº£ nÄƒng xá»­ lÃ½ sequences\n",
    "\n",
    "xgb_input_features = dl_input_features.copy()  # Start with DL features\n",
    "\n",
    "# ThÃªm lag features CHá»ˆ CHO XGBOOST (Ä‘Ã£ Ä‘Æ°á»£c táº¡o tá»« scaled columns)\n",
    "for col_name in lag_base_columns:\n",
    "    for lag in LAG_STEPS:\n",
    "        lag_col_name = f\"{col_name}_lag{lag}_scaled\"\n",
    "        xgb_input_features.append(lag_col_name)\n",
    "\n",
    "print(f\"\\nğŸ“Š XGBOOST Features: {len(xgb_input_features)} features\")\n",
    "print(f\"   - Deep Learning base features: {len(dl_input_features)}\")\n",
    "print(f\"   - Lag features (from scaled columns): {len(lag_base_columns) * len(LAG_STEPS)}\")\n",
    "print(f\"   - Total: {len(xgb_input_features)} features\")\n",
    "\n",
    "# Target variable (Ä‘Ã£ scaled)\n",
    "target_feature = \"PM2_5_scaled\"\n",
    "\n",
    "print(f\"\\nâœ… Model-specific features prepared:\")\n",
    "print(f\"  ğŸ§  CNN1D-BLSTM-Attention: {len(dl_input_features)} features\")\n",
    "print(f\"  ğŸ§  LSTM: {len(dl_input_features)} features\")  \n",
    "print(f\"  ğŸ“Š XGBoost: {len(xgb_input_features)} features\")\n",
    "print(f\"  ğŸ¯ Target: {target_feature}\")\n",
    "\n",
    "# âš ï¸ CRITICAL: Verify ALL columns exist\n",
    "missing_dl = [col for col in dl_input_features if col not in df_train.columns]\n",
    "missing_xgb = [col for col in xgb_input_features if col not in df_train.columns]\n",
    "missing_target = target_feature not in df_train.columns\n",
    "\n",
    "if missing_dl or missing_xgb or missing_target:\n",
    "    print(f\"\\nâŒ MISSING COLUMNS DETECTED:\")\n",
    "    if missing_dl: \n",
    "        print(f\"  DL models: {missing_dl}\")\n",
    "    if missing_xgb: \n",
    "        print(f\"  XGBoost: {missing_xgb[:5]}...\")  # Show first 5\n",
    "    if missing_target:\n",
    "        print(f\"  Target: {target_feature}\")\n",
    "    \n",
    "    print(f\"\\nâš ï¸  Available scaled columns:\")\n",
    "    scaled_cols = [c for c in df_train.columns if c.endswith('_scaled')]\n",
    "    print(f\"  {scaled_cols[:10]}...\")\n",
    "    \n",
    "    raise ValueError(\"Missing required feature columns! Check normalization step.\")\n",
    "else:\n",
    "    print(f\"\\nâœ… All feature columns exist in datasets!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700d4016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ“Š FEATURE ENGINEERING SUMMARY\n",
      "======================================================================\n",
      "\n",
      "1ï¸âƒ£ Dataset Statistics:\n",
      "   - Total records: 289,050\n",
      "   - Total records: 289,050\n",
      "   - Total locations: 14\n",
      "   - Time range: 2022-11-01 â†’ 2025-09-30\n",
      "\n",
      "2ï¸âƒ£ Feature Categories:\n",
      "   ğŸ“Œ Deep Learning Features: 15 features\n",
      "      - Current Pollutants (scaled): PM10, NO2, SO2\n",
      "      - Current Weather (scaled): temperature_2m, relative_humidity_2m, wind_speed_10m, wind_direction_10m, precipitation\n",
      "      - Time Features (cyclic): hour_sin/cos, month_sin/cos, day_of_week_sin/cos\n",
      "      - Time Features (linear): is_weekend\n",
      "   ğŸ“Œ XGBoost Features: 63 features\n",
      "      - Deep Learning features: 15\n",
      "      - Lag Features: 48 features\n",
      "      - Variables: PM2_5, PM10, NO2, SO2, temperature_2m, relative_humidity_2m, wind_speed_10m, precipitation\n",
      "      - Lag steps: [1, 2, 3, 6, 12, 24]\n",
      "\n",
      "3ï¸âƒ£ Target Variable:\n",
      "   ğŸ¯ PM2_5_scaled (normalized PM2.5)\n",
      "\n",
      "4ï¸âƒ£ Normalization:\n",
      "   âœ… All numerical features scaled to [0, 1] using train set only\n",
      "   âœ… Scaler params saved to: scaler_params.json\n",
      "   âœ… No data leakage (temporal split before normalization)\n",
      "\n",
      "5ï¸âƒ£ Data Quality:\n",
      "   âœ… No missing values in target\n",
      "   âœ… No missing values in features\n",
      "   âœ… Outliers removed\n",
      "   âœ… Time series continuous\n",
      "\n",
      "6ï¸âƒ£ Model-Specific Datasets:\n",
      "   ğŸ§  Deep Learning (CNN1D-BLSTM & LSTM): 15 base features\n",
      "   ğŸ“Š XGBoost: 63 features (with lag features)\n",
      "\n",
      "ğŸ’¾ Feature metadata saved to: ..\\data\\processed\\feature_metadata.json\n",
      "======================================================================\n",
      "   - Total locations: 14\n",
      "   - Time range: 2022-11-01 â†’ 2025-09-30\n",
      "\n",
      "2ï¸âƒ£ Feature Categories:\n",
      "   ğŸ“Œ Deep Learning Features: 15 features\n",
      "      - Current Pollutants (scaled): PM10, NO2, SO2\n",
      "      - Current Weather (scaled): temperature_2m, relative_humidity_2m, wind_speed_10m, wind_direction_10m, precipitation\n",
      "      - Time Features (cyclic): hour_sin/cos, month_sin/cos, day_of_week_sin/cos\n",
      "      - Time Features (linear): is_weekend\n",
      "   ğŸ“Œ XGBoost Features: 63 features\n",
      "      - Deep Learning features: 15\n",
      "      - Lag Features: 48 features\n",
      "      - Variables: PM2_5, PM10, NO2, SO2, temperature_2m, relative_humidity_2m, wind_speed_10m, precipitation\n",
      "      - Lag steps: [1, 2, 3, 6, 12, 24]\n",
      "\n",
      "3ï¸âƒ£ Target Variable:\n",
      "   ğŸ¯ PM2_5_scaled (normalized PM2.5)\n",
      "\n",
      "4ï¸âƒ£ Normalization:\n",
      "   âœ… All numerical features scaled to [0, 1] using train set only\n",
      "   âœ… Scaler params saved to: scaler_params.json\n",
      "   âœ… No data leakage (temporal split before normalization)\n",
      "\n",
      "5ï¸âƒ£ Data Quality:\n",
      "   âœ… No missing values in target\n",
      "   âœ… No missing values in features\n",
      "   âœ… Outliers removed\n",
      "   âœ… Time series continuous\n",
      "\n",
      "6ï¸âƒ£ Model-Specific Datasets:\n",
      "   ğŸ§  Deep Learning (CNN1D-BLSTM & LSTM): 15 base features\n",
      "   ğŸ“Š XGBoost: 63 features (with lag features)\n",
      "\n",
      "ğŸ’¾ Feature metadata saved to: ..\\data\\processed\\feature_metadata.json\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# BÆ°á»›c 7: Prepare Final Model Datasets\n",
    "print(\"\\nğŸ”„ Step 7: Preparing Final Model-Specific Datasets...\")\n",
    "\n",
    "# ========================================\n",
    "# DEEP LEARNING DATASETS (CNN1D-BLSTM & LSTM)\n",
    "# ========================================\n",
    "# KhÃ´ng cáº§n lag features, chá»‰ cáº§n base features + time features\n",
    "\n",
    "print(f\"\\nğŸ§  Deep Learning datasets (no lag features):\")\n",
    "\n",
    "# Select only DL features + target\n",
    "dl_train = df_train.select(\"location_id\", \"datetime\", target_feature, *dl_input_features)\n",
    "dl_val = df_val.select(\"location_id\", \"datetime\", target_feature, *dl_input_features)\n",
    "dl_test = df_test.select(\"location_id\", \"datetime\", target_feature, *dl_input_features)\n",
    "\n",
    "# Cache\n",
    "dl_train = dl_train.cache()\n",
    "dl_val = dl_val.cache()\n",
    "dl_test = dl_test.cache()\n",
    "\n",
    "dl_train_count = dl_train.count()\n",
    "dl_val_count = dl_val.count()\n",
    "dl_test_count = dl_test.count()\n",
    "\n",
    "print(f\"  âœ“ Train: {dl_train_count:,} records, {len(dl_input_features)} features\")\n",
    "print(f\"  âœ“ Val:   {dl_val_count:,} records, {len(dl_input_features)} features\")\n",
    "print(f\"  âœ“ Test:  {dl_test_count:,} records, {len(dl_input_features)} features\")\n",
    "\n",
    "# ========================================\n",
    "# XGBOOST DATASETS\n",
    "# ========================================\n",
    "# Cáº§n cáº£ base features + lag features\n",
    "\n",
    "print(f\"\\nğŸ“Š XGBoost datasets (with lag features):\")\n",
    "\n",
    "# Select XGB features + target\n",
    "xgb_train = df_train.select(\"location_id\", \"datetime\", target_feature, *xgb_input_features)\n",
    "xgb_val = df_val.select(\"location_id\", \"datetime\", target_feature, *xgb_input_features)\n",
    "xgb_test = df_test.select(\"location_id\", \"datetime\", target_feature, *xgb_input_features)\n",
    "\n",
    "# Cache\n",
    "xgb_train = xgb_train.cache()\n",
    "xgb_val = xgb_val.cache()\n",
    "xgb_test = xgb_test.cache()\n",
    "\n",
    "xgb_train_count = xgb_train.count()\n",
    "xgb_val_count = xgb_val.count()\n",
    "xgb_test_count = xgb_test.count()\n",
    "\n",
    "print(f\"  âœ“ Train: {xgb_train_count:,} records, {len(xgb_input_features)} features\")\n",
    "print(f\"  âœ“ Val:   {xgb_val_count:,} records, {len(xgb_input_features)} features\")\n",
    "print(f\"  âœ“ Test:  {xgb_test_count:,} records, {len(xgb_input_features)} features\")\n",
    "\n",
    "print(f\"\\nâœ… Final datasets prepared!\")\n",
    "print(f\"   ğŸ§  Deep Learning: {len(dl_input_features)} features (no lags)\")\n",
    "print(f\"   ğŸ“Š XGBoost: {len(xgb_input_features)} features (with {len(lag_base_columns) * len(LAG_STEPS)} lags)\")\n",
    "print(f\"   ğŸ¯ Target: {target_feature}\")\n",
    "print(f\"   âœ… All datasets cleaned and ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebc9c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BÆ°á»›c 8: Feature Engineering Summary + Metadata Saving\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š FEATURE ENGINEERING PIPELINE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nâœ… PIPELINE EXECUTION ORDER (Correct - No Data Leakage):\")\n",
    "print(f\"   1ï¸âƒ£ Time Features â†’ Added cyclic (sin/cos) + is_weekend\")\n",
    "print(f\"   2ï¸âƒ£ Temporal Split â†’ 70% train / 15% val / 15% test\")\n",
    "print(f\"   3ï¸âƒ£ Normalization â†’ Min-Max [0,1] using TRAIN statistics ONLY\")\n",
    "print(f\"   4ï¸âƒ£ Lag Features + Null Handling â†’ Created FROM SCALED columns, dropped nulls\")\n",
    "print(f\"   5ï¸âƒ£ Scaler Params â†’ Saved for inference\")\n",
    "print(f\"   6ï¸âƒ£ Model Features â†’ Prepared for Deep Learning & XGBoost\")\n",
    "print(f\"   7ï¸âƒ£ Final Datasets â†’ Ready for training\")\n",
    "\n",
    "print(f\"\\nğŸ“Š DATASET STATISTICS:\")\n",
    "print(f\"   Total records: {dl_train_count + dl_val_count + dl_test_count:,}\")\n",
    "print(f\"   Total locations: {df_train.select('location_id').distinct().count()}\")\n",
    "print(f\"   Time range: {min_time.strftime('%Y-%m-%d')} â†’ {max_time.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ FEATURE BREAKDOWN:\")\n",
    "print(f\"   ğŸ§  Deep Learning (CNN1D-BLSTM & LSTM): {len(dl_input_features)} features\")\n",
    "print(f\"      â”œâ”€ Pollutants (scaled): 3 (PM10, NO2, SO2)\")\n",
    "print(f\"      â”œâ”€ Weather (scaled): 5 (temp, humidity, wind, precipitation)\")\n",
    "print(f\"      â”œâ”€ Time (cyclic): 6 (hour, month, day_of_week â†’ sin/cos)\")\n",
    "print(f\"      â””â”€ Time (binary): 1 (is_weekend)\")\n",
    "print(f\"   \")\n",
    "print(f\"   ğŸ“Š XGBoost: {len(xgb_input_features)} features\")\n",
    "print(f\"      â”œâ”€ Deep Learning features: {len(dl_input_features)}\")\n",
    "print(f\"      â””â”€ Lag features: {len(lag_base_columns) * len(LAG_STEPS)} ({len(lag_base_columns)} vars Ã— {len(LAG_STEPS)} lags)\")\n",
    "\n",
    "print(f\"\\nğŸ¯ TARGET VARIABLE:\")\n",
    "print(f\"   {target_feature} (normalized PM2.5 in [0, 1])\")\n",
    "\n",
    "print(f\"\\nâœ… DATA QUALITY CHECKS:\")\n",
    "print(f\"   âœ“ No missing values in target\")\n",
    "print(f\"   âœ“ No missing values in features\")\n",
    "print(f\"   âœ“ No outliers (removed by WHO/EPA standards)\")\n",
    "print(f\"   âœ“ Proper temporal ordering\")\n",
    "print(f\"   âœ“ No data leakage (train/val/test temporally separated)\")\n",
    "print(f\"   âœ“ Correct scale relationship (lag from scaled columns)\")\n",
    "print(f\"   âœ“ No nulls in lag features (first {max(LAG_STEPS)}h dropped)\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ SAVED ARTIFACTS:\")\n",
    "print(f\"   ğŸ“ scaler_params.json â†’ Min-Max parameters (train set only)\")\n",
    "print(f\"   ğŸ“ feature_metadata.json â†’ Feature lists & configuration\")\n",
    "\n",
    "print(f\"\\nğŸš€ READY FOR NEXT PHASE:\")\n",
    "print(f\"   Variables in memory:\")\n",
    "print(f\"   - Deep Learning: dl_train, dl_val, dl_test\")\n",
    "print(f\"   - XGBoost: xgb_train, xgb_val, xgb_test\")\n",
    "print(f\"   Next step: Sequence creation for Deep Learning models\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========================================\n",
    "# SAVE FEATURE METADATA\n",
    "# ========================================\n",
    "# LÆ°u metadata vá» feature engineering Ä‘á»ƒ tham kháº£o trong tÆ°Æ¡ng lai\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Metadata cho feature engineering\n",
    "dataset_metadata = {\n",
    "    \"project\": \"PM2.5 Prediction\",\n",
    "    \"preprocessing_version\": \"2.0_refactored\",\n",
    "    \"pipeline_order\": [\n",
    "        \"Time Features (cyclic encoding)\",\n",
    "        \"Temporal Split (70/15/15)\",\n",
    "        \"Normalization (train stats only)\",\n",
    "        \"Lag Features (from scaled columns)\",\n",
    "        \"Null Handling (drop first 24h per location)\"\n",
    "    ],\n",
    "    \"deep_learning_features\": dl_input_features,\n",
    "    \"xgboost_features\": xgb_input_features,\n",
    "    \"target_feature\": target_feature,\n",
    "    \"lag_config\": {\n",
    "        \"lag_steps\": LAG_STEPS,\n",
    "        \"lag_base_columns\": lag_base_columns,\n",
    "        \"total_lag_features\": len(lag_base_columns) * len(LAG_STEPS)\n",
    "    },\n",
    "    \"temporal_split\": {\n",
    "        \"train_end\": train_end.isoformat(),\n",
    "        \"val_end\": val_end.isoformat(),\n",
    "        \"min_time\": min_time.isoformat(),\n",
    "        \"max_time\": max_time.isoformat()\n",
    "    },\n",
    "    \"dataset_counts\": {\n",
    "        \"dl_train\": dl_train_count,\n",
    "        \"dl_val\": dl_val_count,\n",
    "        \"dl_test\": dl_test_count,\n",
    "        \"xgb_train\": xgb_train_count,\n",
    "        \"xgb_val\": xgb_val_count,\n",
    "        \"xgb_test\": xgb_test_count\n",
    "    },\n",
    "    \"total_features\": {\n",
    "        \"deep_learning\": len(dl_input_features),\n",
    "        \"xgboost\": len(xgb_input_features)\n",
    "    }\n",
    "}\n",
    "\n",
    "# ========================================\n",
    "# ADAPTIVE PATH (Kaggle vs Colab vs Local)\n",
    "# ========================================\n",
    "if IN_KAGGLE:\n",
    "    # ğŸ† Kaggle: Write to /kaggle/working (auto-saved on commit)\n",
    "    processed_dir = Path(\"/kaggle/working/processed\")\n",
    "    print(f\"\\nğŸ† Kaggle mode: Saving metadata to {processed_dir}\")\n",
    "    \n",
    "elif IN_COLAB:\n",
    "    # ğŸŒ Colab: Write to Google Drive\n",
    "    processed_dir = Path(\"/content/drive/MyDrive/pm25-data/processed\")\n",
    "    print(f\"\\nğŸŒ Colab mode: Saving metadata to Google Drive\")\n",
    "    \n",
    "else:\n",
    "    # ğŸ’» Local: Write to project folder\n",
    "    processed_dir = Path(\"../data/processed\")\n",
    "    print(f\"\\nğŸ’» Local mode: Saving metadata to {processed_dir}\")\n",
    "\n",
    "# Táº¡o thÆ° má»¥c vá»›i parents=True (táº¡o cáº£ parent directories náº¿u chÆ°a cÃ³)\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# LÆ°u metadata\n",
    "metadata_path = processed_dir / \"feature_metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(dataset_metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\nğŸ’¾ Feature metadata saved to: {metadata_path}\")\n",
    "print(f\"   âœ… Pipeline version: 2.0 (refactored - no data leakage)\")\n",
    "print(f\"   âœ… Contains: feature lists, lag config, split info, dataset counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523011ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ Step 7: Creating Sequence Data for Deep Learning Models...\n",
      "â˜ï¸ CLOUD ENVIRONMENT: Using optimal sequences\n",
      "ğŸ“Š Creating sequences for each model...\n",
      "\n",
      "ğŸ§  CNN1D-BLSTM-Attention (48 timesteps):\n",
      "    Creating 48-step sequences...\n",
      "        Processing batch 1/5...\n",
      "        Processing batch 2/5...\n",
      "        Processing batch 2/5...\n",
      "        Processing batch 3/5...\n",
      "        Processing batch 3/5...\n",
      "        Processing batch 4/5...\n",
      "        Processing batch 4/5...\n",
      "        Processing batch 5/5...\n",
      "        Processing batch 5/5...\n",
      "    âŒ CNN sequence creation failed: An error occurred while calling o16487.count.\n",
      ": org.apache.spark.SparkException: Job aborted due to ...\n",
      "\n",
      "ğŸ”„ LSTM (24 timesteps):\n",
      "    Creating 24-step sequences...\n",
      "    âŒ CNN sequence creation failed: An error occurred while calling o16487.count.\n",
      ": org.apache.spark.SparkException: Job aborted due to ...\n",
      "\n",
      "ğŸ”„ LSTM (24 timesteps):\n",
      "    Creating 24-step sequences...\n",
      "        Processing batch 1/5...\n",
      "        Processing batch 1/5...\n",
      "    âŒ LSTM sequence creation failed: An error occurred while calling o17353.cache.\n",
      ": java.lang.IllegalStateException: Cannot call methods...\n",
      "\n",
      "âœ… Sequence data preparation completed!\n",
      "    âŒ LSTM sequence creation failed: An error occurred while calling o17353.cache.\n",
      ": java.lang.IllegalStateException: Cannot call methods...\n",
      "\n",
      "âœ… Sequence data preparation completed!\n"
     ]
    }
   ],
   "source": [
    "# BÆ°á»›c 9: Create Sequence Data for Deep Learning Models\n",
    "print(\"\\nğŸ”„ Step 9: Creating Sequence Data for Deep Learning Models...\")\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "# Sequence configuration (optimized for Colab)\n",
    "CNN_SEQUENCE_LENGTH = 48  # Optimal for long-term patterns\n",
    "LSTM_SEQUENCE_LENGTH = 24  # Optimal for medium-term patterns\n",
    "\n",
    "print(f\"âš™ï¸  Sequence Configuration:\")\n",
    "print(f\"   - CNN1D-BLSTM-Attention: {CNN_SEQUENCE_LENGTH} timesteps\")\n",
    "print(f\"   - LSTM: {LSTM_SEQUENCE_LENGTH} timesteps\")\n",
    "\n",
    "def create_sequences_optimized(df, feature_cols, target_col, sequence_length):\n",
    "    \"\"\"\n",
    "    Optimized sequence creation with checkpointing to avoid StackOverflow\n",
    "    \n",
    "    ğŸ¯ Key Strategy:\n",
    "    - Batch processing to avoid deep logical plans\n",
    "    - Checkpoint after each batch to reset plan depth\n",
    "    - Use broadcast joins for efficiency\n",
    "    - Single final filter for null handling\n",
    "    \n",
    "    ğŸ”‘ Null Handling (2-Layer Protection):\n",
    "    Layer 1: Drop first N records/location (incomplete history)\n",
    "    Layer 2: Filter ANY null in sequences (data gaps)\n",
    "    Result: 100% clean sequences with ZERO nulls\n",
    "    \"\"\"\n",
    "    print(f\"    Creating {sequence_length}-step sequences...\")\n",
    "    \n",
    "    window_spec = Window.partitionBy(\"location_id\").orderBy(\"datetime\")\n",
    "    \n",
    "    # ========================================\n",
    "    # LAYER 1: Drop first N records (incomplete history)\n",
    "    # ========================================\n",
    "    df_base = df.select(\"location_id\", \"datetime\", target_col, *feature_cols) \\\n",
    "                .repartition(4, \"location_id\") \\\n",
    "                .withColumn(\"row_num\", F.row_number().over(window_spec)) \\\n",
    "                .filter(F.col(\"row_num\") > sequence_length) \\\n",
    "                .drop(\"row_num\") \\\n",
    "                .cache()\n",
    "    \n",
    "    records_after_layer1 = df_base.count()  # Materialize\n",
    "    print(f\"      ğŸ›¡ï¸  Layer 1: Dropped first {sequence_length} records/location\")\n",
    "    print(f\"         Records: {records_after_layer1:,}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # BATCH PROCESSING (é¿å… StackOverflow)\n",
    "    # ========================================\n",
    "    # Chia features thÃ nh batches nhá» Ä‘á»ƒ trÃ¡nh logical plan quÃ¡ sÃ¢u\n",
    "    BATCH_SIZE = 4  # Má»—i batch xá»­ lÃ½ 4 features (4 Ã— 48 lags = 192 ops - safe)\n",
    "    feature_batches = [feature_cols[i:i+BATCH_SIZE] for i in range(0, len(feature_cols), BATCH_SIZE)]\n",
    "    \n",
    "    print(f\"        ğŸ“¦ Processing {len(feature_batches)} batches ({len(feature_cols)} features)...\")\n",
    "    \n",
    "    base_cols = [\"location_id\", \"datetime\"]\n",
    "    result_df = df_base.select(*base_cols)\n",
    "    \n",
    "    for batch_idx, batch_features in enumerate(feature_batches, 1):\n",
    "        print(f\"           Batch {batch_idx}/{len(feature_batches)}: {len(batch_features)} features\")\n",
    "        \n",
    "        # Táº¡o batch DataFrame\n",
    "        batch_df = df_base.select(*base_cols, *batch_features)\n",
    "        \n",
    "        # Táº¡o sequences cho batch nÃ y\n",
    "        for col_name in batch_features:\n",
    "            # Táº¡o array of lags [t-1, t-2, ..., t-N]\n",
    "            lag_exprs = [F.lag(col_name, step).over(window_spec) for step in range(1, sequence_length + 1)]\n",
    "            batch_df = batch_df.withColumn(f\"{col_name}_sequence\", F.array(*lag_exprs))\n",
    "        \n",
    "        # Select chá»‰ sequence columns\n",
    "        sequence_cols = [f\"{col}_sequence\" for col in batch_features]\n",
    "        batch_df = batch_df.select(*base_cols, *sequence_cols).cache()\n",
    "        batch_df.count()  # Materialize Ä‘á»ƒ reset logical plan\n",
    "        \n",
    "        # Join vÃ o result\n",
    "        result_df = result_df.join(batch_df, base_cols, \"inner\")\n",
    "        \n",
    "        # Unpersist batch (giáº£i phÃ³ng memory)\n",
    "        batch_df.unpersist()\n",
    "    \n",
    "    # ========================================\n",
    "    # LAYER 2: Filter nulls in sequences\n",
    "    # ========================================\n",
    "    print(f\"        ğŸ” Filtering null sequences...\")\n",
    "    \n",
    "    all_sequence_cols = [f\"{col}_sequence\" for col in feature_cols]\n",
    "    \n",
    "    # Build null filter: ALL sequences must be NOT NULL\n",
    "    from functools import reduce\n",
    "    null_filter = reduce(\n",
    "        lambda acc, col: acc & F.col(col).isNotNull(),\n",
    "        all_sequence_cols,\n",
    "        F.lit(True)\n",
    "    )\n",
    "    \n",
    "    # Also check: NO null VALUES inside arrays (extra safety)\n",
    "    # Trick: size(array) should equal sequence_length (nulls make size smaller)\n",
    "    for seq_col in all_sequence_cols:\n",
    "        null_filter = null_filter & (F.size(seq_col) == sequence_length)\n",
    "    \n",
    "    result_df = result_df.filter(null_filter)\n",
    "    records_after_layer2 = result_df.count()\n",
    "    dropped = records_after_layer1 - records_after_layer2\n",
    "    \n",
    "    if dropped > 0:\n",
    "        print(f\"      ğŸ›¡ï¸  Layer 2: Dropped {dropped:,} records with nulls\")\n",
    "        print(f\"         Records: {records_after_layer1:,} â†’ {records_after_layer2:,}\")\n",
    "    else:\n",
    "        print(f\"      âœ… Layer 2: No data gaps detected\")\n",
    "    \n",
    "    # ========================================\n",
    "    # FINAL: Add target and clean up\n",
    "    # ========================================\n",
    "    result_df = result_df.join(\n",
    "        df_base.select(\"location_id\", \"datetime\", target_col),\n",
    "        [\"location_id\", \"datetime\"],\n",
    "        \"inner\"\n",
    "    ).filter(F.col(target_col).isNotNull()) \\\n",
    "     .withColumnRenamed(target_col, \"target_value\") \\\n",
    "     .cache()\n",
    "    \n",
    "    final_count = result_df.count()\n",
    "    retention_rate = (final_count / records_after_layer1) * 100\n",
    "    \n",
    "    print(f\"      âœ… Final: {final_count:,} records ({retention_rate:.1f}% retained)\")\n",
    "    \n",
    "    # Cleanup\n",
    "    df_base.unpersist()\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "print(\"\\nğŸ“Š Creating sequences for each model...\")\n",
    "\n",
    "# Create CNN1D-BLSTM sequences\n",
    "print(f\"\\nğŸ§  CNN1D-BLSTM-Attention ({CNN_SEQUENCE_LENGTH} timesteps):\")\n",
    "try:\n",
    "    cnn_train_clean = create_sequences_optimized(dl_train, dl_input_features, target_feature, CNN_SEQUENCE_LENGTH)\n",
    "    cnn_val_clean = create_sequences_optimized(dl_val, dl_input_features, target_feature, CNN_SEQUENCE_LENGTH)\n",
    "    cnn_test_clean = create_sequences_optimized(dl_test, dl_input_features, target_feature, CNN_SEQUENCE_LENGTH)\n",
    "    print(f\"    âœ… CNN sequences created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"    âŒ CNN sequence creation failed: {str(e)[:100]}...\")\n",
    "    cnn_train_clean = cnn_val_clean = cnn_test_clean = None\n",
    "\n",
    "# Create LSTM sequences  \n",
    "print(f\"\\nğŸ”„ LSTM ({LSTM_SEQUENCE_LENGTH} timesteps):\")\n",
    "try:\n",
    "    lstm_train_clean = create_sequences_optimized(dl_train, dl_input_features, target_feature, LSTM_SEQUENCE_LENGTH)\n",
    "    lstm_val_clean = create_sequences_optimized(dl_val, dl_input_features, target_feature, LSTM_SEQUENCE_LENGTH)\n",
    "    lstm_test_clean = create_sequences_optimized(dl_test, dl_input_features, target_feature, LSTM_SEQUENCE_LENGTH)\n",
    "    print(f\"    âœ… LSTM sequences created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"    âŒ LSTM sequence creation failed: {str(e)[:100]}...\")\n",
    "    lstm_train_clean = lstm_val_clean = lstm_test_clean = None\n",
    "\n",
    "print(f\"\\nâœ… Sequence data preparation completed!\")\n",
    "print(f\"\\nğŸ“‹ Data Quality Guarantee:\")\n",
    "print(f\"   âœ“ Layer 1: No incomplete history (first {CNN_SEQUENCE_LENGTH}/{LSTM_SEQUENCE_LENGTH} records dropped)\")\n",
    "print(f\"   âœ“ Layer 2: No data gaps in middle (nulls filtered out)\")\n",
    "print(f\"   âœ“ Result: 100% clean sequences with ZERO nulls\")\n",
    "print(f\"   âœ“ Ready for high-quality model training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a99d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Step 10: Exporting Final Datasets to Disk...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cnn_train_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      9\u001b[39m processed_dir.mkdir(exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Check dataset availability\u001b[39;00m\n\u001b[32m     12\u001b[39m datasets_ready = {\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcnn\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mcnn_train_clean\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m cnn_val_clean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m cnn_test_clean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlstm\u001b[39m\u001b[33m\"\u001b[39m: lstm_train_clean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m lstm_val_clean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m lstm_test_clean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mxgb\u001b[39m\u001b[33m\"\u001b[39m: xgb_train \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m xgb_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m xgb_test \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     16\u001b[39m }\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mğŸ“Š Dataset Status:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model, ready \u001b[38;5;129;01min\u001b[39;00m datasets_ready.items():\n",
      "\u001b[31mNameError\u001b[39m: name 'cnn_train_clean' is not defined"
     ]
    }
   ],
   "source": [
    "# BÆ°á»›c 10: Export Final Datasets to Disk\n",
    "print(\"\\nğŸ“¦ Step 10: Exporting Final Datasets to Disk...\")\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ========================================\n",
    "# ADAPTIVE OUTPUT PATH (Kaggle vs Colab vs Local)\n",
    "# ========================================\n",
    "if IN_KAGGLE:\n",
    "    # ğŸ† Kaggle: Write to /kaggle/working (auto-saved on commit)\n",
    "    processed_dir = Path(\"/kaggle/working/processed\")\n",
    "    print(f\"ğŸ† Kaggle mode: Saving to {processed_dir}\")\n",
    "    print(f\"   âš ï¸  Files will be auto-saved when you commit notebook\")\n",
    "    \n",
    "elif IN_COLAB:\n",
    "    # ğŸŒ Colab: Write to Google Drive\n",
    "    processed_dir = Path(\"/content/drive/MyDrive/pm25-data/processed\")\n",
    "    print(f\"ğŸŒ Colab mode: Saving to Google Drive\")\n",
    "    \n",
    "else:\n",
    "    # ğŸ’» Local: Write to project folder\n",
    "    processed_dir = Path(\"../data/processed\")\n",
    "    print(f\"ğŸ’» Local mode: Saving to {processed_dir}\")\n",
    "\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check dataset availability\n",
    "datasets_ready = {\n",
    "    \"cnn\": cnn_train_clean is not None and cnn_val_clean is not None and cnn_test_clean is not None,\n",
    "    \"lstm\": lstm_train_clean is not None and lstm_val_clean is not None and lstm_test_clean is not None,\n",
    "    \"xgb\": xgb_train is not None and xgb_val is not None and xgb_test is not None\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ“Š Dataset Status:\")\n",
    "for model, ready in datasets_ready.items():\n",
    "    model_name = {\"cnn\": \"CNN1D-BLSTM\", \"lstm\": \"LSTM\", \"xgb\": \"XGBoost\"}[model]\n",
    "    status = \"âœ… Ready\" if ready else \"âŒ Not Ready\"\n",
    "    print(f\"  {model_name}: {status}\")\n",
    "\n",
    "# ========================================\n",
    "# EXPORT DATASETS TO PARQUET\n",
    "# ========================================\n",
    "print(f\"\\nğŸ’¾ Exporting datasets to Parquet format...\")\n",
    "\n",
    "export_summary = {\n",
    "    \"cnn\": {\"train\": 0, \"val\": 0, \"test\": 0},\n",
    "    \"lstm\": {\"train\": 0, \"val\": 0, \"test\": 0},\n",
    "    \"xgb\": {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "}\n",
    "\n",
    "# Export CNN1D-BLSTM datasets\n",
    "if datasets_ready[\"cnn\"]:\n",
    "    print(f\"\\n  ğŸ§  Exporting CNN1D-BLSTM datasets...\")\n",
    "    cnn_dir = processed_dir / \"cnn_sequences\"\n",
    "    cnn_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    cnn_train_clean.write.mode(\"overwrite\").parquet(str(cnn_dir / \"train\"))\n",
    "    cnn_val_clean.write.mode(\"overwrite\").parquet(str(cnn_dir / \"val\"))\n",
    "    cnn_test_clean.write.mode(\"overwrite\").parquet(str(cnn_dir / \"test\"))\n",
    "    \n",
    "    export_summary[\"cnn\"][\"train\"] = cnn_train_clean.count()\n",
    "    export_summary[\"cnn\"][\"val\"] = cnn_val_clean.count()\n",
    "    export_summary[\"cnn\"][\"test\"] = cnn_test_clean.count()\n",
    "    \n",
    "    print(f\"     âœ… Saved to: {cnn_dir}/\")\n",
    "    print(f\"        - train: {export_summary['cnn']['train']:,} records\")\n",
    "    print(f\"        - val:   {export_summary['cnn']['val']:,} records\")\n",
    "    print(f\"        - test:  {export_summary['cnn']['test']:,} records\")\n",
    "\n",
    "# Export LSTM datasets\n",
    "if datasets_ready[\"lstm\"]:\n",
    "    print(f\"\\n  ğŸ”„ Exporting LSTM datasets...\")\n",
    "    lstm_dir = processed_dir / \"lstm_sequences\"\n",
    "    lstm_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    lstm_train_clean.write.mode(\"overwrite\").parquet(str(lstm_dir / \"train\"))\n",
    "    lstm_val_clean.write.mode(\"overwrite\").parquet(str(lstm_dir / \"val\"))\n",
    "    lstm_test_clean.write.mode(\"overwrite\").parquet(str(lstm_dir / \"test\"))\n",
    "    \n",
    "    export_summary[\"lstm\"][\"train\"] = lstm_train_clean.count()\n",
    "    export_summary[\"lstm\"][\"val\"] = lstm_val_clean.count()\n",
    "    export_summary[\"lstm\"][\"test\"] = lstm_test_clean.count()\n",
    "    \n",
    "    print(f\"     âœ… Saved to: {lstm_dir}/\")\n",
    "    print(f\"        - train: {export_summary['lstm']['train']:,} records\")\n",
    "    print(f\"        - val:   {export_summary['lstm']['val']:,} records\")\n",
    "    print(f\"        - test:  {export_summary['lstm']['test']:,} records\")\n",
    "\n",
    "# Export XGBoost datasets\n",
    "if datasets_ready[\"xgb\"]:\n",
    "    print(f\"\\n  ğŸ“Š Exporting XGBoost datasets...\")\n",
    "    xgb_dir = processed_dir / \"xgboost\"\n",
    "    xgb_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    xgb_train.write.mode(\"overwrite\").parquet(str(xgb_dir / \"train\"))\n",
    "    xgb_val.write.mode(\"overwrite\").parquet(str(xgb_dir / \"val\"))\n",
    "    xgb_test.write.mode(\"overwrite\").parquet(str(xgb_dir / \"test\"))\n",
    "    \n",
    "    export_summary[\"xgb\"][\"train\"] = xgb_train.count()\n",
    "    export_summary[\"xgb\"][\"val\"] = xgb_val.count()\n",
    "    export_summary[\"xgb\"][\"test\"] = xgb_test.count()\n",
    "    \n",
    "    print(f\"     âœ… Saved to: {xgb_dir}/\")\n",
    "    print(f\"        - train: {export_summary['xgb']['train']:,} records\")\n",
    "    print(f\"        - val:   {export_summary['xgb']['val']:,} records\")\n",
    "    print(f\"        - test:  {export_summary['xgb']['test']:,} records\")\n",
    "\n",
    "# ========================================\n",
    "# SAVE METADATA\n",
    "# ========================================\n",
    "print(f\"\\nğŸ’¾ Saving metadata...\")\n",
    "\n",
    "# Create comprehensive metadata\n",
    "final_metadata = {\n",
    "    \"project\": \"PM2.5 Prediction\",\n",
    "    \"preprocessing_completed\": True,\n",
    "    \"export_timestamp\": str(pd.Timestamp.now()),\n",
    "    \"environment\": \"kaggle\" if IN_KAGGLE else (\"colab\" if IN_COLAB else \"local\"),\n",
    "    \"models\": {\n",
    "        \"cnn1d_blstm\": {\n",
    "            \"sequence_length\": CNN_SEQUENCE_LENGTH,\n",
    "            \"features\": len(dl_input_features),\n",
    "            \"ready\": datasets_ready[\"cnn\"],\n",
    "            \"export_path\": str(processed_dir / \"cnn_sequences\"),\n",
    "            \"record_counts\": export_summary[\"cnn\"]\n",
    "        },\n",
    "        \"lstm\": {\n",
    "            \"sequence_length\": LSTM_SEQUENCE_LENGTH, \n",
    "            \"features\": len(dl_input_features),\n",
    "            \"ready\": datasets_ready[\"lstm\"],\n",
    "            \"export_path\": str(processed_dir / \"lstm_sequences\"),\n",
    "            \"record_counts\": export_summary[\"lstm\"]\n",
    "        },\n",
    "        \"xgboost\": {\n",
    "            \"features\": len(xgb_input_features),\n",
    "            \"lag_steps\": LAG_STEPS,\n",
    "            \"ready\": datasets_ready[\"xgb\"],\n",
    "            \"export_path\": str(processed_dir / \"xgboost\"),\n",
    "            \"record_counts\": export_summary[\"xgb\"]\n",
    "        }\n",
    "    },\n",
    "    \"feature_details\": {\n",
    "        \"deep_learning_features\": dl_input_features,\n",
    "        \"xgboost_features\": xgb_input_features,\n",
    "        \"target\": target_feature\n",
    "    },\n",
    "    \"data_format\": \"parquet\",\n",
    "    \"null_handling\": {\n",
    "        \"strategy\": \"2-layer protection\",\n",
    "        \"layer1\": f\"Dropped first {CNN_SEQUENCE_LENGTH}/{LSTM_SEQUENCE_LENGTH} records per location\",\n",
    "        \"layer2\": \"Filtered records with nulls in sequence history\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = processed_dir / \"datasets_ready.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(final_metadata, f, indent=2)\n",
    "\n",
    "print(f\"   âœ… Metadata saved to: {metadata_path}\")\n",
    "\n",
    "# Save scaler params\n",
    "scaler_path = processed_dir / \"scaler_params.json\"\n",
    "scaler_json = {\n",
    "    col: {\"min\": float(params[\"min\"]), \"max\": float(params[\"max\"])} \n",
    "    for col, params in scaler_params.items()\n",
    "}\n",
    "with open(scaler_path, 'w') as f:\n",
    "    json.dump(scaler_json, f, indent=2)\n",
    "print(f\"   âœ… Scaler params saved to: {scaler_path}\")\n",
    "\n",
    "# Save feature metadata\n",
    "feature_metadata_path = processed_dir / \"feature_metadata.json\"\n",
    "feature_metadata = {\n",
    "    \"deep_learning_features\": dl_input_features,\n",
    "    \"xgboost_features\": xgb_input_features,\n",
    "    \"target\": target_feature,\n",
    "    \"lag_steps\": LAG_STEPS,\n",
    "    \"lag_base_columns\": lag_base_columns\n",
    "}\n",
    "with open(feature_metadata_path, 'w') as f:\n",
    "    json.dump(feature_metadata, f, indent=2)\n",
    "print(f\"   âœ… Feature metadata saved to: {feature_metadata_path}\")\n",
    "\n",
    "# ========================================\n",
    "# FINAL SUMMARY\n",
    "# ========================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"âœ… DATA PREPROCESSING & EXPORT COMPLETE!\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    print(f\"\\nğŸ† KAGGLE OUTPUT:\")\n",
    "    print(f\"   ğŸ“‚ Location: /kaggle/working/processed/\")\n",
    "    print(f\"   ğŸ“Œ To save permanently:\")\n",
    "    print(f\"      1. Click 'Save Version' (top right)\")\n",
    "    print(f\"      2. Choose 'Save & Run All' (recommended)\")\n",
    "    print(f\"      3. Wait for completion (~20-30 min)\")\n",
    "    print(f\"      4. Output will appear in 'Output' tab\")\n",
    "    print(f\"      5. Use as dataset: '+ Add Data' â†’ Your Output\")\n",
    "    \n",
    "elif IN_COLAB:\n",
    "    print(f\"\\nğŸŒ COLAB OUTPUT:\")\n",
    "    print(f\"   ğŸ“‚ Saved to Google Drive: {processed_dir}\")\n",
    "    print(f\"   âœ… Files persist across sessions\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\nğŸ’» LOCAL OUTPUT:\")\n",
    "    print(f\"   ğŸ“‚ Location: {processed_dir.absolute()}\")\n",
    "\n",
    "print(f\"\\nğŸ“‚ Exported Directory Structure:\")\n",
    "print(f\"   {processed_dir}/\")\n",
    "print(f\"   â”œâ”€â”€ cnn_sequences/\")\n",
    "print(f\"   â”‚   â”œâ”€â”€ train/  ({export_summary['cnn']['train']:,} records)\")\n",
    "print(f\"   â”‚   â”œâ”€â”€ val/    ({export_summary['cnn']['val']:,} records)\")\n",
    "print(f\"   â”‚   â””â”€â”€ test/   ({export_summary['cnn']['test']:,} records)\")\n",
    "print(f\"   â”œâ”€â”€ lstm_sequences/\")\n",
    "print(f\"   â”‚   â”œâ”€â”€ train/  ({export_summary['lstm']['train']:,} records)\")\n",
    "print(f\"   â”‚   â”œâ”€â”€ val/    ({export_summary['lstm']['val']:,} records)\")\n",
    "print(f\"   â”‚   â””â”€â”€ test/   ({export_summary['lstm']['test']:,} records)\")\n",
    "print(f\"   â”œâ”€â”€ xgboost/\")\n",
    "print(f\"   â”‚   â”œâ”€â”€ train/  ({export_summary['xgb']['train']:,} records)\")\n",
    "print(f\"   â”‚   â”œâ”€â”€ val/    ({export_summary['xgb']['val']:,} records)\")\n",
    "print(f\"   â”‚   â””â”€â”€ test/   ({export_summary['xgb']['test']:,} records)\")\n",
    "print(f\"   â”œâ”€â”€ scaler_params.json\")\n",
    "print(f\"   â”œâ”€â”€ feature_metadata.json\")\n",
    "print(f\"   â””â”€â”€ datasets_ready.json\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Total Dataset Sizes:\")\n",
    "total_cnn = sum(export_summary['cnn'].values())\n",
    "total_lstm = sum(export_summary['lstm'].values())\n",
    "total_xgb = sum(export_summary['xgb'].values())\n",
    "print(f\"   - CNN1D-BLSTM: {total_cnn:,} records ({CNN_SEQUENCE_LENGTH} timesteps, {len(dl_input_features)} features)\")\n",
    "print(f\"   - LSTM:        {total_lstm:,} records ({LSTM_SEQUENCE_LENGTH} timesteps, {len(dl_input_features)} features)\")\n",
    "print(f\"   - XGBoost:     {total_xgb:,} records ({len(xgb_input_features)} features)\")\n",
    "\n",
    "print(f\"\\nğŸš€ Ready for Model Training Phase!\")\n",
    "if IN_KAGGLE:\n",
    "    print(f\"   ğŸ“Œ Next: Create new notebook, add this output as dataset\")\n",
    "    print(f\"   ğŸ“Œ Load: spark.read.parquet('/kaggle/input/<output-name>/processed/...')\")\n",
    "elif IN_COLAB:\n",
    "    print(f\"   ğŸ“Œ Load from Drive in next session\")\n",
    "else:\n",
    "    print(f\"   ğŸ“Œ Load: spark.read.parquet('{processed_dir}/...')\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d4f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \udcca Kiá»ƒm tra kÃ­ch thÆ°á»›c files Parquet Ä‘Ã£ export\n",
    "print(\"\\n\udcca Parquet File Size Analysis...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import os\n",
    "\n",
    "# ========================================\n",
    "# ADAPTIVE PATH\n",
    "# ========================================\n",
    "if IN_KAGGLE:\n",
    "    processed_dir = Path(\"/kaggle/working/processed\")\n",
    "    print(f\"ğŸ† Kaggle mode: Analyzing {processed_dir}\")\n",
    "elif IN_COLAB:\n",
    "    processed_dir = Path(\"/content/drive/MyDrive/pm25-data/processed\")\n",
    "    print(f\"ğŸŒ Colab mode: Analyzing Google Drive\")\n",
    "else:\n",
    "    processed_dir = Path(\"../data/processed\")\n",
    "    print(f\"ğŸ’» Local mode: Analyzing {processed_dir}\")\n",
    "\n",
    "# ========================================\n",
    "# TÃ­nh kÃ­ch thÆ°á»›c thÆ° má»¥c\n",
    "# ========================================\n",
    "def get_dir_size(path):\n",
    "    \"\"\"TÃ­nh tá»•ng kÃ­ch thÆ°á»›c cá»§a thÆ° má»¥c (bao gá»“m táº¥t cáº£ subdirectories)\"\"\"\n",
    "    total_size = 0\n",
    "    try:\n",
    "        for dirpath, dirnames, filenames in os.walk(path):\n",
    "            for filename in filenames:\n",
    "                filepath = os.path.join(dirpath, filename)\n",
    "                if os.path.exists(filepath):\n",
    "                    total_size += os.path.getsize(filepath)\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸  Error accessing {path}: {e}\")\n",
    "        return 0\n",
    "    return total_size\n",
    "\n",
    "def format_size(bytes_size):\n",
    "    \"\"\"Format bytes thÃ nh human-readable\"\"\"\n",
    "    for unit in ['B', 'KB', 'MB', 'GB']:\n",
    "        if bytes_size < 1024.0:\n",
    "            return f\"{bytes_size:.2f} {unit}\"\n",
    "        bytes_size /= 1024.0\n",
    "    return f\"{bytes_size:.2f} TB\"\n",
    "\n",
    "# ========================================\n",
    "# PhÃ¢n tÃ­ch tá»«ng dataset\n",
    "# ========================================\n",
    "print(\"\\nğŸ“¦ Dataset Sizes:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "datasets = {\n",
    "    'CNN Sequences': 'cnn_sequences',\n",
    "    'LSTM Sequences': 'lstm_sequences',\n",
    "    'XGBoost Data': 'xgboost'\n",
    "}\n",
    "\n",
    "total_size = 0\n",
    "size_breakdown = {}\n",
    "\n",
    "for name, folder in datasets.items():\n",
    "    dataset_path = processed_dir / folder\n",
    "    if dataset_path.exists():\n",
    "        # TÃ­nh size cho tá»«ng split\n",
    "        splits = ['train', 'val', 'test']\n",
    "        dataset_total = 0\n",
    "        print(f\"\\nğŸ§  {name}:\")\n",
    "        \n",
    "        for split in splits:\n",
    "            split_path = dataset_path / split\n",
    "            if split_path.exists():\n",
    "                size = get_dir_size(split_path)\n",
    "                dataset_total += size\n",
    "                print(f\"   - {split:5s}: {format_size(size):>12s}\")\n",
    "        \n",
    "        print(f\"   {'Total:':7s} {format_size(dataset_total):>12s}\")\n",
    "        size_breakdown[name] = dataset_total\n",
    "        total_size += dataset_total\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  {name}: Folder not found ({dataset_path})\")\n",
    "\n",
    "# Metadata files\n",
    "print(f\"\\n\udccb Metadata Files:\")\n",
    "metadata_files = ['scaler_params.json', 'feature_metadata.json', 'datasets_ready.json']\n",
    "metadata_total = 0\n",
    "for file in metadata_files:\n",
    "    file_path = processed_dir / file\n",
    "    if file_path.exists():\n",
    "        size = os.path.getsize(file_path)\n",
    "        metadata_total += size\n",
    "        print(f\"   - {file:25s}: {format_size(size):>12s}\")\n",
    "total_size += metadata_total\n",
    "\n",
    "# ========================================\n",
    "# Tá»•ng káº¿t\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š TOTAL SIZE SUMMARY:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, size in size_breakdown.items():\n",
    "    percentage = (size / total_size * 100) if total_size > 0 else 0\n",
    "    print(f\"   {name:20s}: {format_size(size):>12s} ({percentage:5.1f}%)\")\n",
    "\n",
    "print(f\"   {'Metadata':20s}: {format_size(metadata_total):>12s} ({(metadata_total/total_size*100):5.1f}%)\")\n",
    "print(f\"\\n   {'GRAND TOTAL':20s}: {format_size(total_size):>12s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95740d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¼ EXAMPLE: Load Preprocessed Data with Pandas\n",
    "print(\"\\nğŸ¼ Loading Preprocessed Data with Pandas...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========================================\n",
    "# ADAPTIVE PATH\n",
    "# ========================================\n",
    "if IN_KAGGLE:\n",
    "    data_dir = Path(\"/kaggle/input/<your-dataset-name>/processed\")\n",
    "    print(f\"ğŸ† Kaggle mode: Loading from /kaggle/input/\")\n",
    "    print(f\"   Replace <your-dataset-name> with actual dataset name\")\n",
    "elif IN_COLAB:\n",
    "    data_dir = Path(\"/content/drive/MyDrive/pm25-data/processed\")\n",
    "    print(f\"ğŸŒ Colab mode: Loading from Google Drive\")\n",
    "else:\n",
    "    data_dir = Path(\"../data/processed\")\n",
    "    print(f\"ğŸ’» Local mode: Loading from {data_dir}\")\n",
    "\n",
    "# ========================================\n",
    "# LOAD PARQUET FILES\n",
    "# ========================================\n",
    "print(\"\\nğŸ“¦ Loading datasets...\")\n",
    "\n",
    "try:\n",
    "    # CNN sequences (48 timesteps)\n",
    "    print(\"\\nğŸ§  CNN1D-BLSTM-Attention:\")\n",
    "    cnn_train = pd.read_parquet(data_dir / 'cnn_sequences' / 'train')\n",
    "    cnn_val = pd.read_parquet(data_dir / 'cnn_sequences' / 'val')\n",
    "    cnn_test = pd.read_parquet(data_dir / 'cnn_sequences' / 'test')\n",
    "    print(f\"   âœ… Train: {cnn_train.shape} | Val: {cnn_val.shape} | Test: {cnn_test.shape}\")\n",
    "    \n",
    "    # LSTM sequences (24 timesteps)\n",
    "    print(\"\\nğŸ”„ LSTM:\")\n",
    "    lstm_train = pd.read_parquet(data_dir / 'lstm_sequences' / 'train')\n",
    "    lstm_val = pd.read_parquet(data_dir / 'lstm_sequences' / 'val')\n",
    "    lstm_test = pd.read_parquet(data_dir / 'lstm_sequences' / 'test')\n",
    "    print(f\"   âœ… Train: {lstm_train.shape} | Val: {lstm_val.shape} | Test: {lstm_test.shape}\")\n",
    "    \n",
    "    # XGBoost data (flat features)\n",
    "    print(\"\\nğŸ“Š XGBoost:\")\n",
    "    xgb_train = pd.read_parquet(data_dir / 'xgboost' / 'train')\n",
    "    xgb_val = pd.read_parquet(data_dir / 'xgboost' / 'val')\n",
    "    xgb_test = pd.read_parquet(data_dir / 'xgboost' / 'test')\n",
    "    print(f\"   âœ… Train: {xgb_train.shape} | Val: {xgb_val.shape} | Test: {xgb_test.shape}\")\n",
    "    \n",
    "    print(f\"\\nâœ… All datasets loaded successfully!\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nâŒ Error: Dataset not found!\")\n",
    "    print(f\"   {e}\")\n",
    "    print(f\"\\nğŸ’¡ Make sure to:\")\n",
    "    if IN_KAGGLE:\n",
    "        print(f\"   1. Add this notebook's output as dataset\")\n",
    "        print(f\"   2. Update <your-dataset-name> in path\")\n",
    "    else:\n",
    "        print(f\"   1. Run previous cells to generate data\")\n",
    "        print(f\"   2. Check path: {data_dir}\")\n",
    "\n",
    "# ========================================\n",
    "# LOAD METADATA\n",
    "# ========================================\n",
    "print(\"\\nğŸ“‹ Loading metadata...\")\n",
    "\n",
    "import json\n",
    "\n",
    "try:\n",
    "    # Scaler parameters (for denormalization)\n",
    "    with open(data_dir / 'scaler_params.json', 'r') as f:\n",
    "        scaler_params = json.load(f)\n",
    "    print(f\"   âœ… Scaler params: {len(scaler_params)} features\")\n",
    "    \n",
    "    # Feature metadata\n",
    "    with open(data_dir / 'feature_metadata.json', 'r') as f:\n",
    "        feature_metadata = json.load(f)\n",
    "    print(f\"   âœ… Feature metadata: {feature_metadata['preprocessing_version']}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"   âš ï¸  Metadata files not found (optional)\")\n",
    "    scaler_params = None\n",
    "    feature_metadata = None\n",
    "\n",
    "# ========================================\n",
    "# DATA PREPARATION FOR DEEP LEARNING\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ¯ Prepare Data for Training:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Example: CNN data preparation\n",
    "print(\"\\nğŸ“¦ CNN1D-BLSTM data preparation:\")\n",
    "\n",
    "# Get sequence columns\n",
    "sequence_cols = [col for col in cnn_train.columns if col.endswith('_sequence')]\n",
    "print(f\"   - Sequence features: {len(sequence_cols)}\")\n",
    "print(f\"   - Feature names: {sequence_cols[:3]}... (showing first 3)\")\n",
    "\n",
    "# Convert to numpy arrays for deep learning\n",
    "print(\"\\n   Converting to numpy arrays...\")\n",
    "\n",
    "# Extract sequences (each row has arrays)\n",
    "X_cnn_train = np.array([\n",
    "    np.stack([cnn_train[col].iloc[i] for col in sequence_cols], axis=0)\n",
    "    for i in range(len(cnn_train))\n",
    "])  # Shape: (samples, features, timesteps)\n",
    "\n",
    "# Transpose to (samples, timesteps, features) for Keras/PyTorch\n",
    "X_cnn_train = X_cnn_train.transpose(0, 2, 1)\n",
    "y_cnn_train = cnn_train['target_value'].values\n",
    "\n",
    "print(f\"   âœ… X_train shape: {X_cnn_train.shape} (samples, timesteps, features)\")\n",
    "print(f\"   âœ… y_train shape: {y_cnn_train.shape}\")\n",
    "\n",
    "# Same for validation and test\n",
    "X_cnn_val = np.array([\n",
    "    np.stack([cnn_val[col].iloc[i] for col in sequence_cols], axis=0)\n",
    "    for i in range(len(cnn_val))\n",
    "]).transpose(0, 2, 1)\n",
    "y_cnn_val = cnn_val['target_value'].values\n",
    "\n",
    "X_cnn_test = np.array([\n",
    "    np.stack([cnn_test[col].iloc[i] for col in sequence_cols], axis=0)\n",
    "    for i in range(len(cnn_test))\n",
    "]).transpose(0, 2, 1)\n",
    "y_cnn_test = cnn_test['target_value'].values\n",
    "\n",
    "print(f\"   âœ… Val:  X={X_cnn_val.shape}, y={y_cnn_val.shape}\")\n",
    "print(f\"   âœ… Test: X={X_cnn_test.shape}, y={y_cnn_test.shape}\")\n",
    "\n",
    "# ========================================\n",
    "# EXAMPLE: XGBoost data preparation\n",
    "# ========================================\n",
    "print(\"\\nğŸ“¦ XGBoost data preparation:\")\n",
    "\n",
    "# XGBoost data is already flat (no sequences)\n",
    "X_xgb_train = xgb_train.drop(['location_id', 'datetime', 'target_value'], axis=1).values\n",
    "y_xgb_train = xgb_train['target_value'].values\n",
    "\n",
    "X_xgb_val = xgb_val.drop(['location_id', 'datetime', 'target_value'], axis=1).values\n",
    "y_xgb_val = xgb_val['target_value'].values\n",
    "\n",
    "X_xgb_test = xgb_test.drop(['location_id', 'datetime', 'target_value'], axis=1).values\n",
    "y_xgb_test = xgb_test['target_value'].values\n",
    "\n",
    "print(f\"   âœ… X_train shape: {X_xgb_train.shape} (samples, features)\")\n",
    "print(f\"   âœ… y_train shape: {y_xgb_train.shape}\")\n",
    "print(f\"   âœ… Val:  X={X_xgb_val.shape}, y={y_xgb_val.shape}\")\n",
    "print(f\"   âœ… Test: X={X_xgb_test.shape}, y={y_xgb_test.shape}\")\n",
    "\n",
    "# ========================================\n",
    "# SUMMARY\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… DATA READY FOR TRAINING!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nğŸ“Š Available datasets:\")\n",
    "print(\"   ğŸ§  CNN1D-BLSTM-Attention:\")\n",
    "print(f\"      X_cnn_train: {X_cnn_train.shape}\")\n",
    "print(f\"      X_cnn_val:   {X_cnn_val.shape}\")\n",
    "print(f\"      X_cnn_test:  {X_cnn_test.shape}\")\n",
    "\n",
    "print(\"\\n   ğŸ”„ LSTM: (Similar structure, use lstm_train/val/test)\")\n",
    "\n",
    "print(\"\\n   ğŸ“Š XGBoost:\")\n",
    "print(f\"      X_xgb_train: {X_xgb_train.shape}\")\n",
    "print(f\"      X_xgb_val:   {X_xgb_val.shape}\")\n",
    "print(f\"      X_xgb_test:  {X_xgb_test.shape}\")\n",
    "\n",
    "print(\"\\nğŸš€ Next steps:\")\n",
    "print(\"   1. Build model: model = tf.keras.Sequential([...])\")\n",
    "print(\"   2. Compile: model.compile(optimizer='adam', loss='mse')\")\n",
    "print(\"   3. Train: model.fit(X_cnn_train, y_cnn_train, epochs=50)\")\n",
    "print(\"   4. Evaluate: model.evaluate(X_cnn_test, y_cnn_test)\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1511033",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_train.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

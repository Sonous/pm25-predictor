{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b24733ca",
   "metadata": {},
   "source": [
    "# PM2.5 Prediction - Data Preprocessing\n\nNotebook này thực hiện tiền xử lý dữ liệu với PySpark:\n1. Kết nối Spark cluster\n2. Đọc và khám phá dữ liệu\n3. Tổng quan về dataset\n4. **Làm sạch dữ liệu** (Outlier Removal -> Missing Value Imputation)\n5. Feature engineering\n6. Data summary & statistics\n7. Lưu dữ liệu đã xử lý"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6420c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\nimport sys\n\n# Detect environment (Kaggle vs Colab vs Local)\nIN_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_KAGGLE:\n    print(\"[KAGGLE] Running on Kaggle\")\n    \n    # Kaggle has Java pre-installed, just set JAVA_HOME\n    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n    os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ.get(\"PATH\", \"\")\n    \n    # Install PySpark\n    print(\"[INSTALL] Installing PySpark...\")\n    !pip install -q pyspark\n    print(\"[OK] PySpark installed\")\n    print(f\"[OK] Java: {os.environ['JAVA_HOME']}\")\n    \nelif IN_COLAB:\n    print(\"[COLAB] Running on Google Colab\")\n    \n    # Install Java 11 (required for PySpark)\n    print(\"[INSTALL] Installing Java 11...\")\n    !apt-get install -y openjdk-11-jdk-headless -qq > /dev/null\n    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n    os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ.get(\"PATH\", \"\")\n    print(f\"[OK] Java installed: {os.environ['JAVA_HOME']}\")\n    \n    # Install PySpark\n    print(\"[INSTALL] Installing PySpark...\")\n    !pip install -q pyspark\n    print(\"[OK] PySpark installed\")\n    \n    # Mount Google Drive (optional - if data is in Drive)\n    # from google.colab import drive\n    # drive.mount('/content/drive')\n    \nelse:\n    print(\"[LOCAL] Running on Local Machine\")\n    \n    # Set Java 21 for PySpark (local only)\n    os.environ['JAVA_HOME'] = r'C:\\Program Files\\Java\\jdk-21'\n    os.environ['PATH'] = os.environ['JAVA_HOME'] + r'\\bin;' + os.environ.get('PATH', '')\n    print(f\"[OK] Using Java: {os.environ['JAVA_HOME']}\")\n\n# Common imports\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\nfrom pyspark.sql.types import *\nfrom pyspark.sql.window import Window\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set_style('whitegrid')\n\nprint(\"[OK] All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18df14d2",
   "metadata": {},
   "source": [
    "## 1. Kết nối Spark Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057cbfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo Spark Session với cấu hình tùy theo môi trường\nif IN_KAGGLE:\n    # Kaggle configuration - Balanced (4 cores, 16GB RAM)\n    spark = SparkSession.builder \\\n        .appName(\"PM25-Preprocessing\") \\\n        .master(\"local[4]\") \\\n        .config(\"spark.driver.memory\", \"8g\") \\\n        .config(\"spark.executor.memory\", \"6g\") \\\n        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n        .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n        .config(\"spark.default.parallelism\", \"8\") \\\n        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n        .getOrCreate()\n    \n    print(\"[KAGGLE] Spark running on Kaggle (4 cores, 12GB total)\")\n    \nelif IN_COLAB:\n    # Colab configuration - Lighter settings (2 cores, 12GB RAM)\n    spark = SparkSession.builder \\\n        .appName(\"PM25-Preprocessing\") \\\n        .master(\"local[2]\") \\\n        .config(\"spark.driver.memory\", \"2g\") \\\n        .config(\"spark.executor.memory\", \"2g\") \\\n        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n        .getOrCreate()\n    \n    print(\"[COLAB] Spark running on Colab (2 cores, 4GB total)\")\n    \nelse:\n    # Local configuration - OPTIMIZED for 8-core AMD Ryzen + 15.7GB RAM\n    spark = SparkSession.builder \\\n        .appName(\"PM25-Preprocessing\") \\\n        .master(\"local[8]\") \\\n        .config(\"spark.driver.memory\", \"8g\") \\\n        .config(\"spark.executor.memory\", \"4g\") \\\n        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n        .config(\"spark.sql.shuffle.partitions\", \"16\") \\\n        .config(\"spark.default.parallelism\", \"16\") \\\n        .config(\"spark.python.worker.timeout\", \"600\") \\\n        .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n        .config(\"spark.network.timeout\", \"600s\") \\\n        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n        .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\") \\\n        .getOrCreate()\n    \n    print(\"[LOCAL] Spark running on Local (8 cores, 12GB total - OPTIMIZED)\")\n\nprint(f\"[OK] Spark version: {spark.version}\")\nprint(f\"[OK] Spark mode: {spark.sparkContext.master}\")\nprint(f\"[OK] Application ID: {spark.sparkContext.applicationId}\")\nprint(f\"[OK] Cores: {spark.sparkContext.defaultParallelism}\")\nprint(f\"[OK] Parallelism: {spark.conf.get('spark.default.parallelism', 'default')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cdf19f",
   "metadata": {},
   "source": [
    "## 2. Định nghĩa Schema và Scan Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6e4012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema cho dữ liệu OpenAQ\nopenaq_schema = StructType([\n    StructField(\"location_id\", StringType(), True),\n    StructField(\"sensors_id\", StringType(), True),\n    StructField(\"location\", StringType(), True),\n    StructField(\"datetime\", TimestampType(), True),\n    StructField(\"lat\", DoubleType(), True),\n    StructField(\"lon\", DoubleType(), True),\n    StructField(\"parameter\", StringType(), True),\n    StructField(\"units\", StringType(), True),\n    StructField(\"value\", DoubleType(), True)\n])\n\n# Schema cho dữ liệu Weather\nweather_schema = StructType([\n    StructField(\"time\", TimestampType(), True),\n    StructField(\"temperature_2m\", DoubleType(), True),\n    StructField(\"relative_humidity_2m\", DoubleType(), True),\n    StructField(\"wind_speed_10m\", DoubleType(), True),\n    StructField(\"wind_direction_10m\", DoubleType(), True),\n    StructField(\"surface_pressure\", DoubleType(), True),\n    StructField(\"precipitation\", DoubleType(), True)\n])\n\nprint(\"[OK] Schemas defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb7210a",
   "metadata": {},
   "source": [
    "### 2.1 Scan và Map Files theo Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\nimport re\nfrom pathlib import Path\n\n# ========================================\n# KAGGLE: Sử dụng đường dẫn Kaggle dataset\n# ========================================\nif IN_KAGGLE:\n    # [KAGGLE] Kaggle paths\n    # Format: /kaggle/input/{dataset-name}/\n    raw_data_path = Path(\"/kaggle/input/pm25-hongkong-raw\")  # ← Thay tên dataset của bạn\n    print(f\"[KAGGLE] Using Kaggle dataset: {raw_data_path}\")\n    \nelif IN_COLAB:\n    # [COLAB] Colab: Mount Google Drive\n    from google.colab import drive\n    drive.mount('/content/drive')\n    raw_data_path = Path(\"/content/drive/MyDrive/pm25-data/raw\")  # ← Thay đường dẫn Drive của bạn\n    print(f\"[COLAB] Using Google Drive: {raw_data_path}\")\n    \nelse:\n    # [LOCAL] Local path (giữ nguyên)\n    raw_data_path = Path(\"../data/raw\")\n    print(f\"[LOCAL] Using local path: {raw_data_path}\")\n\n# Tìm tất cả các file pollutant\npollutant_files = list(raw_data_path.glob(\"pollutant_location_*.csv\"))\n\nprint(f\"[FILES] Found {len(pollutant_files)} pollutant files:\")\n\n# Tạo mapping giữa pollutant và weather files\nlocation_mapping = {}\n\nfor pollutant_file in pollutant_files:\n    # Extract location_id từ tên file: pollutant_location_7727.csv -> 7727\n    match = re.search(r'pollutant_location_(\\d+)\\.csv', pollutant_file.name)\n    \n    if match:\n        location_id = match.group(1)\n        weather_file = raw_data_path / f\"weather_location_{location_id}.csv\"\n        \n        # Kiểm tra file weather tương ứng có tồn tại không\n        if weather_file.exists():\n            location_mapping[location_id] = {\n                'pollutant': str(pollutant_file),\n                'weather': str(weather_file)\n            }\n            print(f\"  [OK] Location {location_id}: {pollutant_file.name} + {weather_file.name}\")\n        else:\n            print(f\"  [WARNING]  Location {location_id}: Missing weather file!\")\n\nprint(f\"\\n[SUCCESS] Total locations to process: {len(location_mapping)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bd57cd",
   "metadata": {},
   "source": [
    "### 2.2 Xử lý từng Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c0476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List để chứa dataframes của từng location\nall_locations_data = []\n\nfor location_id, files in location_mapping.items():\n    print(f\"\\n[PROCESSING] Processing Location {location_id}...\")\n    \n    # Đọc pollutant data\n    df_air = spark.read.csv(\n        files['pollutant'],\n        header=True,\n        schema=openaq_schema\n    )\n    \n    # [?] LỌC CHỈ LẤY CÁC CHỈ SỐ QUAN TÂM: PM2.5, PM10, SO2, NO2\n    df_air = df_air.filter(\n        F.col(\"parameter\").isin([\"pm25\", \"pm10\", \"so2\", \"no2\"])\n    )\n    \n    # Đọc weather data\n    df_weather = spark.read.csv(\n        files['weather'],\n        header=True,\n        schema=weather_schema\n    )\n    \n    print(f\"  [DATA] Air quality (PM2.5, PM10, SO2, NO2): {df_air.count():,} records\")\n    print(f\"  [?]  Weather: {df_weather.count():,} records\")\n    \n    # Weather data - drop missing (ít missing)\n    df_weather_clean = df_weather.na.drop()\n    \n    # Pivot pollutant data\n    df_air_pivot = df_air.groupBy(\n        \"location_id\", \"location\", \"datetime\", \"lat\", \"lon\"\n    ).pivot(\"parameter\").agg(F.first(\"value\"))\n    \n    # Rename columns\n    column_mapping = {\n        \"pm25\": \"PM2_5\",\n        \"pm10\": \"PM10\",\n        \"no2\": \"NO2\",\n        \"so2\": \"SO2\"\n    }\n    \n    for old_name, new_name in column_mapping.items():\n        if old_name in df_air_pivot.columns:\n            df_air_pivot = df_air_pivot.withColumnRenamed(old_name, new_name)\n    \n    # Join với weather data (theo datetime)\n    df_location = df_air_pivot.join(\n        df_weather_clean,\n        df_air_pivot.datetime == df_weather_clean.time,\n        \"inner\"\n    ).drop(\"time\")\n    \n    print(f\"  [OK] After join: {df_location.count():,} records\")\n    \n    # Thêm vào list\n    all_locations_data.append(df_location)\n\nprint(f\"\\n[SUCCESS] Processed {len(all_locations_data)} locations successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b156b356",
   "metadata": {},
   "source": [
    "### 2.3 Gộp tất cả Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409a2995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gộp tất cả locations lại\nprint(f\"[PROCESSING] Combining {len(all_locations_data)} locations...\")\n\ndf_combined = all_locations_data[0]\nfor df in all_locations_data[1:]:\n    df_combined = df_combined.union(df)\n\n# OPTIMIZE: Cache để tránh recompute nhiều lần\ndf_combined = df_combined.cache()\n\n# OPTIMIZE: Trigger action 1 lần, tránh count() nhiều lần\nprint(\"⏳ Computing combined dataset (this may take a moment)...\")\ntotal_records = df_combined.count()\nnum_locations = df_combined.select('location_id').distinct().count()\n\nprint(f\"[SUCCESS] Combined dataset: {total_records:,} total records\")\nprint(f\"[SUCCESS] Number of locations: {num_locations}\")\n\n# OPTIMIZE: Chỉ show sample, không orderBy toàn bộ dataset (rất chậm!)\nprint(\"\\n[METADATA] Sample records (unsorted):\")\ndf_combined.show(10, truncate=False)\n\n# OPTIONAL: Nếu cần sort, chỉ sort 1 partition nhỏ để xem\n# df_combined.orderBy(\"location_id\", \"datetime\").limit(50).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3751add",
   "metadata": {},
   "source": [
    "## 3. Tổng quan Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae5670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thống kê theo location\nprint(\"[DATA] Dataset Overview by Location:\")\ndf_combined.groupBy(\"location_id\", \"location\").count().orderBy(\"location_id\").show(truncate=False)\n\n# Time range của từng location\nprint(\"\\n[?] Time Range by Location:\")\ndf_combined.groupBy(\"location_id\").agg(\n    F.min(\"datetime\").alias(\"start_date\"),\n    F.max(\"datetime\").alias(\"end_date\"),\n    F.count(\"*\").alias(\"records\")\n).orderBy(\"location_id\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368fb50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiểm tra missing values\nprint(\"[WARNING]  Missing Values Summary:\")\nfor col_name in df_combined.columns:\n    null_count = df_combined.filter(F.col(col_name).isNull()).count()\n    total = df_combined.count()\n    pct = (null_count / total) * 100\n    if null_count > 0:  # Chỉ hiển thị cột có missing\n        print(f\"  {col_name:25s}: {null_count:8,} ({pct:6.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f767ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics tổng quan\nprint(\"[?] Overall Statistics:\")\ndf_combined.select(\n    \"PM2_5\", \"PM10\", \"NO2\", \"SO2\",\n    \"temperature_2m\", \"relative_humidity_2m\", \"wind_speed_10m\", \"precipitation\"\n).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02439644",
   "metadata": {},
   "source": [
    "## 4. Làm sạch Dữ liệu\n",
    "\n",
    "**Quy trình làm sạch:**\n",
    "1. **Loại bỏ Outliers trước** - Để tránh giá trị cực đoan ảnh hưởng đến tính toán statistics\n",
    "2. **Fill Missing Values sau** - Imputation dựa trên dữ liệu đã loại bỏ outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e126b6",
   "metadata": {},
   "source": [
    "### 4.1. Loại bỏ Outliers\n",
    "\n",
    "Loại bỏ các giá trị cực đoan trước khi imputation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8a917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loại bỏ outliers theo WHO/EPA International Standards (cho dữ liệu Hong Kong)\n# [WARNING]  QUAN TRỌNG: PM2.5 là TARGET variable - PHẢI có giá trị thật!\n#     -> Records có PM2.5 = null sẽ BỊ LOẠI BỎ\n#     -> Chỉ các features khác (PM10, NO2, SO2) mới được phép null và impute sau\n\ndf_no_outliers = df_combined.filter(\n    # [TARGET] TARGET: PM2.5 theo WHO Emergency threshold (không cho phép null)\n    (F.col(\"PM2_5\").isNotNull()) & \n    (F.col(\"PM2_5\") >= 0) & (F.col(\"PM2_5\") < 250) &  # WHO Emergency: 250 μg/m³\n    \n    # [DATA] FEATURES: WHO/EPA International Standards - Cho phép null, chỉ loại outliers\n    ((F.col(\"PM10\").isNull()) | ((F.col(\"PM10\") >= 0) & (F.col(\"PM10\") < 430))) &  # WHO Emergency: 430 μg/m³\n    ((F.col(\"NO2\").isNull()) | ((F.col(\"NO2\") >= 0) & (F.col(\"NO2\") < 400))) &     # WHO/EU: 400 μg/m³ (1-hour)\n    ((F.col(\"SO2\").isNull()) | ((F.col(\"SO2\") >= 0) & (F.col(\"SO2\") < 500))) &     # WHO/EU: 500 μg/m³ (10-min)\n    \n    # [?] WEATHER: WMO standards cho Hong Kong\n    (F.col(\"precipitation\") >= 0) & (F.col(\"precipitation\") < 100)  # WMO: 100mm/h extreme rain\n)\n\nrecords_before = df_combined.count()\nrecords_after = df_no_outliers.count()\nremoved = records_before - records_after\n\nprint(f\"[DATA] Outlier Removal:\")\nprint(f\"  Before: {records_before:,} records\")\nprint(f\"  After:  {records_after:,} records\")\nprint(f\"  Removed: {removed:,} records ({removed/records_before*100:.2f}%)\")\nprint(f\"\\n  [WARNING]  Note: Records with PM2.5 = null are REMOVED (target variable must have real values)\")\n\n# Kiểm tra missing values sau khi loại outliers\nprint(\"\\n[WARNING]  Missing values after outlier removal:\")\nfor col_name in [\"PM2_5\", \"PM10\", \"NO2\", \"SO2\"]:\n    if col_name in df_no_outliers.columns:\n        null_count = df_no_outliers.filter(F.col(col_name).isNull()).count()\n        total = df_no_outliers.count()\n        pct = (null_count / total) * 100\n        if null_count > 0:\n            print(f\"  {col_name:10s}: {null_count:8,} ({pct:6.2f}%)\")\n        elif col_name == \"PM2_5\":\n            print(f\"  {col_name:10s}: {null_count:8,} ({pct:6.2f}%) [SUCCESS] (Target - must be 0%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0994f2",
   "metadata": {},
   "source": [
    "### 4.2. Xử lý Missing Values (Interpolation)\n",
    "\n",
    "**Chiến lược Imputation cho Time Series:**\n",
    "- **PM2.5**: Đã loại bỏ tất cả records có null (target variable)\n",
    "- **PM10, NO2, SO2**: Sử dụng **Linear Interpolation** (tốt nhất cho time series)\n",
    "  - Bước 1: **Linear Interpolation** - Nội suy tuyến tính dựa trên giá trị trước & sau\n",
    "  - Bước 2: **Forward Fill** - Xử lý missing ở cuối chuỗi (không có giá trị sau)\n",
    "  - Bước 3: **Backward Fill** - Xử lý missing ở đầu chuỗi (không có giá trị trước)\n",
    "  - Bước 4: **Mean** - Backup cuối cùng (nếu còn missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de82f80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chiến lược Imputation cho Time Series Data\n# Sử dụng PySpark Window Functions - Nội suy tuyến tính dựa trên khoảng cách thời gian\n\n# List các cột FEATURES cần impute (KHÔNG bao gồm PM2.5 - target variable)\npollutant_cols = [\"PM10\", \"NO2\", \"SO2\"]  # [WARNING] Không có PM2.5!\n\nprint(f\"[PROCESSING] Time Series Imputation Strategy (PySpark Native):\")\nprint(f\"   1. True Linear Interpolation - y = y₁ + (y₂-y₁) × (t-t₁)/(t₂-t₁)\")\nprint(f\"   2. Forward Fill - If only prev value available\")\nprint(f\"   3. Backward Fill - If only next value available\")\nprint(f\"   4. Null - If no surrounding values (rare)\")\nprint(f\"\\n   Columns to impute: {pollutant_cols}\")\nprint(f\"   PM2.5 NOT imputed (target variable - already removed nulls)\")\nprint(f\"   [?] Safe: Window partitioned by location_id (no cross-location interpolation)\\n\")\n\n# Cache để tăng performance\ndf_filled = df_no_outliers.cache()\n\n# Kiểm tra missing TRƯỚC khi interpolate\nprint(\"[WARNING]  Missing values BEFORE interpolation:\")\nfor col_name in pollutant_cols:\n    if col_name in df_filled.columns:\n        null_count = df_filled.filter(F.col(col_name).isNull()).count()\n        total = df_filled.count()\n        pct = (null_count / total) * 100\n        if null_count > 0:\n            print(f\"  {col_name:10s}: {null_count:8,} ({pct:6.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a622c600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Áp dụng True Linear Interpolation với PySpark (không dùng Pandas)\n# Nội suy tuyến tính dựa trên khoảng cách thời gian THỰC (epoch)\n# Window function đảm bảo KHÔNG nội suy chéo giữa các locations\n\nprint(\"[PROCESSING] Applying true linear interpolation per location (PySpark native)...\")\n\n# Tạo cột epoch (timestamp dạng số) để tính toán khoảng cách thời gian\ndf_filled = df_filled.withColumn(\"epoch\", F.col(\"datetime\").cast(\"long\"))\n\n# Định nghĩa Window cho từng location\nw_forward = (\n    Window.partitionBy(\"location_id\")\n    .orderBy(\"epoch\")\n    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n)\n\nw_backward = (\n    Window.partitionBy(\"location_id\")\n    .orderBy(\"epoch\")\n    .rowsBetween(Window.currentRow, Window.unboundedFollowing)\n)\n\n# Xử lý từng pollutant column\nfor col_name in pollutant_cols:\n    if col_name not in df_filled.columns:\n        continue\n    \n    print(f\"  ▶ Interpolating {col_name}...\", end=\" \", flush=True)\n    \n    # Bước 1: Tìm giá trị & timestamp TRƯỚC và SAU gần nhất (có giá trị non-null)\n    df_filled = (\n        df_filled\n        .withColumn(f\"{col_name}_prev_value\", F.last(col_name, True).over(w_forward))\n        .withColumn(f\"{col_name}_next_value\", F.first(col_name, True).over(w_backward))\n        .withColumn(f\"{col_name}_prev_time\", F.last(F.when(F.col(col_name).isNotNull(), F.col(\"epoch\")), True).over(w_forward))\n        .withColumn(f\"{col_name}_next_time\", F.first(F.when(F.col(col_name).isNotNull(), F.col(\"epoch\")), True).over(w_backward))\n    )\n    \n    # Bước 2: Tính toán Linear Interpolation theo công thức:\n    # y = y₁ + (y₂ - y₁) * (t - t₁) / (t₂ - t₁)\n    interpolated_value = (\n        F.col(f\"{col_name}_prev_value\") +\n        (F.col(f\"{col_name}_next_value\") - F.col(f\"{col_name}_prev_value\")) *\n        ((F.col(\"epoch\") - F.col(f\"{col_name}_prev_time\")) /\n         (F.col(f\"{col_name}_next_time\") - F.col(f\"{col_name}_prev_time\")))\n    )\n    \n    # Bước 3: Logic chọn giá trị cuối cùng với fallback\n    df_filled = df_filled.withColumn(\n        col_name,\n        F.when(F.col(col_name).isNotNull(), F.col(col_name))  # Giữ nguyên nếu có giá trị\n         .when(\n             # Linear interpolation nếu có cả prev & next và không chia 0\n             (F.col(f\"{col_name}_prev_value\").isNotNull()) &\n             (F.col(f\"{col_name}_next_value\").isNotNull()) &\n             ((F.col(f\"{col_name}_next_time\") - F.col(f\"{col_name}_prev_time\")) != 0),\n             interpolated_value\n         )\n         .when(F.col(f\"{col_name}_prev_value\").isNotNull(), F.col(f\"{col_name}_prev_value\"))  # Forward fill\n         .when(F.col(f\"{col_name}_next_value\").isNotNull(), F.col(f\"{col_name}_next_value\"))  # Backward fill\n         .otherwise(None)  # Vẫn null nếu không có data nào\n    )\n    \n    # Bước 4: Xóa các cột phụ để giảm memory\n    df_filled = df_filled.drop(\n        f\"{col_name}_prev_value\", f\"{col_name}_next_value\",\n        f\"{col_name}_prev_time\", f\"{col_name}_next_time\"\n    )\n    \n    print(\"[OK]\")\n\n# Cache kết quả sau khi interpolation\ndf_filled = df_filled.cache()\n\n# Trigger computation và đếm records\ncount = df_filled.count()\nprint(f\"\\n[SUCCESS] Linear interpolation completed! Total records: {count:,}\")\nprint(f\"   [GEAR]  Method: True linear interpolation based on time distance (epoch)\")\nprint(f\"   [?] Safe: No cross-location interpolation (partitioned by location_id)\")\nprint(f\"   [RUN] Optimized: Native PySpark (no Pandas conversion)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001c1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: PM2.5 không có null, các features khác không có null\nprint(\"\\n[METADATA] Final Missing Values Check (After Interpolation):\")\n\n# Kiểm tra PM2.5 (target)\npm25_nulls = df_filled.filter(F.col(\"PM2_5\").isNull()).count()\nprint(f\"  PM2_5 (Target): {pm25_nulls:,} nulls [SUCCESS] (Must be 0)\")\n\n# Kiểm tra features\ntotal_nulls = 0\nfor col_name in pollutant_cols:\n    if col_name in df_filled.columns:\n        null_count = df_filled.filter(F.col(col_name).isNull()).count()\n        total_nulls += null_count\n        if null_count > 0:\n            print(f\"  {col_name:10s}: {null_count:,} nulls [WARNING]\")\n        else:\n            print(f\"  {col_name:10s}: {null_count:,} nulls [SUCCESS]\")\n\n# Xử lý edge case: Drop records còn null (không có giá trị xung quanh để interpolate)\nif total_nulls > 0:\n    print(f\"\\n[WARNING]  Found {total_nulls} remaining nulls (edge cases with no surrounding data)\")\n    print(f\"   -> Dropping these records to ensure data quality...\")\n    \n    records_before_drop = df_filled.count()\n    \n    # Drop records có bất kỳ feature nào còn null\n    for col_name in pollutant_cols:\n        df_filled = df_filled.filter(F.col(col_name).isNotNull())\n    \n    records_after_drop = df_filled.count()\n    dropped = records_before_drop - records_after_drop\n    \n    print(f\"   Before drop: {records_before_drop:,} records\")\n    print(f\"   After drop:  {records_after_drop:,} records\")\n    print(f\"   Dropped:     {dropped:,} records ({dropped/records_before_drop*100:.2f}%)\")\n    print(f\"\\n   [SUCCESS] All feature columns now have 0 nulls!\")\nelse:\n    print(\"\\n  [SUCCESS] No missing values in any feature columns!\")\n\n# Xóa cột epoch (đã dùng xong)\ndf_filled = df_filled.drop(\"epoch\")\n\n# Verify lần cuối\nprint(f\"\\n[DATA] Final Verification:\")\nfor col_name in [\"PM2_5\"] + pollutant_cols:\n    if col_name in df_filled.columns:\n        null_count = df_filled.filter(F.col(col_name).isNull()).count()\n        print(f\"  {col_name:10s}: {null_count:,} nulls [SUCCESS]\")\n\nprint(f\"\\n[SUCCESS] Data cleaning completed with True Linear Interpolation!\")\nprint(f\"   Final dataset: {df_filled.count():,} records\")\nprint(f\"   [WARNING]  All records have REAL PM2.5 values (target variable)\")\nprint(f\"   [SUCCESS] Features interpolated smoothly (time-based linear interpolation)\")\nprint(f\"   [SUCCESS] Edge cases (no surrounding data) removed\")\nprint(f\"   [RUN] Performance: Native PySpark (no Pandas conversion)\")\n\n# Cập nhật df_combined với dữ liệu đã clean và sắp xếp\ndf_combined = df_filled.orderBy(\"location_id\", \"datetime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7365dd0b",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering & Normalization\n\n**Quy trình ĐÚNG để tránh Data Leakage:**\n1. **Time Features** - Thêm cyclic encoding (sin/cos) và is_weekend (không cần normalize)\n2. **Temporal Split** - Chia train/validation/test theo thời gian (70/15/15)\n3. **Normalization** - Chuẩn hóa **CHỈ numerical GỐC** bằng Min-Max từ train set\n4. **Lag Features** - Tạo lag TỪ CÁC CỘT ĐÃ SCALE (giữ đúng scale relationship)\n5. **Model-Specific Datasets** - Chuẩn bị riêng cho Deep Learning và XGBoost\n6. **Null Handling** - Xử lý nulls trong lag features cuối cùng\n\n**[WARNING] QUAN TRỌNG:** Lag features phải tạo SAU khi normalize để giữ đúng mối quan hệ scale!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03014c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bước 1: Thêm Time Features từ dữ liệu đã clean\nprint(\"[PROCESSING] Step 1: Adding Time Features (No normalization needed)...\")\n\nimport math\n\ndf_features = df_combined \\\n    .withColumn(\"hour\", F.hour(\"datetime\")) \\\n    .withColumn(\"month\", F.month(\"datetime\")) \\\n    .withColumn(\"day_of_week\", F.dayofweek(\"datetime\"))\n\n# Cyclic encoding cho hour (24h cycle)\ndf_features = df_features \\\n    .withColumn(\"hour_sin\", F.sin(2 * math.pi * F.col(\"hour\") / 24)) \\\n    .withColumn(\"hour_cos\", F.cos(2 * math.pi * F.col(\"hour\") / 24))\n\n# Cyclic encoding cho month (12 month cycle)\ndf_features = df_features \\\n    .withColumn(\"month_sin\", F.sin(2 * math.pi * F.col(\"month\") / 12)) \\\n    .withColumn(\"month_cos\", F.cos(2 * math.pi * F.col(\"month\") / 12))\n\n# Cyclic encoding cho day_of_week (7 day cycle)\ndf_features = df_features \\\n    .withColumn(\"day_of_week_sin\", F.sin(2 * math.pi * F.col(\"day_of_week\") / 7)) \\\n    .withColumn(\"day_of_week_cos\", F.cos(2 * math.pi * F.col(\"day_of_week\") / 7))\n\n# Binary feature: is_weekend\ndf_features = df_features \\\n    .withColumn(\"is_weekend\", F.when(F.col(\"day_of_week\").isin([1, 7]), 1).otherwise(0))\n\n# Xóa các cột trung gian\ndf_features = df_features.drop(\"hour\", \"month\", \"day_of_week\")\n\nprint(\"[OK] Time features added successfully!\")\nprint(f\"[OK] Total records: {df_features.count():,}\")\nprint(f\"[OK] Total columns: {len(df_features.columns)}\")\n\nprint(\"\\n[METADATA] Time Features Created:\")\nprint(\"  Cyclic (sin/cos): hour, month, day_of_week -> Already in [-1, 1]\")\nprint(\"  Binary: is_weekend -> Already in [0, 1]\")\nprint(\"  [SUCCESS] No normalization needed for time features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d159476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bước 2: TEMPORAL SPLIT TRƯỚC KHI NORMALIZE (Tránh Data Leakage)\nprint(\"\\n[PROCESSING] Step 2: Temporal Train/Val/Test Split BEFORE Normalization...\")\n\n# Tính toán ngày chia dựa trên percentile thời gian  \ntime_stats = df_features.select(\n    F.min(\"datetime\").alias(\"min_time\"),\n    F.max(\"datetime\").alias(\"max_time\")\n).collect()[0]\n\nmin_time = time_stats[\"min_time\"]\nmax_time = time_stats[\"max_time\"]\ntotal_days = (max_time - min_time).days\n\n# 70% train, 15% validation, 15% test\ntrain_days = int(total_days * 0.70)\nval_days = int(total_days * 0.15)\n\ntrain_end = min_time + pd.Timedelta(days=train_days)\nval_end = train_end + pd.Timedelta(days=val_days)\n\nprint(f\"[?] Temporal Split (Avoiding Data Leakage):\")\nprint(f\"  [?] Train:      {min_time.strftime('%Y-%m-%d')} -> {train_end.strftime('%Y-%m-%d')} ({(train_end - min_time).days} days)\")\nprint(f\"  [?] Validation: {train_end.strftime('%Y-%m-%d')} -> {val_end.strftime('%Y-%m-%d')} ({(val_end - train_end).days} days)\")\nprint(f\"  [?] Test:       {val_end.strftime('%Y-%m-%d')} -> {max_time.strftime('%Y-%m-%d')} ({(max_time - val_end).days} days)\")\n\n# Split data - Count BEFORE caching to trigger evaluation\ndf_train_raw = df_features.filter(F.col(\"datetime\") < train_end)\ndf_val_raw = df_features.filter((F.col(\"datetime\") >= train_end) & (F.col(\"datetime\") < val_end))\ndf_test_raw = df_features.filter(F.col(\"datetime\") >= val_end)\n\ntrain_count = df_train_raw.count()\nval_count = df_val_raw.count() \ntest_count = df_test_raw.count()\ntotal_count = train_count + val_count + test_count\n\nprint(f\"\\n[DATA] Split Results:\")\nprint(f\"  [?] Train: {train_count:8,} ({train_count/total_count*100:.1f}%)\")\nprint(f\"  [?] Val:   {val_count:8,} ({val_count/total_count*100:.1f}%)\")\nprint(f\"  [?] Test:  {test_count:8,} ({test_count/total_count*100:.1f}%)\")\n\n# Now cache after counting (avoids double computation)\ndf_train_raw = df_train_raw.cache()\ndf_val_raw = df_val_raw.cache()\ndf_test_raw = df_test_raw.cache()\n\nprint(f\"\\n[SUCCESS] Temporal split completed!\")\nprint(f\"   [WARNING]  Next: Normalize using TRAIN SET statistics ONLY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816ecee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bước 3: Normalize NUMERICAL GỐC (CHỈ gốc, KHÔNG có lag features)\nprint(f\"\\n[PROCESSING] Step 3: Normalize NUMERICAL BASE FEATURES using TRAIN SET ONLY...\")\n\n# [WARNING] QUAN TRỌNG: CHỈ normalize các cột GỐC, KHÔNG bao gồm lag features\n# Lag features sẽ tạo SAU từ các cột đã scale\nnumerical_base_cols = [\n    # Pollutants (current values only)\n    \"PM2_5\", \"PM10\", \"NO2\", \"SO2\",\n    # Weather features (current values only)\n    \"temperature_2m\", \"relative_humidity_2m\", \"wind_speed_10m\", \n    \"wind_direction_10m\", \"precipitation\"\n]\n\nprint(f\"[DATA] Normalizing {len(numerical_base_cols)} BASE features (NO lag features yet)...\")\nprint(f\"   Features to normalize: {numerical_base_cols}\")\nprint(f\"   [WARNING]  Computing min/max from TRAIN SET ONLY (preventing data leakage)\")\n\n# Tính min/max CHỈ TỪ TRAIN SET\nscaler_params = {}\n\nfor col_name in numerical_base_cols:\n    if col_name in df_train_raw.columns:\n        # CHỈ DÙNG TRAIN SET ĐỂ TÍNH MIN/MAX  \n        stats = df_train_raw.select(\n            F.min(col_name).alias(\"min\"),\n            F.max(col_name).alias(\"max\")\n        ).collect()[0]\n        \n        min_val = stats[\"min\"]\n        max_val = stats[\"max\"]\n        \n        # [WARNING] CRITICAL: Handle None values from null columns\n        if min_val is None or max_val is None:\n            print(f\"  [WARNING]  Skipping {col_name}: All values are null\")\n            continue\n        \n        # [WARNING] CRITICAL: Tránh chia 0 khi min = max\n        if max_val == min_val:\n            max_val = min_val + 1\n        \n        scaler_params[col_name] = {\"min\": min_val, \"max\": max_val}\n        print(f\"  [OK] {col_name:30s}: [{min_val:8.2f}, {max_val:8.2f}] -> [0, 1]\")\n\nprint(f\"\\n[SUCCESS] Scaler parameters computed from TRAIN SET only!\")\n\n# Áp dụng normalization cho tất cả splits\ndef apply_scaling(df, scaler_params):\n    \"\"\"Apply Min-Max scaling using precomputed parameters\"\"\"\n    df_scaled = df\n    for col_name, params in scaler_params.items():\n        if col_name in df.columns:\n            min_val = params[\"min\"]\n            max_val = params[\"max\"]\n            df_scaled = df_scaled.withColumn(\n                f\"{col_name}_scaled\",\n                (F.col(col_name) - min_val) / (max_val - min_val)\n            )\n    return df_scaled\n\nprint(f\"\\n Applying Min-Max scaling [0, 1] to all splits...\")\n\n# Apply scaling and trigger computation\ndf_train = apply_scaling(df_train_raw, scaler_params)\ndf_val = apply_scaling(df_val_raw, scaler_params)\ndf_test = apply_scaling(df_test_raw, scaler_params)\n\n# Trigger computation and cache\n_ = df_train.count()\n_ = df_val.count()\n_ = df_test.count()\n\ndf_train = df_train.cache()\ndf_val = df_val.cache()\ndf_test = df_test.cache()\n\n# Unpersist raw versions to free memory\ndf_train_raw.unpersist()\ndf_val_raw.unpersist()\ndf_test_raw.unpersist()\n\nprint(f\"[SUCCESS] Base feature normalization completed!\")\nprint(f\"   [DATA] All splits normalized using train statistics only\")\nprint(f\"   [WARNING]  Next: Create lag features FROM SCALED COLUMNS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1c6a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bước 4: Lưu Scaler Parameters\nprint(f\"\\n[SAVE] Step 4: Saving Scaler Parameters...\")\n\nimport json\nfrom pathlib import Path\n\nscaler_json = {\n    col: {\"min\": float(params[\"min\"]), \"max\": float(params[\"max\"])} \n    for col, params in scaler_params.items()\n}\n\n# ========================================\n# ADAPTIVE PATH (Kaggle vs Colab vs Local)\n# ========================================\nif IN_KAGGLE:\n    # [KAGGLE] Kaggle: Write to /kaggle/working (auto-saved on commit)\n    processed_dir = Path(\"/kaggle/working/processed\")\n    print(f\"[KAGGLE] Kaggle mode: Saving to {processed_dir}\")\n    \nelif IN_COLAB:\n    # [COLAB] Colab: Write to Google Drive\n    processed_dir = Path(\"/content/drive/MyDrive/pm25-data/processed\")\n    print(f\"[COLAB] Colab mode: Saving to Google Drive\")\n    \nelse:\n    # [LOCAL] Local: Write to project folder\n    processed_dir = Path(\"../data/processed\")\n    print(f\"[LOCAL] Local mode: Saving to {processed_dir}\")\n\n# Tạo thư mục với parents=True (tạo cả parent directories nếu chưa có)\nprocessed_dir.mkdir(parents=True, exist_ok=True)\n\n# Lưu ra file JSON\nscaler_path = processed_dir / \"scaler_params.json\"\nwith open(scaler_path, 'w') as f:\n    json.dump(scaler_json, f, indent=2)\n\nprint(f\"[SUCCESS] Scaler parameters saved to: {scaler_path}\")\nprint(f\"   - Computed from TRAIN SET only (no data leakage)\")\nprint(f\"   - Used for denormalizing predictions during inference\")\nprint(f\"   - Contains {len(scaler_params)} base features\")\n\n# Hiển thị ví dụ\nprint(f\"\\n[METADATA] Example scaler params (from train set):\")\nexample_cols = [\"PM2_5\", \"temperature_2m\", \"wind_speed_10m\"]\nfor col in example_cols:\n    if col in scaler_params:\n        params = scaler_params[col]\n        print(f\"  {col:20s}: min={params['min']:.2f}, max={params['max']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e754fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bước 5: Tạo Lag Features TỪ CÁC CỘT ĐÃ SCALE (CHỈ CHO XGBOOST)\nprint(f\"\\n[PROCESSING] Step 5: Creating Lag Features FROM SCALED COLUMNS (XGBoost only)...\")\n\n# [WARNING] QUAN TRỌNG: Lag features được tạo TỪ CÁC CỘT ĐÃ SCALE\n# -> Đảm bảo lag và gốc có CÙNG SCALE PARAMETERS\n# -> Giữ đúng mối quan hệ giữa giá trị hiện tại và quá khứ\n\nLAG_STEPS = [1, 2, 3, 6, 12, 24]  # 1h, 2h, 3h, 6h, 12h, 24h trước\n\n# Columns cần tạo lag (sử dụng bản SCALED)\nlag_base_columns = [\"PM2_5\", \"PM10\", \"NO2\", \"SO2\", \n                    \"temperature_2m\", \"relative_humidity_2m\", \n                    \"wind_speed_10m\", \"precipitation\"]\n\nprint(f\"\\n[METADATA] Creating lag features:\")\nprint(f\"   Deep Learning models: No lags needed (learn from sequences)\")\nprint(f\"   XGBoost: {len(LAG_STEPS)} lags × {len(lag_base_columns)} variables = {len(LAG_STEPS) * len(lag_base_columns)} features\")\nprint(f\"   [SUCCESS] Using SCALED columns as source (proper scale relationship)\")\n\n# Window cho từng location (sắp xếp theo thời gian)\nw_lag = Window.partitionBy(\"location_id\").orderBy(\"datetime\")\n\n# Tạo lag features cho từng split (train, val, test)\ndef create_lag_features(df, lag_base_columns, lag_steps):\n    \"\"\"Create lag features from SCALED columns\"\"\"\n    df_with_lags = df\n    \n    for col_name in lag_base_columns:\n        col_scaled = f\"{col_name}_scaled\"\n        \n        if col_scaled in df.columns:\n            for lag in lag_steps:\n                lag_col_name = f\"{col_name}_lag{lag}_scaled\"\n                \n                # [SUCCESS] Tạo lag TỪ CỘT ĐÃ SCALE\n                df_with_lags = df_with_lags.withColumn(\n                    lag_col_name,\n                    F.lag(col_scaled, lag).over(w_lag)\n                )\n    \n    return df_with_lags\n\n# Apply to all splits\nprint(f\"\\n[PROCESSING] Creating lag features for all splits...\")\ndf_train = create_lag_features(df_train, lag_base_columns, LAG_STEPS)\ndf_val = create_lag_features(df_val, lag_base_columns, LAG_STEPS)\ndf_test = create_lag_features(df_test, lag_base_columns, LAG_STEPS)\n\nprint(f\"  [OK] Train: {len(LAG_STEPS) * len(lag_base_columns)} lag features created\")\nprint(f\"  [OK] Val:   {len(LAG_STEPS) * len(lag_base_columns)} lag features created\")\nprint(f\"  [OK] Test:  {len(LAG_STEPS) * len(lag_base_columns)} lag features created\")\n\n# Trigger computation and cache\n_ = df_train.count()\n_ = df_val.count()\n_ = df_test.count()\n\ndf_train = df_train.cache()\ndf_val = df_val.cache()\ndf_test = df_test.cache()\n\nprint(f\"\\n[SUCCESS] Lag features created successfully!\")\nprint(f\"   [SUCCESS] All lags created FROM SCALED columns\")\nprint(f\"   [SUCCESS] Lag and base features have SAME scale parameters\")\nprint(f\"   [SUCCESS] Proper temporal relationship preserved\")\n\n# ========================================\n# XỬ LÝ NULL VALUES TRONG LAG FEATURES\n# ========================================\nprint(f\"\\n[PROCESSING] Handling null values in lag features...\")\n\n# Tạo list tất cả lag feature names\nlag_feature_names = [f\"{col}_lag{lag}_scaled\" for col in lag_base_columns for lag in LAG_STEPS]\n\n# Đếm nulls TRƯỚC khi xử lý\nprint(f\"\\n[DATA] Null counts BEFORE handling:\")\nsample_lag_features = lag_feature_names[:3]\nfor lag_col in sample_lag_features:\n    if lag_col in df_train.columns:\n        null_count = df_train.filter(F.col(lag_col).isNull()).count()\n        total_count = df_train.count()\n        print(f\"  {lag_col:35s}: {null_count:8,} nulls ({null_count/total_count*100:.2f}%)\")\n\nprint(f\"\\n[WARNING]  Reason: First {max(LAG_STEPS)} hours of each location have no previous data\")\nprint(f\"   Strategy: DROP records with ANY null lag feature\")\n\n# Track counts before drop\ntrain_before = df_train.count()\nval_before = df_val.count()\ntest_before = df_test.count()\n\n# Function to drop nulls\ndef drop_lag_nulls(df, lag_features):\n    \"\"\"Drop records with any null lag feature\"\"\"\n    df_clean = df\n    for col in lag_features:\n        if col in df.columns:\n            df_clean = df_clean.filter(F.col(col).isNotNull())\n    return df_clean\n\n# Apply to all splits\nprint(f\"\\n[?]  Dropping records with null lag features...\")\ndf_train_clean = drop_lag_nulls(df_train, lag_feature_names)\ndf_val_clean = drop_lag_nulls(df_val, lag_feature_names)\ndf_test_clean = drop_lag_nulls(df_test, lag_feature_names)\n\n# Count after\ntrain_after = df_train_clean.count()\nval_after = df_val_clean.count()\ntest_after = df_test_clean.count()\n\n# Cache cleaned datasets\ndf_train_clean = df_train_clean.cache()\ndf_val_clean = df_val_clean.cache()\ndf_test_clean = df_test_clean.cache()\n\n# Unpersist old ones\ndf_train.unpersist()\ndf_val.unpersist()\ndf_test.unpersist()\n\n# Reassign\ndf_train = df_train_clean\ndf_val = df_val_clean\ndf_test = df_test_clean\n\nprint(f\"\\n[DATA] Records dropped (null lag features):\")\nprint(f\"  [?] Train: {train_before:,} -> {train_after:,} (dropped {train_before - train_after:,}, {(train_before - train_after)/train_before*100:.2f}%)\")\nprint(f\"  [?] Val:   {val_before:,} -> {val_after:,} (dropped {val_before - val_after:,}, {(val_before - val_after)/val_before*100:.2f}%)\")\nprint(f\"  [?] Test:  {test_before:,} -> {test_after:,} (dropped {test_before - test_after:,}, {(test_before - test_after)/test_before*100:.2f}%)\")\n\n# Verify no nulls\nprint(f\"\\n[SUCCESS] Verification - checking for remaining nulls...\")\nsample_check = lag_feature_names[:3]\ntotal_nulls_after = 0\nfor lag_col in sample_check:\n    if lag_col in df_train.columns:\n        null_count = df_train.filter(F.col(lag_col).isNull()).count()\n        total_nulls_after += null_count\n        status = \"[SUCCESS]\" if null_count == 0 else \"[ERROR]\"\n        print(f\"  {lag_col:35s}: {null_count:8,} nulls {status}\")\n\nif total_nulls_after == 0:\n    print(f\"\\n[SUCCESS] All lag features are clean!\")\nelse:\n    print(f\"\\n[WARNING]  Still {total_nulls_after} nulls found!\")\n\nprint(f\"\\n[SUCCESS] Lag features + Null handling completed!\")\nprint(f\"   - Created {len(lag_feature_names)} lag features FROM SCALED columns\")\nprint(f\"   - Lost only first {max(LAG_STEPS)} hours per location\")\nprint(f\"   - All lag features now have valid values\")\nprint(f\"   - Data quality ensured for XGBoost training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45295509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bước 6: Chuẩn bị Features cho từng Model\nprint(\"\\n[PROCESSING] Step 6: Preparing Model-Specific Features...\")\n\n# ========================================\n# FEATURES CHO DEEP LEARNING MODELS (CNN1D-BLSTM, LSTM)\n# ========================================\n# Không cần lag features vì models tự học temporal patterns từ sequences\n\ndl_input_features = []\n\n# 1. Pollutants scaled (trừ PM2_5 - đây là target)\ndl_input_features.extend([\"PM10_scaled\", \"NO2_scaled\", \"SO2_scaled\"])\n\n# 2. Weather features scaled (core features)\ndl_input_features.extend([\n    \"temperature_2m_scaled\", \"relative_humidity_2m_scaled\", \n    \"wind_speed_10m_scaled\", \"wind_direction_10m_scaled\", \"precipitation_scaled\"\n])\n\n# 3. Time features (cyclic encoding - đã ở dạng sin/cos trong [-1, 1])\ndl_input_features.extend([\n    \"hour_sin\", \"hour_cos\", \n    \"month_sin\", \"month_cos\",\n    \"day_of_week_sin\", \"day_of_week_cos\"\n])\n\n# 4. Time features (binary)\ndl_input_features.extend([\"is_weekend\"])\n\nprint(f\"[MODEL] DEEP LEARNING Features: {len(dl_input_features)} features\")\nprint(f\"   - Current pollutants (scaled): 3\")\nprint(f\"   - Weather (scaled): 5\") \nprint(f\"   - Time (cyclic): 6\")\nprint(f\"   - Time (binary): 1\")\nprint(f\"   - NO LAG FEATURES (models learn from sequences)\")\n\n# ========================================  \n# FEATURES CHO XGBOOST\n# ========================================\n# Cần lag features vì không có khả năng xử lý sequences\n\nxgb_input_features = dl_input_features.copy()  # Start with DL features\n\n# Thêm lag features CHỈ CHO XGBOOST (đã được tạo từ scaled columns)\nfor col_name in lag_base_columns:\n    for lag in LAG_STEPS:\n        lag_col_name = f\"{col_name}_lag{lag}_scaled\"\n        xgb_input_features.append(lag_col_name)\n\nprint(f\"\\n[DATA] XGBOOST Features: {len(xgb_input_features)} features\")\nprint(f\"   - Deep Learning base features: {len(dl_input_features)}\")\nprint(f\"   - Lag features (from scaled columns): {len(lag_base_columns) * len(LAG_STEPS)}\")\nprint(f\"   - Total: {len(xgb_input_features)} features\")\n\n# Target variable (đã scaled)\ntarget_feature = \"PM2_5_scaled\"\n\nprint(f\"\\n[SUCCESS] Model-specific features prepared:\")\nprint(f\"  [MODEL] CNN1D-BLSTM-Attention: {len(dl_input_features)} features\")\nprint(f\"  [MODEL] LSTM: {len(dl_input_features)} features\")  \nprint(f\"  [DATA] XGBoost: {len(xgb_input_features)} features\")\nprint(f\"  [TARGET] Target: {target_feature}\")\n\n# [WARNING] CRITICAL: Verify ALL columns exist\nmissing_dl = [col for col in dl_input_features if col not in df_train.columns]\nmissing_xgb = [col for col in xgb_input_features if col not in df_train.columns]\nmissing_target = target_feature not in df_train.columns\n\nif missing_dl or missing_xgb or missing_target:\n    print(f\"\\n[ERROR] MISSING COLUMNS DETECTED:\")\n    if missing_dl: \n        print(f\"  DL models: {missing_dl}\")\n    if missing_xgb: \n        print(f\"  XGBoost: {missing_xgb[:5]}...\")  # Show first 5\n    if missing_target:\n        print(f\"  Target: {target_feature}\")\n    \n    print(f\"\\n[WARNING]  Available scaled columns:\")\n    scaled_cols = [c for c in df_train.columns if c.endswith('_scaled')]\n    print(f\"  {scaled_cols[:10]}...\")\n    \n    raise ValueError(\"Missing required feature columns! Check normalization step.\")\nelse:\n    print(f\"\\n[SUCCESS] All feature columns exist in datasets!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700d4016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bước 7: Prepare Final Model Datasets\nprint(\"\\n[PROCESSING] Step 7: Preparing Final Model-Specific Datasets...\")\n\n# ========================================\n# DEEP LEARNING DATASETS (CNN1D-BLSTM & LSTM)\n# ========================================\n# Không cần lag features, chỉ cần base features + time features\n\nprint(f\"\\n[MODEL] Deep Learning datasets (no lag features):\")\n\n# Select only DL features + target\ndl_train = df_train.select(\"location_id\", \"datetime\", target_feature, *dl_input_features)\ndl_val = df_val.select(\"location_id\", \"datetime\", target_feature, *dl_input_features)\ndl_test = df_test.select(\"location_id\", \"datetime\", target_feature, *dl_input_features)\n\n# Cache\ndl_train = dl_train.cache()\ndl_val = dl_val.cache()\ndl_test = dl_test.cache()\n\ndl_train_count = dl_train.count()\ndl_val_count = dl_val.count()\ndl_test_count = dl_test.count()\n\nprint(f\"  [OK] Train: {dl_train_count:,} records, {len(dl_input_features)} features\")\nprint(f\"  [OK] Val:   {dl_val_count:,} records, {len(dl_input_features)} features\")\nprint(f\"  [OK] Test:  {dl_test_count:,} records, {len(dl_input_features)} features\")\n\n# ========================================\n# XGBOOST DATASETS\n# ========================================\n# Cần cả base features + lag features\n\nprint(f\"\\n[DATA] XGBoost datasets (with lag features):\")\n\n# Select XGB features + target\nxgb_train = df_train.select(\"location_id\", \"datetime\", target_feature, *xgb_input_features)\nxgb_val = df_val.select(\"location_id\", \"datetime\", target_feature, *xgb_input_features)\nxgb_test = df_test.select(\"location_id\", \"datetime\", target_feature, *xgb_input_features)\n\n# Cache\nxgb_train = xgb_train.cache()\nxgb_val = xgb_val.cache()\nxgb_test = xgb_test.cache()\n\nxgb_train_count = xgb_train.count()\nxgb_val_count = xgb_val.count()\nxgb_test_count = xgb_test.count()\n\nprint(f\"  [OK] Train: {xgb_train_count:,} records, {len(xgb_input_features)} features\")\nprint(f\"  [OK] Val:   {xgb_val_count:,} records, {len(xgb_input_features)} features\")\nprint(f\"  [OK] Test:  {xgb_test_count:,} records, {len(xgb_input_features)} features\")\n\nprint(f\"\\n[SUCCESS] Final datasets prepared!\")\nprint(f\"   [MODEL] Deep Learning: {len(dl_input_features)} features (no lags)\")\nprint(f\"   [DATA] XGBoost: {len(xgb_input_features)} features (with {len(lag_base_columns) * len(LAG_STEPS)} lags)\")\nprint(f\"   [TARGET] Target: {target_feature}\")\nprint(f\"   [SUCCESS] All datasets cleaned and ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebc9c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bước 8: Feature Engineering Summary + Metadata Saving\nprint(\"\\n\" + \"=\"*80)\nprint(\"[DATA] FEATURE ENGINEERING PIPELINE SUMMARY\")\nprint(\"=\"*80)\n\nprint(f\"\\n[SUCCESS] PIPELINE EXECUTION ORDER (Correct - No Data Leakage):\")\nprint(f\"   [1] Time Features -> Added cyclic (sin/cos) + is_weekend\")\nprint(f\"   [2] Temporal Split -> 70% train / 15% val / 15% test\")\nprint(f\"   [3] Normalization -> Min-Max [0,1] using TRAIN statistics ONLY\")\nprint(f\"   [4] Lag Features + Null Handling -> Created FROM SCALED columns, dropped nulls\")\nprint(f\"   [5] Scaler Params -> Saved for inference\")\nprint(f\"   [6] Model Features -> Prepared for Deep Learning & XGBoost\")\nprint(f\"   [7] Final Datasets -> Ready for training\")\n\nprint(f\"\\n[DATA] DATASET STATISTICS:\")\nprint(f\"   Total records: {dl_train_count + dl_val_count + dl_test_count:,}\")\nprint(f\"   Total locations: {df_train.select('location_id').distinct().count()}\")\nprint(f\"   Time range: {min_time.strftime('%Y-%m-%d')} -> {max_time.strftime('%Y-%m-%d')}\")\n\nprint(f\"\\n[METADATA] FEATURE BREAKDOWN:\")\nprint(f\"   [MODEL] Deep Learning (CNN1D-BLSTM & LSTM): {len(dl_input_features)} features\")\nprint(f\"      ├─ Pollutants (scaled): 3 (PM10, NO2, SO2)\")\nprint(f\"      ├─ Weather (scaled): 5 (temp, humidity, wind, precipitation)\")\nprint(f\"      ├─ Time (cyclic): 6 (hour, month, day_of_week -> sin/cos)\")\nprint(f\"      └─ Time (binary): 1 (is_weekend)\")\nprint(f\"   \")\nprint(f\"   [DATA] XGBoost: {len(xgb_input_features)} features\")\nprint(f\"      ├─ Deep Learning features: {len(dl_input_features)}\")\nprint(f\"      └─ Lag features: {len(lag_base_columns) * len(LAG_STEPS)} ({len(lag_base_columns)} vars × {len(LAG_STEPS)} lags)\")\n\nprint(f\"\\n[TARGET] TARGET VARIABLE:\")\nprint(f\"   {target_feature} (normalized PM2.5 in [0, 1])\")\n\nprint(f\"\\n[SUCCESS] DATA QUALITY CHECKS:\")\nprint(f\"   [OK] No missing values in target\")\nprint(f\"   [OK] No missing values in features\")\nprint(f\"   [OK] No outliers (removed by WHO/EPA standards)\")\nprint(f\"   [OK] Proper temporal ordering\")\nprint(f\"   [OK] No data leakage (train/val/test temporally separated)\")\nprint(f\"   [OK] Correct scale relationship (lag from scaled columns)\")\nprint(f\"   [OK] No nulls in lag features (first {max(LAG_STEPS)}h dropped)\")\n\nprint(f\"\\n[SAVE] SAVED ARTIFACTS:\")\nprint(f\"   [FILES] scaler_params.json -> Min-Max parameters (train set only)\")\nprint(f\"   [FILES] feature_metadata.json -> Feature lists & configuration\")\n\nprint(f\"\\n[RUN] READY FOR NEXT PHASE:\")\nprint(f\"   Variables in memory:\")\nprint(f\"   - Deep Learning: dl_train, dl_val, dl_test\")\nprint(f\"   - XGBoost: xgb_train, xgb_val, xgb_test\")\nprint(f\"   Next step: Sequence creation for Deep Learning models\")\n\nprint(\"=\"*80)\n\n# ========================================\n# SAVE FEATURE METADATA\n# ========================================\n# Lưu metadata về feature engineering để tham khảo trong tương lai\n\nimport json\nfrom pathlib import Path\n\n# Metadata cho feature engineering\ndataset_metadata = {\n    \"project\": \"PM2.5 Prediction\",\n    \"preprocessing_version\": \"2.0_refactored\",\n    \"pipeline_order\": [\n        \"Time Features (cyclic encoding)\",\n        \"Temporal Split (70/15/15)\",\n        \"Normalization (train stats only)\",\n        \"Lag Features (from scaled columns)\",\n        \"Null Handling (drop first 24h per location)\"\n    ],\n    \"deep_learning_features\": dl_input_features,\n    \"xgboost_features\": xgb_input_features,\n    \"target_feature\": target_feature,\n    \"lag_config\": {\n        \"lag_steps\": LAG_STEPS,\n        \"lag_base_columns\": lag_base_columns,\n        \"total_lag_features\": len(lag_base_columns) * len(LAG_STEPS)\n    },\n    \"temporal_split\": {\n        \"train_end\": train_end.isoformat(),\n        \"val_end\": val_end.isoformat(),\n        \"min_time\": min_time.isoformat(),\n        \"max_time\": max_time.isoformat()\n    },\n    \"dataset_counts\": {\n        \"dl_train\": dl_train_count,\n        \"dl_val\": dl_val_count,\n        \"dl_test\": dl_test_count,\n        \"xgb_train\": xgb_train_count,\n        \"xgb_val\": xgb_val_count,\n        \"xgb_test\": xgb_test_count\n    },\n    \"total_features\": {\n        \"deep_learning\": len(dl_input_features),\n        \"xgboost\": len(xgb_input_features)\n    }\n}\n\n# ========================================\n# ADAPTIVE PATH (Kaggle vs Colab vs Local)\n# ========================================\nif IN_KAGGLE:\n    # [KAGGLE] Kaggle: Write to /kaggle/working (auto-saved on commit)\n    processed_dir = Path(\"/kaggle/working/processed\")\n    print(f\"\\n[KAGGLE] Kaggle mode: Saving metadata to {processed_dir}\")\n    \nelif IN_COLAB:\n    # [COLAB] Colab: Write to Google Drive\n    processed_dir = Path(\"/content/drive/MyDrive/pm25-data/processed\")\n    print(f\"\\n[COLAB] Colab mode: Saving metadata to Google Drive\")\n    \nelse:\n    # [LOCAL] Local: Write to project folder\n    processed_dir = Path(\"../data/processed\")\n    print(f\"\\n[LOCAL] Local mode: Saving metadata to {processed_dir}\")\n\n# Tạo thư mục với parents=True (tạo cả parent directories nếu chưa có)\nprocessed_dir.mkdir(parents=True, exist_ok=True)\n\n# Lưu metadata\nmetadata_path = processed_dir / \"feature_metadata.json\"\nwith open(metadata_path, 'w') as f:\n    json.dump(dataset_metadata, f, indent=2)\n\nprint(f\"\\n[SAVE] Feature metadata saved to: {metadata_path}\")\nprint(f\"   [SUCCESS] Pipeline version: 2.0 (refactored - no data leakage)\")\nprint(f\"   [SUCCESS] Contains: feature lists, lag config, split info, dataset counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523011ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bước 9: Create Sequence Data for Deep Learning Models\nprint(\"\\n[PROCESSING] Step 9: Creating Sequence Data for Deep Learning Models...\")\n\nimport pyspark.sql.functions as F\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import ArrayType, DoubleType\n\n# Sequence configuration (optimized for Colab)\nCNN_SEQUENCE_LENGTH = 48  # Optimal for long-term patterns\nLSTM_SEQUENCE_LENGTH = 24  # Optimal for medium-term patterns\n\nprint(f\"[GEAR]  Sequence Configuration:\")\nprint(f\"   - CNN1D-BLSTM-Attention: {CNN_SEQUENCE_LENGTH} timesteps\")\nprint(f\"   - LSTM: {LSTM_SEQUENCE_LENGTH} timesteps\")\n\ndef create_sequences_optimized(df, feature_cols, target_col, sequence_length):\n    \"\"\"\n    Optimized sequence creation with checkpointing to avoid StackOverflow\n    \n    [TARGET] Key Strategy:\n    - Batch processing to avoid deep logical plans\n    - Checkpoint after each batch to reset plan depth\n    - Use broadcast joins for efficiency\n    - Single final filter for null handling\n    \n    [?] Null Handling (2-Layer Protection):\n    Layer 1: Drop first N records/location (incomplete history)\n    Layer 2: Filter ANY null in sequences (data gaps)\n    Result: 100% clean sequences with ZERO nulls\n    \"\"\"\n    print(f\"    Creating {sequence_length}-step sequences...\")\n    \n    window_spec = Window.partitionBy(\"location_id\").orderBy(\"datetime\")\n    \n    # ========================================\n    # LAYER 1: Drop first N records (incomplete history)\n    # ========================================\n    df_base = df.select(\"location_id\", \"datetime\", target_col, *feature_cols) \\\n                .repartition(4, \"location_id\") \\\n                .withColumn(\"row_num\", F.row_number().over(window_spec)) \\\n                .filter(F.col(\"row_num\") > sequence_length) \\\n                .drop(\"row_num\") \\\n                .cache()\n    \n    records_after_layer1 = df_base.count()  # Materialize\n    print(f\"      [?]  Layer 1: Dropped first {sequence_length} records/location\")\n    print(f\"         Records: {records_after_layer1:,}\")\n    \n    # ========================================\n    # BATCH PROCESSING (避免 StackOverflow)\n    # ========================================\n    # Chia features thành batches nhỏ để tránh logical plan quá sâu\n    BATCH_SIZE = 4  # Mỗi batch xử lý 4 features (4 × 48 lags = 192 ops - safe)\n    feature_batches = [feature_cols[i:i+BATCH_SIZE] for i in range(0, len(feature_cols), BATCH_SIZE)]\n    \n    print(f\"        [INSTALL] Processing {len(feature_batches)} batches ({len(feature_cols)} features)...\")\n    \n    base_cols = [\"location_id\", \"datetime\"]\n    result_df = df_base.select(*base_cols)\n    \n    for batch_idx, batch_features in enumerate(feature_batches, 1):\n        print(f\"           Batch {batch_idx}/{len(feature_batches)}: {len(batch_features)} features\")\n        \n        # Tạo batch DataFrame\n        batch_df = df_base.select(*base_cols, *batch_features)\n        \n        # Tạo sequences cho batch này\n        for col_name in batch_features:\n            # Tạo array of lags [t-1, t-2, ..., t-N]\n            lag_exprs = [F.lag(col_name, step).over(window_spec) for step in range(1, sequence_length + 1)]\n            batch_df = batch_df.withColumn(f\"{col_name}_sequence\", F.array(*lag_exprs))\n        \n        # Select chỉ sequence columns\n        sequence_cols = [f\"{col}_sequence\" for col in batch_features]\n        batch_df = batch_df.select(*base_cols, *sequence_cols).cache()\n        batch_df.count()  # Materialize để reset logical plan\n        \n        # Join vào result\n        result_df = result_df.join(batch_df, base_cols, \"inner\")\n        \n        # Unpersist batch (giải phóng memory)\n        batch_df.unpersist()\n    \n    # ========================================\n    # LAYER 2: Filter nulls in sequences\n    # ========================================\n    print(f\"        [?] Filtering null sequences...\")\n    \n    all_sequence_cols = [f\"{col}_sequence\" for col in feature_cols]\n    \n    # Build null filter: ALL sequences must be NOT NULL\n    from functools import reduce\n    null_filter = reduce(\n        lambda acc, col: acc & F.col(col).isNotNull(),\n        all_sequence_cols,\n        F.lit(True)\n    )\n    \n    # Also check: NO null VALUES inside arrays (extra safety)\n    # Trick: size(array) should equal sequence_length (nulls make size smaller)\n    for seq_col in all_sequence_cols:\n        null_filter = null_filter & (F.size(seq_col) == sequence_length)\n    \n    result_df = result_df.filter(null_filter)\n    records_after_layer2 = result_df.count()\n    dropped = records_after_layer1 - records_after_layer2\n    \n    if dropped > 0:\n        print(f\"      [?]  Layer 2: Dropped {dropped:,} records with nulls\")\n        print(f\"         Records: {records_after_layer1:,} -> {records_after_layer2:,}\")\n    else:\n        print(f\"      [SUCCESS] Layer 2: No data gaps detected\")\n    \n    # ========================================\n    # FINAL: Add target and clean up\n    # ========================================\n    result_df = result_df.join(\n        df_base.select(\"location_id\", \"datetime\", target_col),\n        [\"location_id\", \"datetime\"],\n        \"inner\"\n    ).filter(F.col(target_col).isNotNull()) \\\n     .withColumnRenamed(target_col, \"target_value\") \\\n     .cache()\n    \n    final_count = result_df.count()\n    retention_rate = (final_count / records_after_layer1) * 100\n    \n    print(f\"      [SUCCESS] Final: {final_count:,} records ({retention_rate:.1f}% retained)\")\n    \n    # Cleanup\n    df_base.unpersist()\n    \n    return result_df\n\nprint(\"\\n[DATA] Creating sequences for each model...\")\n\n# Create CNN1D-BLSTM sequences\nprint(f\"\\n[MODEL] CNN1D-BLSTM-Attention ({CNN_SEQUENCE_LENGTH} timesteps):\")\ntry:\n    cnn_train_clean = create_sequences_optimized(dl_train, dl_input_features, target_feature, CNN_SEQUENCE_LENGTH)\n    cnn_val_clean = create_sequences_optimized(dl_val, dl_input_features, target_feature, CNN_SEQUENCE_LENGTH)\n    cnn_test_clean = create_sequences_optimized(dl_test, dl_input_features, target_feature, CNN_SEQUENCE_LENGTH)\n    print(f\"    [SUCCESS] CNN sequences created successfully\")\nexcept Exception as e:\n    print(f\"    [ERROR] CNN sequence creation failed: {str(e)[:100]}...\")\n    cnn_train_clean = cnn_val_clean = cnn_test_clean = None\n\n# Create LSTM sequences  \nprint(f\"\\n[PROCESSING] LSTM ({LSTM_SEQUENCE_LENGTH} timesteps):\")\ntry:\n    lstm_train_clean = create_sequences_optimized(dl_train, dl_input_features, target_feature, LSTM_SEQUENCE_LENGTH)\n    lstm_val_clean = create_sequences_optimized(dl_val, dl_input_features, target_feature, LSTM_SEQUENCE_LENGTH)\n    lstm_test_clean = create_sequences_optimized(dl_test, dl_input_features, target_feature, LSTM_SEQUENCE_LENGTH)\n    print(f\"    [SUCCESS] LSTM sequences created successfully\")\nexcept Exception as e:\n    print(f\"    [ERROR] LSTM sequence creation failed: {str(e)[:100]}...\")\n    lstm_train_clean = lstm_val_clean = lstm_test_clean = None\n\nprint(f\"\\n[SUCCESS] Sequence data preparation completed!\")\nprint(f\"\\n[METADATA] Data Quality Guarantee:\")\nprint(f\"   [OK] Layer 1: No incomplete history (first {CNN_SEQUENCE_LENGTH}/{LSTM_SEQUENCE_LENGTH} records dropped)\")\nprint(f\"   [OK] Layer 2: No data gaps in middle (nulls filtered out)\")\nprint(f\"   [OK] Result: 100% clean sequences with ZERO nulls\")\nprint(f\"   [OK] Ready for high-quality model training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a99d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bước 10: Export Final Datasets to Disk\nprint(\"\\n[INSTALL] Step 10: Exporting Final Datasets to Disk...\")\n\nimport json\nfrom pathlib import Path\n\n# ========================================\n# ADAPTIVE OUTPUT PATH (Kaggle vs Colab vs Local)\n# ========================================\nif IN_KAGGLE:\n    # [KAGGLE] Kaggle: Write to /kaggle/working (auto-saved on commit)\n    processed_dir = Path(\"/kaggle/working/processed\")\n    print(f\"[KAGGLE] Kaggle mode: Saving to {processed_dir}\")\n    print(f\"   [WARNING]  Files will be auto-saved when you commit notebook\")\n    \nelif IN_COLAB:\n    # [COLAB] Colab: Write to Google Drive\n    processed_dir = Path(\"/content/drive/MyDrive/pm25-data/processed\")\n    print(f\"[COLAB] Colab mode: Saving to Google Drive\")\n    \nelse:\n    # [LOCAL] Local: Write to project folder\n    processed_dir = Path(\"../data/processed\")\n    print(f\"[LOCAL] Local mode: Saving to {processed_dir}\")\n\nprocessed_dir.mkdir(parents=True, exist_ok=True)\n\n# Check dataset availability\ndatasets_ready = {\n    \"cnn\": cnn_train_clean is not None and cnn_val_clean is not None and cnn_test_clean is not None,\n    \"lstm\": lstm_train_clean is not None and lstm_val_clean is not None and lstm_test_clean is not None,\n    \"xgb\": xgb_train is not None and xgb_val is not None and xgb_test is not None\n}\n\nprint(f\"\\n[DATA] Dataset Status:\")\nfor model, ready in datasets_ready.items():\n    model_name = {\"cnn\": \"CNN1D-BLSTM\", \"lstm\": \"LSTM\", \"xgb\": \"XGBoost\"}[model]\n    status = \"[SUCCESS] Ready\" if ready else \"[ERROR] Not Ready\"\n    print(f\"  {model_name}: {status}\")\n\n# ========================================\n# EXPORT DATASETS TO PARQUET\n# ========================================\nprint(f\"\\n[SAVE] Exporting datasets to Parquet format...\")\n\nexport_summary = {\n    \"cnn\": {\"train\": 0, \"val\": 0, \"test\": 0},\n    \"lstm\": {\"train\": 0, \"val\": 0, \"test\": 0},\n    \"xgb\": {\"train\": 0, \"val\": 0, \"test\": 0}\n}\n\n# Export CNN1D-BLSTM datasets\nif datasets_ready[\"cnn\"]:\n    print(f\"\\n  [MODEL] Exporting CNN1D-BLSTM datasets...\")\n    cnn_dir = processed_dir / \"cnn_sequences\"\n    cnn_dir.mkdir(exist_ok=True)\n    \n    cnn_train_clean.write.mode(\"overwrite\").parquet(str(cnn_dir / \"train\"))\n    cnn_val_clean.write.mode(\"overwrite\").parquet(str(cnn_dir / \"val\"))\n    cnn_test_clean.write.mode(\"overwrite\").parquet(str(cnn_dir / \"test\"))\n    \n    export_summary[\"cnn\"][\"train\"] = cnn_train_clean.count()\n    export_summary[\"cnn\"][\"val\"] = cnn_val_clean.count()\n    export_summary[\"cnn\"][\"test\"] = cnn_test_clean.count()\n    \n    print(f\"     [SUCCESS] Saved to: {cnn_dir}/\")\n    print(f\"        - train: {export_summary['cnn']['train']:,} records\")\n    print(f\"        - val:   {export_summary['cnn']['val']:,} records\")\n    print(f\"        - test:  {export_summary['cnn']['test']:,} records\")\n\n# Export LSTM datasets\nif datasets_ready[\"lstm\"]:\n    print(f\"\\n  [PROCESSING] Exporting LSTM datasets...\")\n    lstm_dir = processed_dir / \"lstm_sequences\"\n    lstm_dir.mkdir(exist_ok=True)\n    \n    lstm_train_clean.write.mode(\"overwrite\").parquet(str(lstm_dir / \"train\"))\n    lstm_val_clean.write.mode(\"overwrite\").parquet(str(lstm_dir / \"val\"))\n    lstm_test_clean.write.mode(\"overwrite\").parquet(str(lstm_dir / \"test\"))\n    \n    export_summary[\"lstm\"][\"train\"] = lstm_train_clean.count()\n    export_summary[\"lstm\"][\"val\"] = lstm_val_clean.count()\n    export_summary[\"lstm\"][\"test\"] = lstm_test_clean.count()\n    \n    print(f\"     [SUCCESS] Saved to: {lstm_dir}/\")\n    print(f\"        - train: {export_summary['lstm']['train']:,} records\")\n    print(f\"        - val:   {export_summary['lstm']['val']:,} records\")\n    print(f\"        - test:  {export_summary['lstm']['test']:,} records\")\n\n# Export XGBoost datasets\nif datasets_ready[\"xgb\"]:\n    print(f\"\\n  [DATA] Exporting XGBoost datasets...\")\n    xgb_dir = processed_dir / \"xgboost\"\n    xgb_dir.mkdir(exist_ok=True)\n    \n    xgb_train.write.mode(\"overwrite\").parquet(str(xgb_dir / \"train\"))\n    xgb_val.write.mode(\"overwrite\").parquet(str(xgb_dir / \"val\"))\n    xgb_test.write.mode(\"overwrite\").parquet(str(xgb_dir / \"test\"))\n    \n    export_summary[\"xgb\"][\"train\"] = xgb_train.count()\n    export_summary[\"xgb\"][\"val\"] = xgb_val.count()\n    export_summary[\"xgb\"][\"test\"] = xgb_test.count()\n    \n    print(f\"     [SUCCESS] Saved to: {xgb_dir}/\")\n    print(f\"        - train: {export_summary['xgb']['train']:,} records\")\n    print(f\"        - val:   {export_summary['xgb']['val']:,} records\")\n    print(f\"        - test:  {export_summary['xgb']['test']:,} records\")\n\n# ========================================\n# SAVE METADATA\n# ========================================\nprint(f\"\\n[SAVE] Saving metadata...\")\n\n# Create comprehensive metadata\nfinal_metadata = {\n    \"project\": \"PM2.5 Prediction\",\n    \"preprocessing_completed\": True,\n    \"export_timestamp\": str(pd.Timestamp.now()),\n    \"environment\": \"kaggle\" if IN_KAGGLE else (\"colab\" if IN_COLAB else \"local\"),\n    \"models\": {\n        \"cnn1d_blstm\": {\n            \"sequence_length\": CNN_SEQUENCE_LENGTH,\n            \"features\": len(dl_input_features),\n            \"ready\": datasets_ready[\"cnn\"],\n            \"export_path\": str(processed_dir / \"cnn_sequences\"),\n            \"record_counts\": export_summary[\"cnn\"]\n        },\n        \"lstm\": {\n            \"sequence_length\": LSTM_SEQUENCE_LENGTH, \n            \"features\": len(dl_input_features),\n            \"ready\": datasets_ready[\"lstm\"],\n            \"export_path\": str(processed_dir / \"lstm_sequences\"),\n            \"record_counts\": export_summary[\"lstm\"]\n        },\n        \"xgboost\": {\n            \"features\": len(xgb_input_features),\n            \"lag_steps\": LAG_STEPS,\n            \"ready\": datasets_ready[\"xgb\"],\n            \"export_path\": str(processed_dir / \"xgboost\"),\n            \"record_counts\": export_summary[\"xgb\"]\n        }\n    },\n    \"feature_details\": {\n        \"deep_learning_features\": dl_input_features,\n        \"xgboost_features\": xgb_input_features,\n        \"target\": target_feature\n    },\n    \"data_format\": \"parquet\",\n    \"null_handling\": {\n        \"strategy\": \"2-layer protection\",\n        \"layer1\": f\"Dropped first {CNN_SEQUENCE_LENGTH}/{LSTM_SEQUENCE_LENGTH} records per location\",\n        \"layer2\": \"Filtered records with nulls in sequence history\"\n    }\n}\n\n# Save metadata\nmetadata_path = processed_dir / \"datasets_ready.json\"\nwith open(metadata_path, 'w') as f:\n    json.dump(final_metadata, f, indent=2)\n\nprint(f\"   [SUCCESS] Metadata saved to: {metadata_path}\")\n\n# Save scaler params\nscaler_path = processed_dir / \"scaler_params.json\"\nscaler_json = {\n    col: {\"min\": float(params[\"min\"]), \"max\": float(params[\"max\"])} \n    for col, params in scaler_params.items()\n}\nwith open(scaler_path, 'w') as f:\n    json.dump(scaler_json, f, indent=2)\nprint(f\"   [SUCCESS] Scaler params saved to: {scaler_path}\")\n\n# Save feature metadata\nfeature_metadata_path = processed_dir / \"feature_metadata.json\"\nfeature_metadata = {\n    \"deep_learning_features\": dl_input_features,\n    \"xgboost_features\": xgb_input_features,\n    \"target\": target_feature,\n    \"lag_steps\": LAG_STEPS,\n    \"lag_base_columns\": lag_base_columns\n}\nwith open(feature_metadata_path, 'w') as f:\n    json.dump(feature_metadata, f, indent=2)\nprint(f\"   [SUCCESS] Feature metadata saved to: {feature_metadata_path}\")\n\n# ========================================\n# FINAL SUMMARY\n# ========================================\nprint(f\"\\n{'='*80}\")\nprint(f\"[SUCCESS] DATA PREPROCESSING & EXPORT COMPLETE!\")\nprint(f\"{'='*80}\")\n\nif IN_KAGGLE:\n    print(f\"\\n[KAGGLE] KAGGLE OUTPUT:\")\n    print(f\"   [?] Location: /kaggle/working/processed/\")\n    print(f\"   [?] To save permanently:\")\n    print(f\"      1. Click 'Save Version' (top right)\")\n    print(f\"      2. Choose 'Save & Run All' (recommended)\")\n    print(f\"      3. Wait for completion (~20-30 min)\")\n    print(f\"      4. Output will appear in 'Output' tab\")\n    print(f\"      5. Use as dataset: '+ Add Data' -> Your Output\")\n    \nelif IN_COLAB:\n    print(f\"\\n[COLAB] COLAB OUTPUT:\")\n    print(f\"   [?] Saved to Google Drive: {processed_dir}\")\n    print(f\"   [SUCCESS] Files persist across sessions\")\n    \nelse:\n    print(f\"\\n[LOCAL] LOCAL OUTPUT:\")\n    print(f\"   [?] Location: {processed_dir.absolute()}\")\n\nprint(f\"\\n[?] Exported Directory Structure:\")\nprint(f\"   {processed_dir}/\")\nprint(f\"   ├── cnn_sequences/\")\nprint(f\"   │   ├── train/  ({export_summary['cnn']['train']:,} records)\")\nprint(f\"   │   ├── val/    ({export_summary['cnn']['val']:,} records)\")\nprint(f\"   │   └── test/   ({export_summary['cnn']['test']:,} records)\")\nprint(f\"   ├── lstm_sequences/\")\nprint(f\"   │   ├── train/  ({export_summary['lstm']['train']:,} records)\")\nprint(f\"   │   ├── val/    ({export_summary['lstm']['val']:,} records)\")\nprint(f\"   │   └── test/   ({export_summary['lstm']['test']:,} records)\")\nprint(f\"   ├── xgboost/\")\nprint(f\"   │   ├── train/  ({export_summary['xgb']['train']:,} records)\")\nprint(f\"   │   ├── val/    ({export_summary['xgb']['val']:,} records)\")\nprint(f\"   │   └── test/   ({export_summary['xgb']['test']:,} records)\")\nprint(f\"   ├── scaler_params.json\")\nprint(f\"   ├── feature_metadata.json\")\nprint(f\"   └── datasets_ready.json\")\n\nprint(f\"\\n[DATA] Total Dataset Sizes:\")\ntotal_cnn = sum(export_summary['cnn'].values())\ntotal_lstm = sum(export_summary['lstm'].values())\ntotal_xgb = sum(export_summary['xgb'].values())\nprint(f\"   - CNN1D-BLSTM: {total_cnn:,} records ({CNN_SEQUENCE_LENGTH} timesteps, {len(dl_input_features)} features)\")\nprint(f\"   - LSTM:        {total_lstm:,} records ({LSTM_SEQUENCE_LENGTH} timesteps, {len(dl_input_features)} features)\")\nprint(f\"   - XGBoost:     {total_xgb:,} records ({len(xgb_input_features)} features)\")\n\nprint(f\"\\n[RUN] Ready for Model Training Phase!\")\nif IN_KAGGLE:\n    print(f\"   [?] Next: Create new notebook, add this output as dataset\")\n    print(f\"   [?] Load: spark.read.parquet('/kaggle/input/<output-name>/processed/...')\")\nelif IN_COLAB:\n    print(f\"   [?] Load from Drive in next session\")\nelse:\n    print(f\"   [?] Load: spark.read.parquet('{processed_dir}/...')\")\nprint(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d4f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Kiểm tra kích thước files Parquet đã export\nprint(\"\\n Parquet File Size Analysis...\")\nprint(\"=\"*80)\n\nimport os\n\n# ========================================\n# ADAPTIVE PATH\n# ========================================\nif IN_KAGGLE:\n    processed_dir = Path(\"/kaggle/working/processed\")\n    print(f\"[KAGGLE] Kaggle mode: Analyzing {processed_dir}\")\nelif IN_COLAB:\n    processed_dir = Path(\"/content/drive/MyDrive/pm25-data/processed\")\n    print(f\"[COLAB] Colab mode: Analyzing Google Drive\")\nelse:\n    processed_dir = Path(\"../data/processed\")\n    print(f\"[LOCAL] Local mode: Analyzing {processed_dir}\")\n\n# ========================================\n# Tính kích thước thư mục\n# ========================================\ndef get_dir_size(path):\n    \"\"\"Tính tổng kích thước của thư mục (bao gồm tất cả subdirectories)\"\"\"\n    total_size = 0\n    try:\n        for dirpath, dirnames, filenames in os.walk(path):\n            for filename in filenames:\n                filepath = os.path.join(dirpath, filename)\n                if os.path.exists(filepath):\n                    total_size += os.path.getsize(filepath)\n    except Exception as e:\n        print(f\"   [WARNING]  Error accessing {path}: {e}\")\n        return 0\n    return total_size\n\ndef format_size(bytes_size):\n    \"\"\"Format bytes thành human-readable\"\"\"\n    for unit in ['B', 'KB', 'MB', 'GB']:\n        if bytes_size < 1024.0:\n            return f\"{bytes_size:.2f} {unit}\"\n        bytes_size /= 1024.0\n    return f\"{bytes_size:.2f} TB\"\n\n# ========================================\n# Phân tích từng dataset\n# ========================================\nprint(\"\\n[INSTALL] Dataset Sizes:\")\nprint(\"-\" * 80)\n\ndatasets = {\n    'CNN Sequences': 'cnn_sequences',\n    'LSTM Sequences': 'lstm_sequences',\n    'XGBoost Data': 'xgboost'\n}\n\ntotal_size = 0\nsize_breakdown = {}\n\nfor name, folder in datasets.items():\n    dataset_path = processed_dir / folder\n    if dataset_path.exists():\n        # Tính size cho từng split\n        splits = ['train', 'val', 'test']\n        dataset_total = 0\n        print(f\"\\n[MODEL] {name}:\")\n        \n        for split in splits:\n            split_path = dataset_path / split\n            if split_path.exists():\n                size = get_dir_size(split_path)\n                dataset_total += size\n                print(f\"   - {split:5s}: {format_size(size):>12s}\")\n        \n        print(f\"   {'Total:':7s} {format_size(dataset_total):>12s}\")\n        size_breakdown[name] = dataset_total\n        total_size += dataset_total\n    else:\n        print(f\"\\n[WARNING]  {name}: Folder not found ({dataset_path})\")\n\n# Metadata files\nprint(f\"\\n Metadata Files:\")\nmetadata_files = ['scaler_params.json', 'feature_metadata.json', 'datasets_ready.json']\nmetadata_total = 0\nfor file in metadata_files:\n    file_path = processed_dir / file\n    if file_path.exists():\n        size = os.path.getsize(file_path)\n        metadata_total += size\n        print(f\"   - {file:25s}: {format_size(size):>12s}\")\ntotal_size += metadata_total\n\n# ========================================\n# Tổng kết\n# ========================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"[DATA] TOTAL SIZE SUMMARY:\")\nprint(\"=\"*80)\n\nfor name, size in size_breakdown.items():\n    percentage = (size / total_size * 100) if total_size > 0 else 0\n    print(f\"   {name:20s}: {format_size(size):>12s} ({percentage:5.1f}%)\")\n\nprint(f\"   {'Metadata':20s}: {format_size(metadata_total):>12s} ({(metadata_total/total_size*100):5.1f}%)\")\nprint(f\"\\n   {'GRAND TOTAL':20s}: {format_size(total_size):>12s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95740d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [?] EXAMPLE: Load Preprocessed Data with Pandas\nprint(\"\\n[?] Loading Preprocessed Data with Pandas...\")\nprint(\"=\"*80)\n\nimport pandas as pd\nimport numpy as np\n\n# ========================================\n# ADAPTIVE PATH\n# ========================================\nif IN_KAGGLE:\n    data_dir = Path(\"/kaggle/input/<your-dataset-name>/processed\")\n    print(f\"[KAGGLE] Kaggle mode: Loading from /kaggle/input/\")\n    print(f\"   Replace <your-dataset-name> with actual dataset name\")\nelif IN_COLAB:\n    data_dir = Path(\"/content/drive/MyDrive/pm25-data/processed\")\n    print(f\"[COLAB] Colab mode: Loading from Google Drive\")\nelse:\n    data_dir = Path(\"../data/processed\")\n    print(f\"[LOCAL] Local mode: Loading from {data_dir}\")\n\n# ========================================\n# LOAD PARQUET FILES\n# ========================================\nprint(\"\\n[INSTALL] Loading datasets...\")\n\ntry:\n    # CNN sequences (48 timesteps)\n    print(\"\\n[MODEL] CNN1D-BLSTM-Attention:\")\n    cnn_train = pd.read_parquet(data_dir / 'cnn_sequences' / 'train')\n    cnn_val = pd.read_parquet(data_dir / 'cnn_sequences' / 'val')\n    cnn_test = pd.read_parquet(data_dir / 'cnn_sequences' / 'test')\n    print(f\"   [SUCCESS] Train: {cnn_train.shape} | Val: {cnn_val.shape} | Test: {cnn_test.shape}\")\n    \n    # LSTM sequences (24 timesteps)\n    print(\"\\n[PROCESSING] LSTM:\")\n    lstm_train = pd.read_parquet(data_dir / 'lstm_sequences' / 'train')\n    lstm_val = pd.read_parquet(data_dir / 'lstm_sequences' / 'val')\n    lstm_test = pd.read_parquet(data_dir / 'lstm_sequences' / 'test')\n    print(f\"   [SUCCESS] Train: {lstm_train.shape} | Val: {lstm_val.shape} | Test: {lstm_test.shape}\")\n    \n    # XGBoost data (flat features)\n    print(\"\\n[DATA] XGBoost:\")\n    xgb_train = pd.read_parquet(data_dir / 'xgboost' / 'train')\n    xgb_val = pd.read_parquet(data_dir / 'xgboost' / 'val')\n    xgb_test = pd.read_parquet(data_dir / 'xgboost' / 'test')\n    print(f\"   [SUCCESS] Train: {xgb_train.shape} | Val: {xgb_val.shape} | Test: {xgb_test.shape}\")\n    \n    print(f\"\\n[SUCCESS] All datasets loaded successfully!\")\n    \nexcept FileNotFoundError as e:\n    print(f\"\\n[ERROR] Error: Dataset not found!\")\n    print(f\"   {e}\")\n    print(f\"\\n[INFO] Make sure to:\")\n    if IN_KAGGLE:\n        print(f\"   1. Add this notebook's output as dataset\")\n        print(f\"   2. Update <your-dataset-name> in path\")\n    else:\n        print(f\"   1. Run previous cells to generate data\")\n        print(f\"   2. Check path: {data_dir}\")\n\n# ========================================\n# LOAD METADATA\n# ========================================\nprint(\"\\n[METADATA] Loading metadata...\")\n\nimport json\n\ntry:\n    # Scaler parameters (for denormalization)\n    with open(data_dir / 'scaler_params.json', 'r') as f:\n        scaler_params = json.load(f)\n    print(f\"   [SUCCESS] Scaler params: {len(scaler_params)} features\")\n    \n    # Feature metadata\n    with open(data_dir / 'feature_metadata.json', 'r') as f:\n        feature_metadata = json.load(f)\n    print(f\"   [SUCCESS] Feature metadata: {feature_metadata['preprocessing_version']}\")\n    \nexcept FileNotFoundError:\n    print(f\"   [WARNING]  Metadata files not found (optional)\")\n    scaler_params = None\n    feature_metadata = None\n\n# ========================================\n# DATA PREPARATION FOR DEEP LEARNING\n# ========================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"[TARGET] Prepare Data for Training:\")\nprint(\"=\"*80)\n\n# Example: CNN data preparation\nprint(\"\\n[INSTALL] CNN1D-BLSTM data preparation:\")\n\n# Get sequence columns\nsequence_cols = [col for col in cnn_train.columns if col.endswith('_sequence')]\nprint(f\"   - Sequence features: {len(sequence_cols)}\")\nprint(f\"   - Feature names: {sequence_cols[:3]}... (showing first 3)\")\n\n# Convert to numpy arrays for deep learning\nprint(\"\\n   Converting to numpy arrays...\")\n\n# Extract sequences (each row has arrays)\nX_cnn_train = np.array([\n    np.stack([cnn_train[col].iloc[i] for col in sequence_cols], axis=0)\n    for i in range(len(cnn_train))\n])  # Shape: (samples, features, timesteps)\n\n# Transpose to (samples, timesteps, features) for Keras/PyTorch\nX_cnn_train = X_cnn_train.transpose(0, 2, 1)\ny_cnn_train = cnn_train['target_value'].values\n\nprint(f\"   [SUCCESS] X_train shape: {X_cnn_train.shape} (samples, timesteps, features)\")\nprint(f\"   [SUCCESS] y_train shape: {y_cnn_train.shape}\")\n\n# Same for validation and test\nX_cnn_val = np.array([\n    np.stack([cnn_val[col].iloc[i] for col in sequence_cols], axis=0)\n    for i in range(len(cnn_val))\n]).transpose(0, 2, 1)\ny_cnn_val = cnn_val['target_value'].values\n\nX_cnn_test = np.array([\n    np.stack([cnn_test[col].iloc[i] for col in sequence_cols], axis=0)\n    for i in range(len(cnn_test))\n]).transpose(0, 2, 1)\ny_cnn_test = cnn_test['target_value'].values\n\nprint(f\"   [SUCCESS] Val:  X={X_cnn_val.shape}, y={y_cnn_val.shape}\")\nprint(f\"   [SUCCESS] Test: X={X_cnn_test.shape}, y={y_cnn_test.shape}\")\n\n# ========================================\n# EXAMPLE: XGBoost data preparation\n# ========================================\nprint(\"\\n[INSTALL] XGBoost data preparation:\")\n\n# XGBoost data is already flat (no sequences)\nX_xgb_train = xgb_train.drop(['location_id', 'datetime', 'target_value'], axis=1).values\ny_xgb_train = xgb_train['target_value'].values\n\nX_xgb_val = xgb_val.drop(['location_id', 'datetime', 'target_value'], axis=1).values\ny_xgb_val = xgb_val['target_value'].values\n\nX_xgb_test = xgb_test.drop(['location_id', 'datetime', 'target_value'], axis=1).values\ny_xgb_test = xgb_test['target_value'].values\n\nprint(f\"   [SUCCESS] X_train shape: {X_xgb_train.shape} (samples, features)\")\nprint(f\"   [SUCCESS] y_train shape: {y_xgb_train.shape}\")\nprint(f\"   [SUCCESS] Val:  X={X_xgb_val.shape}, y={y_xgb_val.shape}\")\nprint(f\"   [SUCCESS] Test: X={X_xgb_test.shape}, y={y_xgb_test.shape}\")\n\n# ========================================\n# SUMMARY\n# ========================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"[SUCCESS] DATA READY FOR TRAINING!\")\nprint(\"=\"*80)\n\nprint(\"\\n[DATA] Available datasets:\")\nprint(\"   [MODEL] CNN1D-BLSTM-Attention:\")\nprint(f\"      X_cnn_train: {X_cnn_train.shape}\")\nprint(f\"      X_cnn_val:   {X_cnn_val.shape}\")\nprint(f\"      X_cnn_test:  {X_cnn_test.shape}\")\n\nprint(\"\\n   [PROCESSING] LSTM: (Similar structure, use lstm_train/val/test)\")\n\nprint(\"\\n   [DATA] XGBoost:\")\nprint(f\"      X_xgb_train: {X_xgb_train.shape}\")\nprint(f\"      X_xgb_val:   {X_xgb_val.shape}\")\nprint(f\"      X_xgb_test:  {X_xgb_test.shape}\")\n\nprint(\"\\n[RUN] Next steps:\")\nprint(\"   1. Build model: model = tf.keras.Sequential([...])\")\nprint(\"   2. Compile: model.compile(optimizer='adam', loss='mse')\")\nprint(\"   3. Train: model.fit(X_cnn_train, y_cnn_train, epochs=50)\")\nprint(\"   4. Evaluate: model.evaluate(X_cnn_test, y_cnn_test)\")\n\nprint(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1511033",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_train.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}